{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 영화리뷰 감정 분석: SentencePiece 성능 개선 (vocab_size 변경)\n",
    "\n",
    "## 목표\n",
    "1. SentencePiece의 `unigram` 모델 타입을 기준으로, `vocab_size`를 [4000, 8000, 16000, 32000]으로 변경하며 모델을 각각 학습\n",
    "2. 각 `vocab_size`에 따른 감정 분석 모델(BiLSTM, 1D CNN)의 성능 변화를 비교 분석하여 최적의 `vocab_size`를 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from soynlp.normalizer import emoticon_normalize\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('ratings_train.txt')\n",
    "test_df = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 훈련 데이터 전처리 ---\n",
      "전처리 전 데이터 개수: 150000\n",
      "결측치 제거 후: 149995개\n",
      "중복 제거 후: 146182개\n",
      "최종 데이터 개수: 145594개\n",
      "--- 테스트 데이터 전처리 ---\n",
      "전처리 전 데이터 개수: 50000\n",
      "결측치 제거 후: 49997개\n",
      "최종 데이터 개수: 49369개\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df, is_train=True):\n",
    "    df_name = '훈련' if is_train else '테스트'\n",
    "    print(f'--- {df_name} 데이터 전처리 ---')\n",
    "    print(f'전처리 전 데이터 개수: {len(df)}')\n",
    "    df.dropna(inplace=True)\n",
    "    print(f'결측치 제거 후: {len(df)}개')\n",
    "    if is_train:\n",
    "        df.drop_duplicates(subset=['document'], inplace=True)\n",
    "        print(f'중복 제거 후: {len(df)}개')\n",
    "    df['document'] = df['document'].apply(lambda x: emoticon_normalize(x, num_repeats=2) if isinstance(x, str) else x)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[~df['document'].str.match('^[ㄱ-ㅎㅏ-ㅣ]+$', na=False)]\n",
    "    min_char_len = 3\n",
    "    max_char_len = 140\n",
    "    df = df[df['document'].str.len() >= min_char_len]\n",
    "    df['document'] = df['document'].str.slice(0, max_char_len)\n",
    "    print(f'최종 데이터 개수: {len(df)}개')\n",
    "    return df\n",
    "\n",
    "train_df_clean = preprocess_data(train_df.copy(), is_train=True)\n",
    "test_df_clean = preprocess_data(test_df.copy(), is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 및 학습/평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        out = self.fc(hidden)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).permute(0, 2, 1)\n",
    "        conved = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [torch.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        out = self.fc(cat)\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, num_epochs, device, lr=1e-3, model_name='Model'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(f'--- {model_name} 모델 학습 시작 ---')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_correct, epoch_train_total = 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            epoch_train_correct += (predicted == labels).sum().item()\n",
    "            epoch_train_total += labels.size(0)\n",
    "        avg_train_acc = epoch_train_correct / epoch_train_total\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_correct, epoch_val_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs).squeeze()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                epoch_val_correct += (predicted == labels).sum().item()\n",
    "                epoch_val_total += labels.size(0)\n",
    "        avg_val_acc = epoch_val_correct / epoch_val_total\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] -> Train Acc: {avg_train_acc:.4f} | Val Acc: {avg_val_acc:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f'Final Test Accuracy: {test_accuracy:.4f}')\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. vocab_size에 따른 성능 비교 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running Experiment for vocab_size = 4000\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=nsmc_corpus.txt --model_prefix=nsmc_spm_unigram_4000 --vocab_size=4000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: nsmc_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: nsmc_spm_unigram_4000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: nsmc_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 145594 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=5380734\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1682\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 145594 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1890499\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 305522 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 145594\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 356666\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 356666 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=157275 obj=15.3978 num_tokens=840450 num_tokens/piece=5.34382\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=145558 obj=14.3645 num_tokens=845509 num_tokens/piece=5.80874\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109096 obj=14.4679 num_tokens=883090 num_tokens/piece=8.09461\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=108890 obj=14.4092 num_tokens=883713 num_tokens/piece=8.11565\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=81662 obj=14.6464 num_tokens=927916 num_tokens/piece=11.3629\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=81645 obj=14.5828 num_tokens=928059 num_tokens/piece=11.367\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=61233 obj=14.8449 num_tokens=971130 num_tokens/piece=15.8596\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=61233 obj=14.7805 num_tokens=971168 num_tokens/piece=15.8602\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=45924 obj=15.0731 num_tokens=1016035 num_tokens/piece=22.1243\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45924 obj=15.0072 num_tokens=1016025 num_tokens/piece=22.1241\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34443 obj=15.3283 num_tokens=1062380 num_tokens/piece=30.8446\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34443 obj=15.261 num_tokens=1062357 num_tokens/piece=30.8439\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25832 obj=15.6096 num_tokens=1109878 num_tokens/piece=42.9652\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25832 obj=15.5377 num_tokens=1109898 num_tokens/piece=42.966\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19374 obj=15.9227 num_tokens=1160573 num_tokens/piece=59.9036\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19374 obj=15.8414 num_tokens=1160585 num_tokens/piece=59.9043\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14530 obj=16.2676 num_tokens=1213370 num_tokens/piece=83.5079\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14530 obj=16.1768 num_tokens=1213398 num_tokens/piece=83.5098\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10897 obj=16.6431 num_tokens=1272041 num_tokens/piece=116.733\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10897 obj=16.5314 num_tokens=1272524 num_tokens/piece=116.777\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8172 obj=17.0435 num_tokens=1334373 num_tokens/piece=163.286\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8172 obj=16.9276 num_tokens=1334799 num_tokens/piece=163.338\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6129 obj=17.4998 num_tokens=1400973 num_tokens/piece=228.581\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6129 obj=17.3663 num_tokens=1400996 num_tokens/piece=228.585\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4596 obj=18.0359 num_tokens=1479569 num_tokens/piece=321.925\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4596 obj=17.8714 num_tokens=1479607 num_tokens/piece=321.934\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4400 obj=17.9862 num_tokens=1493608 num_tokens/piece=339.456\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4400 obj=17.9585 num_tokens=1493614 num_tokens/piece=339.458\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: nsmc_spm_unigram_4000.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: nsmc_spm_unigram_4000.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BiLSTM (vocab_size=4000) 모델 학습 시작 ---\n",
      "Epoch [1/7] -> Train Acc: 0.6930 | Val Acc: 0.7981\n",
      "Epoch [2/7] -> Train Acc: 0.8239 | Val Acc: 0.8327\n",
      "Epoch [3/7] -> Train Acc: 0.8506 | Val Acc: 0.8414\n",
      "Epoch [4/7] -> Train Acc: 0.8654 | Val Acc: 0.8451\n",
      "Epoch [5/7] -> Train Acc: 0.8773 | Val Acc: 0.8450\n",
      "Epoch [6/7] -> Train Acc: 0.8890 | Val Acc: 0.8485\n",
      "Epoch [7/7] -> Train Acc: 0.8999 | Val Acc: 0.8502\n",
      "Final Test Accuracy: 0.8449\n",
      "--- 1D CNN (vocab_size=4000) 모델 학습 시작 ---\n",
      "Epoch [1/7] -> Train Acc: 0.6728 | Val Acc: 0.7744\n",
      "Epoch [2/7] -> Train Acc: 0.7883 | Val Acc: 0.8190\n",
      "Epoch [3/7] -> Train Acc: 0.8251 | Val Acc: 0.8330\n",
      "Epoch [4/7] -> Train Acc: 0.8452 | Val Acc: 0.8423\n",
      "Epoch [5/7] -> Train Acc: 0.8559 | Val Acc: 0.8447\n",
      "Epoch [6/7] -> Train Acc: 0.8664 | Val Acc: 0.8489\n",
      "Epoch [7/7] -> Train Acc: 0.8735 | Val Acc: 0.8497\n",
      "Final Test Accuracy: 0.8471\n",
      "==================================================\n",
      "Running Experiment for vocab_size = 8000\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=nsmc_corpus.txt --model_prefix=nsmc_spm_unigram_8000 --vocab_size=8000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: nsmc_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: nsmc_spm_unigram_8000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: nsmc_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 145594 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=5380734\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1682\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 145594 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1890499\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 305522 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 145594\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 356666\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 356666 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=157275 obj=15.3978 num_tokens=840450 num_tokens/piece=5.34382\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=145558 obj=14.3645 num_tokens=845509 num_tokens/piece=5.80874\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109096 obj=14.4679 num_tokens=883090 num_tokens/piece=8.09461\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=108890 obj=14.4092 num_tokens=883713 num_tokens/piece=8.11565\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=81662 obj=14.6464 num_tokens=927916 num_tokens/piece=11.3629\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=81645 obj=14.5828 num_tokens=928059 num_tokens/piece=11.367\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=61233 obj=14.8449 num_tokens=971130 num_tokens/piece=15.8596\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=61233 obj=14.7805 num_tokens=971168 num_tokens/piece=15.8602\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=45924 obj=15.0731 num_tokens=1016035 num_tokens/piece=22.1243\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45924 obj=15.0072 num_tokens=1016025 num_tokens/piece=22.1241\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34443 obj=15.3283 num_tokens=1062380 num_tokens/piece=30.8446\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34443 obj=15.261 num_tokens=1062357 num_tokens/piece=30.8439\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25832 obj=15.6096 num_tokens=1109878 num_tokens/piece=42.9652\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25832 obj=15.5377 num_tokens=1109898 num_tokens/piece=42.966\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19374 obj=15.9227 num_tokens=1160573 num_tokens/piece=59.9036\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19374 obj=15.8414 num_tokens=1160585 num_tokens/piece=59.9043\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14530 obj=16.2676 num_tokens=1213370 num_tokens/piece=83.5079\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14530 obj=16.1768 num_tokens=1213398 num_tokens/piece=83.5098\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10897 obj=16.6431 num_tokens=1272041 num_tokens/piece=116.733\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10897 obj=16.5314 num_tokens=1272524 num_tokens/piece=116.777\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=16.9014 num_tokens=1316762 num_tokens/piece=149.632\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=16.8188 num_tokens=1316855 num_tokens/piece=149.643\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: nsmc_spm_unigram_8000.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: nsmc_spm_unigram_8000.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BiLSTM (vocab_size=8000) 모델 학습 시작 ---\n",
      "Epoch [1/7] -> Train Acc: 0.6830 | Val Acc: 0.7891\n",
      "Epoch [2/7] -> Train Acc: 0.8216 | Val Acc: 0.8233\n",
      "Epoch [3/7] -> Train Acc: 0.8530 | Val Acc: 0.8403\n",
      "Epoch [4/7] -> Train Acc: 0.8737 | Val Acc: 0.8459\n",
      "Epoch [5/7] -> Train Acc: 0.8889 | Val Acc: 0.8513\n",
      "Epoch [6/7] -> Train Acc: 0.9028 | Val Acc: 0.8494\n",
      "Epoch [7/7] -> Train Acc: 0.9150 | Val Acc: 0.8502\n",
      "Final Test Accuracy: 0.8445\n",
      "--- 1D CNN (vocab_size=8000) 모델 학습 시작 ---\n",
      "Epoch [1/7] -> Train Acc: 0.6470 | Val Acc: 0.7525\n",
      "Epoch [2/7] -> Train Acc: 0.7710 | Val Acc: 0.8104\n",
      "Epoch [3/7] -> Train Acc: 0.8227 | Val Acc: 0.8329\n",
      "Epoch [4/7] -> Train Acc: 0.8479 | Val Acc: 0.8419\n",
      "Epoch [5/7] -> Train Acc: 0.8627 | Val Acc: 0.8488\n",
      "Epoch [6/7] -> Train Acc: 0.8749 | Val Acc: 0.8508\n",
      "Epoch [7/7] -> Train Acc: 0.8819 | Val Acc: 0.8520\n",
      "Final Test Accuracy: 0.8472\n",
      "==================================================\n",
      "Running Experiment for vocab_size = 16000\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=nsmc_corpus.txt --model_prefix=nsmc_spm_unigram_16000 --vocab_size=16000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: nsmc_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: nsmc_spm_unigram_16000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: nsmc_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 145594 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=5380734\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1682\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 145594 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1890499\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 305522 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 145594\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 356666\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 356666 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=157275 obj=15.3978 num_tokens=840450 num_tokens/piece=5.34382\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=145558 obj=14.3645 num_tokens=845509 num_tokens/piece=5.80874\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109096 obj=14.4679 num_tokens=883090 num_tokens/piece=8.09461\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=108890 obj=14.4092 num_tokens=883713 num_tokens/piece=8.11565\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=81662 obj=14.6464 num_tokens=927916 num_tokens/piece=11.3629\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=81645 obj=14.5828 num_tokens=928059 num_tokens/piece=11.367\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=61233 obj=14.8449 num_tokens=971130 num_tokens/piece=15.8596\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=61233 obj=14.7805 num_tokens=971168 num_tokens/piece=15.8602\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=45924 obj=15.0731 num_tokens=1016035 num_tokens/piece=22.1243\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45924 obj=15.0072 num_tokens=1016025 num_tokens/piece=22.1241\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34443 obj=15.3283 num_tokens=1062380 num_tokens/piece=30.8446\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34443 obj=15.261 num_tokens=1062357 num_tokens/piece=30.8439\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25832 obj=15.6096 num_tokens=1109878 num_tokens/piece=42.9652\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25832 obj=15.5377 num_tokens=1109898 num_tokens/piece=42.966\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19374 obj=15.9227 num_tokens=1160573 num_tokens/piece=59.9036\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19374 obj=15.8414 num_tokens=1160585 num_tokens/piece=59.9043\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17600 obj=15.9698 num_tokens=1177191 num_tokens/piece=66.8859\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17600 obj=15.9436 num_tokens=1177196 num_tokens/piece=66.8861\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: nsmc_spm_unigram_16000.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: nsmc_spm_unigram_16000.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BiLSTM (vocab_size=16000) 모델 학습 시작 ---\n",
      "Epoch [1/7] -> Train Acc: 0.6565 | Val Acc: 0.7205\n",
      "Epoch [2/7] -> Train Acc: 0.7960 | Val Acc: 0.8172\n",
      "Epoch [3/7] -> Train Acc: 0.8511 | Val Acc: 0.8355\n",
      "Epoch [4/7] -> Train Acc: 0.8749 | Val Acc: 0.8426\n",
      "Epoch [5/7] -> Train Acc: 0.8932 | Val Acc: 0.8468\n",
      "Epoch [6/7] -> Train Acc: 0.9091 | Val Acc: 0.8470\n",
      "Epoch [7/7] -> Train Acc: 0.9222 | Val Acc: 0.8462\n",
      "Final Test Accuracy: 0.8440\n",
      "--- 1D CNN (vocab_size=16000) 모델 학습 시작 ---\n",
      "Epoch [1/7] -> Train Acc: 0.6366 | Val Acc: 0.7396\n",
      "Epoch [2/7] -> Train Acc: 0.7641 | Val Acc: 0.7967\n",
      "Epoch [3/7] -> Train Acc: 0.8191 | Val Acc: 0.8232\n",
      "Epoch [4/7] -> Train Acc: 0.8499 | Val Acc: 0.8366\n",
      "Epoch [5/7] -> Train Acc: 0.8672 | Val Acc: 0.8435\n",
      "Epoch [6/7] -> Train Acc: 0.8822 | Val Acc: 0.8474\n",
      "Epoch [7/7] -> Train Acc: 0.8922 | Val Acc: 0.8488\n",
      "Final Test Accuracy: 0.8461\n",
      "==================================================\n",
      "Running Experiment for vocab_size = 32000\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=nsmc_corpus.txt --model_prefix=nsmc_spm_unigram_32000 --vocab_size=32000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: nsmc_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: nsmc_spm_unigram_32000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 32000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: nsmc_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 145594 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=5380734\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1682\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 145594 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1890499\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 305522 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 145594\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 356666\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 356666 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=157275 obj=15.3978 num_tokens=840450 num_tokens/piece=5.34382\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=145558 obj=14.3645 num_tokens=845509 num_tokens/piece=5.80874\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109096 obj=14.4679 num_tokens=883090 num_tokens/piece=8.09461\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=108890 obj=14.4092 num_tokens=883713 num_tokens/piece=8.11565\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=81662 obj=14.6464 num_tokens=927916 num_tokens/piece=11.3629\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=81645 obj=14.5828 num_tokens=928059 num_tokens/piece=11.367\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=61233 obj=14.8449 num_tokens=971130 num_tokens/piece=15.8596\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=61233 obj=14.7805 num_tokens=971168 num_tokens/piece=15.8602\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=45924 obj=15.0731 num_tokens=1016035 num_tokens/piece=22.1243\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45924 obj=15.0072 num_tokens=1016025 num_tokens/piece=22.1241\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35200 obj=15.2991 num_tokens=1058373 num_tokens/piece=30.0674\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=35200 obj=15.2379 num_tokens=1058352 num_tokens/piece=30.0668\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: nsmc_spm_unigram_32000.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: nsmc_spm_unigram_32000.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BiLSTM (vocab_size=32000) 모델 학습 시작 ---\n",
      "Epoch [1/7] -> Train Acc: 0.6598 | Val Acc: 0.7437\n",
      "Epoch [2/7] -> Train Acc: 0.7980 | Val Acc: 0.8085\n",
      "Epoch [3/7] -> Train Acc: 0.8501 | Val Acc: 0.8236\n",
      "Epoch [4/7] -> Train Acc: 0.8757 | Val Acc: 0.8362\n",
      "Epoch [5/7] -> Train Acc: 0.9033 | Val Acc: 0.8407\n",
      "Epoch [6/7] -> Train Acc: 0.9219 | Val Acc: 0.8401\n",
      "Epoch [7/7] -> Train Acc: 0.9365 | Val Acc: 0.8397\n",
      "Final Test Accuracy: 0.8384\n",
      "--- 1D CNN (vocab_size=32000) 모델 학습 시작 ---\n",
      "Epoch [1/7] -> Train Acc: 0.6394 | Val Acc: 0.7346\n",
      "Epoch [2/7] -> Train Acc: 0.7562 | Val Acc: 0.7873\n",
      "Epoch [3/7] -> Train Acc: 0.8166 | Val Acc: 0.8142\n",
      "Epoch [4/7] -> Train Acc: 0.8521 | Val Acc: 0.8283\n",
      "Epoch [5/7] -> Train Acc: 0.8761 | Val Acc: 0.8372\n",
      "Epoch [6/7] -> Train Acc: 0.8937 | Val Acc: 0.8407\n",
      "Epoch [7/7] -> Train Acc: 0.9072 | Val Acc: 0.8430\n",
      "Final Test Accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "# 말뭉치 파일 생성\n",
    "corpus_file = 'nsmc_corpus.txt'\n",
    "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "    for doc in train_df_clean['document']:\n",
    "        f.write(doc + '\\n')\n",
    "\n",
    "results = {}\n",
    "vocab_sizes_to_test = [4000, 8000, 16000, 32000]\n",
    "model_type = 'unigram'\n",
    "\n",
    "for vocab_size in vocab_sizes_to_test:\n",
    "    print(f'{'='*50}')\n",
    "    print(f'Running Experiment for vocab_size = {vocab_size}')\n",
    "    print(f'{'='*50}')\n",
    "    \n",
    "    # 1. SentencePiece 모델 학습\n",
    "    model_prefix = f'nsmc_spm_{model_type}_{vocab_size}'\n",
    "    params = f'--input={corpus_file} --model_prefix={model_prefix} --vocab_size={vocab_size} --model_type={model_type}'\n",
    "    spm.SentencePieceTrainer.Train(params)\n",
    "    \n",
    "    # 2. 토크나이저 로드 및 데이터 준비\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(f'{model_prefix}.model')\n",
    "    \n",
    "    def sp_tokenize(s, corpus):\n",
    "        sequences = [s.EncodeAsIds(text) for text in corpus]\n",
    "        sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "        return pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "        \n",
    "    X_train = sp_tokenize(sp, train_df_clean['document'].tolist())\n",
    "    X_test = sp_tokenize(sp, test_df_clean['document'].tolist())\n",
    "    y_train = torch.tensor(train_df_clean['label'].values, dtype=torch.float32)\n",
    "    y_test = torch.tensor(test_df_clean['label'].values, dtype=torch.float32)\n",
    "    \n",
    "    val_size = 40000\n",
    "    X_val, y_val = X_train[:val_size], y_train[:val_size]\n",
    "    X_train_final, y_train_final = X_train[val_size:], y_train[val_size:]\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_final, y_train_final)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    batch_size = 512\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    # 3. 모델 학습 및 평가\n",
    "    if torch.backends.mps.is_available(): device = torch.device('mps')\n",
    "    else: device = torch.device('cpu')\n",
    "    \n",
    "    VOCAB_SIZE = sp.GetPieceSize()\n",
    "    EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM = 128, 128, 1\n",
    "    NUM_EPOCHS, LEARNING_RATE = 7, 1e-3\n",
    "    \n",
    "    # BiLSTM\n",
    "    bilstm_model = LSTMModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, padding_idx=0)\n",
    "    bilstm_acc = train_and_evaluate(bilstm_model, train_loader, val_loader, test_loader, NUM_EPOCHS, device, lr=LEARNING_RATE, model_name=f'BiLSTM (vocab_size={vocab_size})')\n",
    "    \n",
    "    # 1D CNN\n",
    "    N_FILTERS, FILTER_SIZES, DROPOUT = 100, [3,4,5], 0.5\n",
    "    cnn_model = CNNModel(VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "    cnn_acc = train_and_evaluate(cnn_model, train_loader, val_loader, test_loader, NUM_EPOCHS, device, lr=LEARNING_RATE, model_name=f'1D CNN (vocab_size={vocab_size})')\n",
    "    \n",
    "    results[vocab_size] = {'BiLSTM_Accuracy': bilstm_acc, 'CNN_Accuracy': cnn_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 최종 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BiLSTM_Accuracy  CNN_Accuracy\n",
      "Vocab Size                               \n",
      "4000               0.844882      0.847131\n",
      "8000               0.844518      0.847171\n",
      "16000              0.844011      0.846098\n",
      "32000              0.838421      0.838198\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results).T\n",
    "results_df.index.name = 'Vocab Size'\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goingdeeper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

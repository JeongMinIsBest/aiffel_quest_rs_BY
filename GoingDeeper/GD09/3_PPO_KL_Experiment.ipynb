{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886d4012",
   "metadata": {},
   "source": [
    "# 3. PPO KL 페널티 비교 실험\n",
    "\n",
    "이 노트북은 사전 학습된 SFT, RM 모델을 불러와 PPO 단계만 실행합니다.\n",
    "KL 페널티 계수(`kl_coef`) 리스트 `[0.0, 0.1, 0.5]`에 대해 순차적으로 학습과 평가를 모두 수행하고, 마지막에 결과를 하나의 표로 요약합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ca3a9",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트 및 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21767de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import wandb\n",
    "from typing import Dict, List\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cedf0be",
   "metadata": {},
   "source": [
    "## 2. 실험 파라미터 및 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== 실험할 KL 페널티 리스트 ===========\n",
    "KL_VALUES = [0.0, 0.1, 0.5]\n",
    "# ======================================================\n",
    "\n",
    "# 불러올 SFT/RM 모델의 버전 (v2.5 사용)\n",
    "BASE_VERSION_NAME = 'v2.5' \n",
    "\n",
    "# PPO 파라미터\n",
    "PPO_NUM_EPISODES = 100\n",
    "PPO_MAX_EPOCHS = 3\n",
    "\n",
    "# 모델 및 데이터 경로\n",
    "BASE_MODEL_NAME = 'skt/kogpt2-base-v2'\n",
    "SFT_MODEL_NAME = f'models/sft_output_model_{BASE_VERSION_NAME}'\n",
    "RM_MODEL_NAME = f'models/rm_output_model_{BASE_VERSION_NAME}'\n",
    "\n",
    "DATA_PATH_3_PPO = f'data/kochatgpt_3_PPO_{BASE_VERSION_NAME}.jsonl'\n",
    "\n",
    "# 토크나이저 및 PPO 프롬프트 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_NAME, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512\n",
    ")\n",
    "\n",
    "with open(DATA_PATH_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "print(f\"실험할 KL 계수: {KL_VALUES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt.trainer.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# wandb 로깅과 loss 기록을 위한 Custom Callback 정의\n",
    "class WandbPlottingCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "\n",
    "    def on_learn_batch_end(self, metrics: dict, experience: \"Experience\") -> None:\n",
    "        # 학습 스텝이 끝날 때마다 actor와 critic의 loss를 wandb에 기록하고 리스트에 저장\n",
    "        wandb.log(metrics)\n",
    "        if 'actor_loss' in metrics:\n",
    "            self.actor_losses.append(metrics['actor_loss'])\n",
    "        if 'critic_loss' in metrics:\n",
    "            self.critic_losses.append(metrics['critic_loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f954d3",
   "metadata": {},
   "source": [
    "## 3. PPO 실험 루프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204425a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 딕셔너리 초기화\n",
    "all_results = {}\n",
    "all_actor_logs = {}\n",
    "all_critic_logs = {}\n",
    "\n",
    "# --- 데이터셋에서 평가를 위한 프롬프트 100개 로드 ---\n",
    "with open(DATA_PATH_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "if len(list_data_dict) > 100:\n",
    "    test_prompts = [item['prompt'] for item in list_data_dict[-100:]]\n",
    "else:\n",
    "    test_prompts = [item['prompt'] for item in list_data_dict]\n",
    "\n",
    "print(f\"총 {len(test_prompts)}개의 프롬프트로 평가를 진행합니다.\")\n",
    "\n",
    "PROMPT_DICT = {\"prompt_input\": (\"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\")}\n",
    "formatted_test_prompts = [PROMPT_DICT['prompt_input'].format_map({'prompt': p}) for p in test_prompts]\n",
    "\n",
    "def generation(input_text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(torch.cuda.current_device())\n",
    "    outputs = model.model.generate(input_ids, max_length=250, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1)\n",
    "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    prompt_part = input_text.split(\"### Response(응답):\")[0] + \"### Response(응답):\"\n",
    "    return output_text.replace(prompt_part, \"\").strip()\n",
    "\n",
    "# --- 실험 루프 시작 ---\n",
    "for kl_coef in KL_VALUES:\n",
    "    print(f\"\\n======================================================\")\n",
    "    print(f\"=== KL_COEF = {kl_coef} 실험 시작 ===\")\n",
    "    print(f\"======================================================\\n\")\n",
    "\n",
    "    with NaiveStrategy().model_init_context():\n",
    "        actor = GPTActor(pretrained=SFT_MODEL_NAME, lora_rank=16).to(torch.cuda.current_device())\n",
    "        critic = GPTCritic(pretrained=RM_MODEL_NAME, lora_rank=8).to(torch.cuda.current_device())\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())\n",
    "\n",
    "    actor_optim = torch.optim.Adam(actor.parameters(), lr=5e-6)\n",
    "    critic_optim = torch.optim.Adam(critic.parameters(), lr=5e-6)\n",
    "\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "        (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)\n",
    "\n",
    "    ppo_run_name = f'ppo_kl_{kl_coef}_{BASE_VERSION_NAME}'\n",
    "    wandb.init(project=\"kochatgpt_tuning_kl_exp\", name=ppo_run_name, reinit=True)\n",
    "\n",
    "    # 콜백 인스턴스 생성\n",
    "    wandb_plot_callback = WandbPlottingCallback()\n",
    "\n",
    "    trainer = PPOTrainer(\n",
    "        NaiveStrategy(), actor, critic, reward_model, initial_model, actor_optim, critic_optim,\n",
    "        max_epochs=PPO_MAX_EPOCHS, train_batch_size=8, tokenizer=tokenize_fn, max_length=128,\n",
    "        do_sample=True, temperature=1.0, top_k=50, kl_coef=kl_coef,\n",
    "        pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,\n",
    "        callbacks=[wandb_plot_callback]  # 콜백 추가\n",
    "    )\n",
    "\n",
    "    trainer.fit(list_prompt, num_episodes=PPO_NUM_EPISODES, max_timesteps=3, update_timesteps=3)\n",
    "\n",
    "    # 학습 기록 저장\n",
    "    all_actor_logs[kl_coef] = wandb_plot_callback.actor_losses\n",
    "    all_critic_logs[kl_coef] = wandb_plot_callback.critic_losses\n",
    "\n",
    "    print(f\"\\n--- KL={kl_coef} 모델 생성 결과 ---\")\n",
    "    current_results = []\n",
    "    for p, fp in zip(test_prompts, formatted_test_prompts):\n",
    "        output = generation(fp, actor, tokenizer)\n",
    "        current_results.append(output)\n",
    "    all_results[kl_coef] = current_results\n",
    "\n",
    "    wandb.finish()\n",
    "    del actor, critic, initial_model, reward_model, trainer, actor_optim, critic_optim\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b221ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Actor/Critic Loss 비교 그래프\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Actor Loss 그래프\n",
    "for kl_coef in KL_VALUES:\n",
    "    logs = all_actor_logs.get(kl_coef, [])\n",
    "    if logs:\n",
    "        steps = range(len(logs))\n",
    "        ax1.plot(steps, logs, label=f'Actor Loss (kl={kl_coef})')\n",
    "ax1.set_title('PPO Actor Loss Comparison')\n",
    "ax1.set_xlabel('Training Steps')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Critic Loss 그래프\n",
    "for kl_coef in KL_VALUES:\n",
    "    logs = all_critic_logs.get(kl_coef, [])\n",
    "    if logs:\n",
    "        steps = range(len(logs))\n",
    "        ax2.plot(steps, logs, label=f'Critic Loss (kl={kl_coef})')\n",
    "ax2.set_title('PPO Critic Loss Comparison')\n",
    "ax2.set_xlabel('Training Steps')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a410526",
   "metadata": {},
   "source": [
    "## 4. 최종 비교 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04172b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 마크다운 테이블 생성\n",
    "# 동적으로 헤더 생성\n",
    "header = '| Prompt | ' + ' | '.join([f'PPO (kl={kl})' for kl in KL_VALUES]) + ' |'\n",
    "separator = '| :--- | ' + ' | '.join([':---' for _ in KL_VALUES]) + ' |'\n",
    "markdown_table = f'{header}\\n{separator}'\n",
    "\n",
    "# test_prompts는 실험 루프 셀에서 이미 정의되어 있다고 가정합니다.\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    # 프롬프트를 안전하게 처리하고 행 시작\n",
    "    safe_prompt = prompt.replace('|', '\\|')\n",
    "    row = f'| **{safe_prompt}** '\n",
    "\n",
    "    # 각 kl_coef 값에 대한 결과를 행에 추가\n",
    "    for kl in KL_VALUES:\n",
    "        # 결과 딕셔너리에서 결과 가져오기\n",
    "        output = all_results.get(kl, [''] * len(test_prompts))[i]\n",
    "        # 줄바꿈 문자를 HTML <br> 태그로 변경하여 셀 내 줄바꿈 처리\n",
    "        output_html = output.replace('\\n', '<br>')\n",
    "        row += f'| {output_html} '\n",
    "\n",
    "    markdown_table += f'\\n{row}|'\n",
    "\n",
    "display(Markdown(markdown_table))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

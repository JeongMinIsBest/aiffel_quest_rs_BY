{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Ko-En) 번역기 v2.0\n",
    "---\n",
    "### 프로젝트 목표\n",
    "한국어-영어 번역을 위한 트랜스포머 모델에 메캅 형태소 분석 추가\n",
    "\n",
    "**주요 변경 사항 (v1.3 대비):**\n",
    "1. 데이터 증강: NLLB 모델을 활용한 역번역(Back-translation) 기법을 도입하여 학습 데이터의 양과 질을 대폭 향상.\n",
    "2. 토크나이저: 한국어 토큰화 시, `Mecab` 형태소 분석기를 `SentencePiece` 이전에 적용하여 언어적 특성 반영을 강화.\n",
    "3. 훈련 방식 업그레이드:\n",
    "    - 옵티마이저: Adam -> AdamW로 변경하고 `weight_decay`를 적용하여 정규화 성능 개선.\n",
    "    - 손실 함수: `Label Smoothing`을 적용하여 모델의 과신을 방지하고 일반화 성능 향상.\n",
    "    - 학습률 스케줄러: `역제곱근 감쇠` 방식 (All You Need is Attention 논문 방식)-> '코사인 어닐링' 방식으로 변경하여 더 안정적인 수렴 유도.\n",
    "4. 평가 방식: 빔 서치(Beam Search) 디코딩을 기본으로 사용하고, BLEU, METEOR, ROUGE, BERTScore를 모두 측정하는 종합 평가 파이프라인 구축."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "# !pip install konlpy\n",
    "# !pip install rouge-score bert-score\n",
    "# !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "# %cd Mecab-ko-for-Google-Colab/\n",
    "# !bash install_mecab-ko_on_colab_light_220429.sh\n",
    "\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import locale\n",
    "\n",
    "# 데이터 처리 및 연산\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline\n",
    "\n",
    "# 자연어 처리(NLP) 및 머신러닝\n",
    "import sentencepiece as spm\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# 시각화 및 진행률 표시\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 14\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"모든 랜덤 시드를 고정하여 재현성을 보장합니다.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
    "\n",
    "# 사용할 시드 값 설정\n",
    "SEED = 14\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 하이퍼파라미터 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "SRC_VOCAB_SIZE = 18838\n",
    "TGT_VOCAB_SIZE = 18838\n",
    "D_MODEL = 512\n",
    "N_LAYERS = 4\n",
    "N_HEADS = 8\n",
    "D_FF = 4096\n",
    "DROPOUT = 0.15\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "CHECKPOINT_PATH = \"transformer-2.0-checkpoint.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 준비 및 전처리 & 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 94123, Dev: 1000, Test: 2000\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 경로 설정\n",
    "data_dir = 'data'\n",
    "train_kor_path = os.path.join(data_dir, 'korean-english-park.train.ko')\n",
    "train_eng_path = os.path.join(data_dir, 'korean-english-park.train.en')\n",
    "dev_kor_path = os.path.join(data_dir, 'korean-english-park.dev.ko')\n",
    "dev_eng_path = os.path.join(data_dir, 'korean-english-park.dev.en')\n",
    "test_kor_path = os.path.join(data_dir, 'korean-english-park.test.ko')\n",
    "test_eng_path = os.path.join(data_dir, 'korean-english-park.test.en')\n",
    "\n",
    "# 2. 원본 데이터 로딩\n",
    "with open(train_kor_path, \"r\", encoding='utf-8') as f: train_kor_raw = f.read().splitlines()\n",
    "with open(train_eng_path, \"r\", encoding='utf-8') as f: train_eng_raw = f.read().splitlines()\n",
    "with open(dev_kor_path, \"r\", encoding='utf-8') as f: dev_kor_raw = f.read().splitlines()\n",
    "with open(dev_eng_path, \"r\", encoding='utf-8') as f: dev_eng_raw = f.read().splitlines()\n",
    "with open(test_kor_path, \"r\", encoding='utf-8') as f: test_kor_raw = f.read().splitlines()\n",
    "with open(test_eng_path, \"r\", encoding='utf-8') as f: test_eng_raw = f.read().splitlines()\n",
    "\n",
    "print(f\"Train: {len(train_kor_raw)}, Dev: {len(dev_kor_raw)}, Test: {len(test_kor_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cleaning Train Dataset ---\n",
      "Original: 94123 pairs\n",
      "After filtering: 93912 pairs (211 pairs rejected)\n",
      "\n",
      "--- Cleaning Dev Dataset ---\n",
      "Original: 1000 pairs\n",
      "After filtering: 999 pairs (1 pairs rejected)\n",
      "\n",
      "--- Cleaning Test Dataset ---\n",
      "Original: 2000 pairs\n",
      "After filtering: 1996 pairs (4 pairs rejected)\n",
      "\n",
      "--- Final Corpus Sizes ---\n",
      "Train: 93912, Dev: 999, Test: 1996\n",
      "\n",
      "==================================================\n",
      "         제거된 데이터 샘플 (최대 10개)         \n",
      "==================================================\n",
      "\n",
      "Sample 1:\n",
      "  - Reason: Keyword '어휘 :'\n",
      "  - KO: 어휘 :\n",
      "  - EN: The Geneva-based commission, in its annual study of the industry titled “World Robotics 2001,” said a record 100,000 robots were installed last year, up 25 percent on 1999.\n",
      "\n",
      "Sample 2:\n",
      "  - Reason: Keyword '어휘 :'\n",
      "  - KO: 어휘 :\n",
      "  - EN: Postal Service - whose postmaster told a Senate panel that the financial impact of the anthrax crisis could be several billion dollars - uses robots to sort parcels, but other automated equipment sorts letters.\n",
      "\n",
      "Sample 3:\n",
      "  - Reason: Keyword '어휘 :'\n",
      "  - KO: 어휘 :\n",
      "  - EN: the United States will take every measure against what is perhaps greatest danger of all that may result from hostile states or terrorist groups armed with weapons of mass destruction,\" Bush said in a written statement.\n",
      "\n",
      "Sample 4:\n",
      "  - Reason: Keyword '번역 :'\n",
      "  - KO: Army 미국 육군  / shoot down 을 쏘아 떨어뜨리다  / artillery 대포  / shell 포탄  / in mid-flight 비행 중인  / defense industry 국방 산업  / breakthrough (발명에 의한) 비약적 전진, 그 발명  / manufacturer 제조업체 번역 :\n",
      "  - EN: Army used a high-energy laser to shoot down an artillery shell in mid-flight in a defense industry breakthrough, the Army and the manufacturer said.\n",
      "\n",
      "Sample 5:\n",
      "  - Reason: Keyword '어휘 :'\n",
      "  - KO: 어휘 : examine 면밀히 조사하다   plight 상태, 양상   revive 회복시키다\n",
      "  - EN: In our Focus report, Bob Dody examines the plight of U.S. cities in the 1990s and what's being done to revive them.\n",
      "\n",
      "Sample 6:\n",
      "  - Reason: Keyword '번역 :'\n",
      "  - KO: general 일반적인, 전반에 걸치는   shortage 부족   skilled labor 숙련된 노동력   workshop 작업장, 공장   mine 광산, 광업소 번역 :\n",
      "  - EN: During the second half of the 14th century, / there was a general shortage of skilled labor / for the workshops and the mines.\n",
      "\n",
      "Sample 7:\n",
      "  - Reason: Keyword '번역 :'\n",
      "  - KO: it는 to buy this product를 받는다. 번역 :\n",
      "  - EN: What causes or motivates the shopper / to buy this product?\n",
      "\n",
      "Sample 8:\n",
      "  - Reason: Keyword '어휘 :'\n",
      "  - KO: 어휘 : psychologist 심리학자   tactful 재치있는   no less 못지않게   discouraging 비관적인, 가망이 희박한   give  a label  에 꼬리표를 부치다   have a cure 치료를 받다\n",
      "  - EN: Kenneth Gergen, a Swarthmore College professor and psychologist, is a little more tactful but no less discouraging.\n",
      "\n",
      "Sample 9:\n",
      "  - Reason: Keyword '번역 :'\n",
      "  - KO: 비록 오늘날의 전쟁터에서는 여성들도 사상자들 중에 포함되기는 하지만 여성들은 여전히 직접적인 전투 임무에서 배제되고 있다. 번역 :\n",
      "  - EN: Women are still excluded / from direct combat roles, / although women would be among the casualties / on today's battlefield.\n",
      "\n",
      "Sample 10:\n",
      "  - Reason: Keyword '번역 :'\n",
      "  - KO: first of all 무엇보다도   hard 힘든   physical work 육체 노동   a way of life 생활의 방편 번역 :\n",
      "  - EN: First of all, / hard physical work is a way of life / for all of these long-lived peoples.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"기존의 기본 정제 함수\"\"\"\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # 숫자 보존\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9가-힣?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def clean_and_filter_corpus_with_rejection_sampling(kor_raw, eng_raw):\n",
    "    \"\"\"\n",
    "    데이터를 필터링하고, 제거된 샘플과 그 이유를 반환하는 새로운 클리닝 함수.\n",
    "    \"\"\"\n",
    "    print(f\"Original: {len(kor_raw)} pairs\")\n",
    "\n",
    "    filtered_kor, filtered_eng = [], []\n",
    "    rejected_samples = []  # 제거된 샘플을 저장할 리스트\n",
    "\n",
    "    filter_keywords = ['번역 :', '어휘 :', 'http', '▶']\n",
    "\n",
    "    for ko_sent, en_sent in zip(kor_raw, eng_raw):\n",
    "        rejection_reason = None\n",
    "\n",
    "        # 규칙 1 & 2: 키워드 및 URL/특수기호\n",
    "        for keyword in filter_keywords:\n",
    "            if keyword in ko_sent or keyword in en_sent:\n",
    "                rejection_reason = f\"Keyword '{keyword}'\"\n",
    "                break\n",
    "        if rejection_reason:\n",
    "            rejected_samples.append((rejection_reason, ko_sent, en_sent))\n",
    "            continue\n",
    "\n",
    "        # 규칙 3: 사전 형식 데이터 (세미콜론 2개 초과)\n",
    "        if ko_sent.count(';') > 2 or en_sent.count(';') > 2:\n",
    "            rejection_reason = \"Too many semicolons\"\n",
    "            rejected_samples.append((rejection_reason, ko_sent, en_sent))\n",
    "            continue\n",
    "\n",
    "        # 모든 필터링 규칙을 통과한 경우\n",
    "        filtered_kor.append(ko_sent)\n",
    "        filtered_eng.append(en_sent)\n",
    "\n",
    "    print(f\"After filtering: {len(filtered_kor)} pairs ({len(rejected_samples)} pairs rejected)\")\n",
    "\n",
    "    # 최종 정제\n",
    "    cleaned_kor_corpus = [preprocess_sentence(html.unescape(s)) for s in filtered_kor]\n",
    "    cleaned_eng_corpus = [preprocess_sentence(html.unescape(s)) for s in filtered_eng]\n",
    "\n",
    "    return cleaned_kor_corpus, cleaned_eng_corpus, rejected_samples\n",
    "\n",
    "\n",
    "# --- 새로운 클리닝 함수를 사용하여 전체 데이터셋 재처리 ---\n",
    "print(\"--- Cleaning Train Dataset ---\")\n",
    "train_kor_corpus, train_eng_corpus, rejected_train = clean_and_filter_corpus_with_rejection_sampling(train_kor_raw, train_eng_raw)\n",
    "print(\"\\n--- Cleaning Dev Dataset ---\")\n",
    "dev_kor_corpus, dev_eng_corpus, rejected_dev = clean_and_filter_corpus_with_rejection_sampling(dev_kor_raw, dev_eng_raw)\n",
    "print(\"\\n--- Cleaning Test Dataset ---\")\n",
    "test_kor_corpus, test_eng_corpus, rejected_test = clean_and_filter_corpus_with_rejection_sampling(test_kor_raw, test_eng_raw)\n",
    "\n",
    "print(\"\\n--- Final Corpus Sizes ---\")\n",
    "print(f\"Train: {len(train_kor_corpus)}, Dev: {len(dev_kor_corpus)}, Test: {len(test_kor_corpus)}\")\n",
    "\n",
    "\n",
    "# --- 제거된 샘플 출력 ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"         제거된 데이터 샘플 (최대 10개)         \")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 훈련 데이터에서 제거된 샘플만 확인\n",
    "total_rejected = rejected_train + rejected_dev + rejected_test\n",
    "if not total_rejected:\n",
    "    print(\"제거된 데이터가 없습니다.\")\n",
    "else:\n",
    "    for i, (reason, ko, en) in enumerate(total_rejected[:10]):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  - Reason: {reason}\")\n",
    "        print(f\"  - KO: {ko}\")\n",
    "        print(f\"  - EN: {en}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained Korean Word2Vec model from: data/ko.vec\n",
      "Model loaded successfully.\n",
      "Starting data augmentation with pre-trained ko.vec model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318e5f0361b34b72a7f225e5c92f0272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmenting sentences:   0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation complete. 39009 sentences generated.\n",
      "\n",
      "--- Augmentation Samples (with ko.vec Model) ---\n",
      "Sample 1:\n",
      "  - Original KO:    현지언론은 지난해 10월 14일 히로시마에 있는 한 식당에서 미군 4명이 피해 여성을 만나 인근 공원에 주차시킨 차로 유인해 성폭행한 사건이 발생했다고 보도했다 .\n",
      "  - Augmented KO:   현지언론은 이번 10월 14일 히로시마에 있는 한 식당에서 미군 4명이 피해 여성을 만나 인근 공원에 주차시킨 차로 유인해 성폭행한 사건이 발생했다고 보도했다 .\n",
      "  - Corresponding EN: local media reported that the four men met the woman in a restaurant in hiroshima on october 14 , 2007 , then allegedly attacked and raped her in a car in nearby parking lot .\n",
      "Sample 2:\n",
      "  - Original KO:    엘든은 페어리가 자신이 라디오방송과 인터뷰하는 내용을 듣게 됐고 그렇게 하다가 그의 인턴이 됐다 고 밝혔다 .\n",
      "  - Augmented KO:   엘든은 페어리가 자신이 라디오방송과 인터뷰하는 내용을 듣게 됐고 그렇 하다가 그의 인턴이 됐다 고 밝혔다 .\n",
      "  - Corresponding EN: fairey heard elden interviewed on the radio and one thing led to another , said the teen .\n",
      "Sample 3:\n",
      "  - Original KO:    또한 3대 자동차 회사에는 미국 내 딜러가 1만4000명이고 딜러가 고용한 직원수도 74만명에 달한다 .\n",
      "  - Augmented KO:   물론 3대 자동차 회사에는 미국 내 딜러가 1만4000명이고 딜러가 고용한 직원수도 74만명에 달한다 .\n",
      "  - Corresponding EN: in addition , the three automakers have about 14 , 000 u . s . dealerships that between them employ another 740 , 000 workers .\n",
      "Sample 4:\n",
      "  - Original KO:    러시아 제일의 지명 수배자인 체첸 반군 지도자 샤밀 바사예프가 잉구세티아 공화국 남부에서 있은 트럭 폭발 사고로 사망했다 .\n",
      "  - Augmented KO:   러시아 제일의 지명 수배자인 체첸 반군 지도자 샤밀 바사예프가 잉구세티아 공화국 남부에서 있은 트럭 폭발 사고로 사망했다 동양사\n",
      "  - Corresponding EN: russia s fsb security service chief , nikolai patrushev , said basayev was killed in a special operation .\n",
      "Sample 5:\n",
      "  - Original KO:    보트가 양어장에 다가갔을 때 모든 연어가 해파리 촉수에 찔리고 스트레스를 받아 죽거나 죽어가고 있었다 .\n",
      "  - Augmented KO:   보트가 양어장에 다가갔을 때 몇몇 연어가 해파리 촉수에 찔리고 스트레스를 받아 죽거나 죽어가고 있었다 .\n",
      "  - Corresponding EN: all the fish were dead or dying from stings and stress by the time the boats reached the pens , he said .\n",
      "\n",
      "Total training sentences after augmentation: 132921\n"
     ]
    }
   ],
   "source": [
    "# 1. 사전 학습된 한국어 Word2Vec 모델 로드 (.vec 텍스트 파일)\n",
    "model_path = 'data/ko.vec' # .vec 파일 경로로 수정\n",
    "print(f\"Loading pre-trained Korean Word2Vec model from: {model_path}\")\n",
    "try:\n",
    "    # .vec 파일은 텍스트 파일이므로 binary=False로 설정합니다.\n",
    "    wv = KeyedVectors.load_word2vec_format(model_path, binary=False, unicode_errors='ignore')\n",
    "    print(\"Model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Model file not found at '{model_path}'. Please check the path.\")\n",
    "    wv = KeyedVectors(200)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Model file could not be loaded. Error: {e}\")\n",
    "    wv = KeyedVectors(200)\n",
    "\n",
    "\n",
    "def augment_with_pretrained_wv(kor_corpus, eng_corpus, wv, num_augmented_sentences=40000):\n",
    "    \"\"\"\n",
    "    사전 학습된 임베딩 모델(gensim KeyedVectors)을 사용한 Lexical Substitution으로 데이터를 증강합니다.\n",
    "    \"\"\"\n",
    "    print(\"Starting data augmentation with pre-trained ko.vec model...\")\n",
    "\n",
    "    augmented_kor, augmented_eng = [], []\n",
    "    original_indices = list(range(len(kor_corpus)))\n",
    "    indices_to_augment = random.choices(original_indices, k=num_augmented_sentences)\n",
    "    successful_augment_indices = []\n",
    "\n",
    "    for sent_idx in tqdm(indices_to_augment, desc=\"Augmenting sentences\"):\n",
    "        original_sentence = kor_corpus[sent_idx]\n",
    "        tokens = original_sentence.split()\n",
    "\n",
    "        # gensim KeyedVectors 객체에 단어 포함 여부 확인\n",
    "        valid_tokens = [tok for tok in tokens if tok in wv.key_to_index]\n",
    "\n",
    "        if not valid_tokens:\n",
    "            continue\n",
    "\n",
    "        target_word = random.choice(valid_tokens)\n",
    "\n",
    "        try:\n",
    "            similar_words = wv.most_similar(target_word, topn=5)\n",
    "            synonym = random.choice(similar_words)[0]\n",
    "            new_kor_sentence = \" \".join([synonym if tok == target_word else tok for tok in tokens])\n",
    "\n",
    "            augmented_kor.append(new_kor_sentence)\n",
    "            augmented_eng.append(eng_corpus[sent_idx])\n",
    "            successful_augment_indices.append(sent_idx)\n",
    "\n",
    "        except (KeyError, IndexError):\n",
    "            continue\n",
    "\n",
    "    print(f\"Augmentation complete. {len(augmented_kor)} sentences generated.\")\n",
    "    return augmented_kor, augmented_eng, successful_augment_indices\n",
    "\n",
    "# 데이터 증강 실행 (40,000개)\n",
    "augmented_kor_corpus, augmented_eng_corpus, augmented_indices = augment_with_pretrained_wv(\n",
    "    train_kor_corpus, train_eng_corpus, wv, num_augmented_sentences=40000\n",
    "    )\n",
    "\n",
    "# --- 증강 샘플 출력 ---\n",
    "print(\"\\n--- Augmentation Samples (with ko.vec Model) ---\")\n",
    "num_samples_to_print = 5\n",
    "if not augmented_kor_corpus:\n",
    "    print(\"No sentences were successfully augmented.\")\n",
    "else:\n",
    "    for i in range(min(num_samples_to_print, len(augmented_kor_corpus))):\n",
    "        original_idx = augmented_indices[i]\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f'  - Original KO:    {train_kor_corpus[original_idx]}')\n",
    "        print(f'  - Augmented KO:   {augmented_kor_corpus[i]}')\n",
    "        print(f'  - Corresponding EN: {train_eng_corpus[original_idx]}')\n",
    "# --- 샘플 출력 끝 ---\n",
    "\n",
    "# 기존 훈련 데이터에 증강된 데이터 추가\n",
    "train_kor_corpus.extend(augmented_kor_corpus)\n",
    "train_eng_corpus.extend(augmented_eng_corpus)\n",
    "\n",
    "print(f'\\nTotal training sentences after augmentation: {len(train_kor_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 개인용 컴퓨터 사용의 상당 부분은 이것보다 뛰어날 수 있느냐 ?\n",
      "After : 개인 용 컴퓨터 사용 의 상당 부분 은 이것 보다 뛰어날 수 있 느냐 ?\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "# 1. Mecab 초기화\n",
    "mecab = Mecab()\n",
    "def mecab_tokenize_corpus(corpus):\n",
    "    mecab_corpus = []\n",
    "    for sentence in corpus:\n",
    "        # 형태소 분리 후 공백으로 join\n",
    "        morphs = mecab.morphs(sentence)\n",
    "        mecab_corpus.append(\" \".join(morphs))\n",
    "    return mecab_corpus\n",
    "\n",
    "# 2. 한국어 데이터셋 Mecab 처리\n",
    "train_kor_mecab = mecab_tokenize_corpus(train_kor_corpus)\n",
    "dev_kor_mecab   = mecab_tokenize_corpus(dev_kor_corpus)\n",
    "test_kor_mecab  = mecab_tokenize_corpus(test_kor_corpus)\n",
    "\n",
    "print(\"Before:\", train_kor_corpus[0])\n",
    "print(\"After :\", train_kor_mecab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./ko_corpus.txt --model_prefix=ko_spm --vocab_size=18838 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./ko_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ko_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 18838\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ./ko_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 132920 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=10018504\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1173\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 132920 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=5381426\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 39183 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 132920\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 52846\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 52846 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=29401 obj=8.84341 num_tokens=101963 num_tokens/piece=3.46801\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=23027 obj=8.32206 num_tokens=102261 num_tokens/piece=4.44092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19087 obj=8.24976 num_tokens=102259 num_tokens/piece=5.35752\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=18995 obj=8.23903 num_tokens=102445 num_tokens/piece=5.39326\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ko_spm.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ko_spm.vocab\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./en_corpus.txt --model_prefix=en_spm --vocab_size=18838 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./en_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: en_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 18838\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ./en_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 132893 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=17205963\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9887% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=39\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999887\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 132893 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=10773863\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 83615 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 132893\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 46090\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 46090 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=38531 obj=9.89455 num_tokens=86251 num_tokens/piece=2.23848\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=27869 obj=8.04617 num_tokens=86389 num_tokens/piece=3.09982\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20898 obj=7.96637 num_tokens=89321 num_tokens/piece=4.27414\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20882 obj=7.9452 num_tokens=89374 num_tokens/piece=4.27995\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20620 obj=7.94304 num_tokens=89481 num_tokens/piece=4.33952\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20617 obj=7.94127 num_tokens=89567 num_tokens/piece=4.34433\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: en_spm.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: en_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(corpus, vocab_size, lang, pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    file = f'./{lang}_corpus.txt'\n",
    "    model_prefix = f'{lang}_spm'\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={file} --model_prefix={model_prefix} --vocab_size={vocab_size}' + \n",
    "        f' --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}'\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'{model_prefix}.model')\n",
    "    return tokenizer\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(train_kor_mecab, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(train_eng_corpus, TGT_VOCAB_SIZE, \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터셋 및 DataLoader 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 2077\n",
      "Number of batches in valid_loader: 16\n",
      "Number of batches in test_loader: 32\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_corpus, tgt_corpus, src_tokenizer, tgt_tokenizer):\n",
    "        self.src_corpus = src_corpus\n",
    "        self.tgt_corpus = tgt_corpus\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_tokenizer.encode_as_ids(self.src_corpus[idx])\n",
    "        tgt = self.tgt_tokenizer.encode_as_ids(self.tgt_corpus[idx])\n",
    "\n",
    "        # 텐서의 데이터 타입을 torch.long으로 명시적으로 지정합니다.\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"배치 내의 시퀀스들을 패딩하여 동일한 길이로 만듭니다.\"\"\"\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=ko_tokenizer.pad_id())\n",
    "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=en_tokenizer.pad_id())\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# Dataset 및 DataLoader 인스턴스 생성\n",
    "train_dataset = TranslationDataset(train_kor_mecab, train_eng_corpus, ko_tokenizer, en_tokenizer)\n",
    "valid_dataset = TranslationDataset(dev_kor_mecab, dev_eng_corpus, ko_tokenizer, en_tokenizer)\n",
    "test_dataset = TranslationDataset(test_kor_mecab, test_eng_corpus, ko_tokenizer, en_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in valid_loader: {len(valid_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 트랜스포머 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    입력 임베딩에 위치 정보를 추가하는 클래스입니다.\n",
    "    Transformer 모델은 순서 정보가 없으므로, 토큰의 위치를 알려주기 위해 sin/cos 함수를 사용합니다.\n",
    "    이 방식은 고정 위치 인코딩으로, 학습되지 않는 파라미터(buffer)로 등록됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # sin/cos 함수에 사용할 div_term 계산: 주파수 조절을 위한 값\n",
    "        div_term = torch.exp(torch.arange(0, emb_size, 2) * (-math.log(10000.0) / emb_size))\n",
    "        # 각 위치(0~maxlen)에 대한 인덱스 생성\n",
    "        position = torch.arange(maxlen).unsqueeze(1)\n",
    "        # 위치 임베딩 행렬 초기화 (maxlen, emb_size)\n",
    "        pos_embedding = torch.zeros(maxlen, emb_size)\n",
    "        # 짝수 인덱스: sin 함수 적용\n",
    "        pos_embedding[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 홀수 인덱스: cos 함수 적용\n",
    "        pos_embedding[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 배치 차원 추가 (1, maxlen, emb_size)\n",
    "        pos_embedding = pos_embedding.unsqueeze(0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 학습되지 않는 파라미터로 등록\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_embedding: (batch_size, seq_len, emb_size)\n",
    "        Returns:\n",
    "            token_embedding + pos_embedding: 위치 정보가 더해진 임베딩\n",
    "        \"\"\"\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:, :token_embedding.size(1), :])\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    다중 헤드 어텐션 메커니즘을 구현한 클래스.\n",
    "    쿼리, 키, 값 행렬을 여러 헤드로 분할하여 병렬로 어텐션을 계산하고, 결과를 결합합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads  # 각 헤드의 차원\n",
    "        # 쿼리, 키, 값 행렬을 위한 선형 변환 레이어\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        # 최종 출력 선형 변환 레이어\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        스케일드 닷-프로덕트 어텐션 계산.\n",
    "        Args:\n",
    "            Q: 쿼리 행렬\n",
    "            K: 키 행렬\n",
    "            V: 값 행렬\n",
    "            mask: 어텐션 마스크 (선택적)\n",
    "        Returns:\n",
    "            out: 어텐션 가중치 적용된 값 행렬\n",
    "            attentions: 어텐션 가중치 행렬\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        QK = torch.matmul(Q, K.transpose(-1, -2))  # QK^T 계산\n",
    "        scaled_qk = QK / math.sqrt(d_k)  # 스케일링\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)  # 마스크 적용 (매우 작은 값 더하기)\n",
    "        attentions = nn.Softmax(dim=-1)(scaled_qk)  # 소프트맥스 적용\n",
    "        out = torch.matmul(attentions, V)  # 가중치 적용\n",
    "        return out, attentions\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        입력 텐서를 여러 헤드로 분할.\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x: (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        bsz, seq_len, _ = x.size()\n",
    "        x = x.view(bsz, seq_len, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)  # 차원 재배치\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 헤드를 다시 결합.\n",
    "        Args:\n",
    "            x: (batch_size, num_heads, seq_len, depth)\n",
    "        Returns:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        bsz, _, seq_len, _ = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return x.view(bsz, seq_len, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: 쿼리 입력 (batch_size, seq_len, d_model)\n",
    "            K: 키 입력\n",
    "            V: 값 입력\n",
    "            mask: 어텐션 마스크\n",
    "        Returns:\n",
    "            out: 어텐션 적용된 출력\n",
    "            attention_weights: 어텐션 가중치\n",
    "        \"\"\"\n",
    "        # 헤드 분할 후 어텐션 계산\n",
    "        WQ = self.split_heads(self.W_q(Q))\n",
    "        WK = self.split_heads(self.W_k(K))\n",
    "        WV = self.split_heads(self.W_v(V))\n",
    "        out, attention_weights = self.scaled_dot_product_attention(WQ, WK, WV, mask)\n",
    "        # 헤드 결합 후 선형 변환\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        return out, attention_weights\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "    포지션 와이즈 피드포워드 네트워크.\n",
    "    각 위치별로 독립적으로 적용되는 2층 완전 연결 네트워크 (ReLU 활성화 함수 사용).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)  # 첫 번째 레이어 (차원 확장)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)  # 두 번째 레이어 (원래 차원으로 복원)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    인코더의 단일 레이어.\n",
    "    셀프 어텐션과 피드포워드 네트워크를 포함하며, 레이어 정규화와 드롭아웃을 적용합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)  # 첫 번째 정규화\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)  # 두 번째 정규화\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 입력 텐서\n",
    "            mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 출력 텐서\n",
    "            enc_attn: 셀프 어텐션 가중치\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        # 셀프 어텐션 + 드롭아웃 + 잔차 연결\n",
    "        out, enc_attn = self.enc_self_attn(self.norm_1(x), self.norm_1(x), self.norm_1(x), mask)\n",
    "        out = self.do(out) + residual\n",
    "        residual = out\n",
    "        # 피드포워드 네트워크 + 드롭아웃 + 잔차 연결\n",
    "        out = self.ffn(self.norm_2(out))\n",
    "        out = self.do(out) + residual\n",
    "        return out, enc_attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    디코더의 단일 레이어.\n",
    "    셀프 어텐션, 인코더-디코더 어텐션, 피드포워드 네트워크를 포함합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)  # 셀프 어텐션\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)  # 인코더-디코더 어텐션\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 디코더 입력\n",
    "            enc_out: 인코더 출력\n",
    "            dec_enc_mask: 디코더-인코더 어텐션 마스크\n",
    "            padding_mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 출력 텐서\n",
    "            dec_attn: 셀프 어텐션 가중치\n",
    "            dec_enc_attn: 인코더-디코더 어텐션 가중치\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        # 셀프 어텐션 (look-ahead 마스크 적용)\n",
    "        out, dec_attn = self.dec_self_attn(self.norm_1(x), self.norm_1(x), self.norm_1(x), mask=padding_mask)\n",
    "        out = self.do(out) + residual\n",
    "        residual = out\n",
    "        # 인코더-디코더 어텐션\n",
    "        out, dec_enc_attn = self.enc_dec_attn(self.norm_2(out), enc_out, enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out) + residual\n",
    "        residual = out\n",
    "        # 피드포워드 네트워크\n",
    "        out = self.ffn(self.norm_3(out))\n",
    "        out = self.do(out) + residual\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    인코더 전체 구조.\n",
    "    임베딩 레이어, 위치 인코딩, 여러 개의 인코더 레이어로 구성됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout, vocab_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # 토큰 임베딩\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout)  # 위치 인코딩\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 입력 시퀀스 (batch_size, seq_len)\n",
    "            mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 인코더 출력\n",
    "            enc_attns: 각 레이어의 어텐션 가중치 리스트\n",
    "        \"\"\"\n",
    "        out = self.embedding(x) * math.sqrt(self.d_model)  # 임베딩 스케일링\n",
    "        out = self.pos_encoding(out)  # 위치 인코딩 추가\n",
    "        enc_attns = []\n",
    "        for layer in self.enc_layers:\n",
    "            out, enc_attn = layer(out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        return out, enc_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    디코더 전체 구조.\n",
    "    임베딩 레이어, 위치 인코딩, 여러 개의 디코더 레이어로 구성됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 디코더 입력 시퀀스\n",
    "            enc_out: 인코더 출력\n",
    "            dec_enc_mask: 디코더-인코더 어텐션 마스크\n",
    "            padding_mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 디코더 출력\n",
    "            dec_attns: 셀프 어텐션 가중치 리스트\n",
    "            dec_enc_attns: 인코더-디코더 어텐션 가중치 리스트\n",
    "        \"\"\"\n",
    "        out = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        out = self.pos_encoding(out)\n",
    "        dec_attns, dec_enc_attns = [], []\n",
    "        for layer in self.dec_layers:\n",
    "            out, dec_attn, dec_enc_attn = layer(out, enc_out, dec_enc_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    전체 Transformer 모델.\n",
    "    인코더와 디코더를 연결하고, 최종 출력 레이어를 포함합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout, src_vocab_size)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout, tgt_vocab_size)\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)  # 최종 출력 레이어\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: 소스 시퀀스 (batch_size, src_seq_len)\n",
    "            tgt: 타겟 시퀀스 (batch_size, tgt_seq_len)\n",
    "        Returns:\n",
    "            logits: 최종 예측 로짓 (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "            enc_attns: 인코더 어텐션 가중치 리스트\n",
    "            dec_attns: 디코더 셀프 어텐션 가중치 리스트\n",
    "            dec_enc_attns: 디코더-인코더 어텐션 가중치 리스트\n",
    "        \"\"\"\n",
    "        # 마스크 생성\n",
    "        src_mask = (src == ko_tokenizer.pad_id()).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt == en_tokenizer.pad_id()).unsqueeze(1).unsqueeze(2)\n",
    "        lookahead_mask = torch.triu(torch.ones(tgt.shape[1], tgt.shape[1]), diagonal=1).bool().to(device)\n",
    "        tgt_mask = tgt_mask | lookahead_mask\n",
    "        # 인코더/디코더 순전파\n",
    "        enc_out, enc_attns = self.encoder(src, src_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLearningRateScheduler:\n",
    "    \"\"\"\n",
    "    \"Attention Is All You Need\" 논문에서 제안된 custom learning rate scheduler.\n",
    "    Warm-up 기간 동안 학습률을 선형적으로 증가시킨 후, step 수의 역제곱근에 비례하여 감소시킵니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_steps = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"학습률을 업데이트합니다.\"\"\"\n",
    "        self.num_steps += 1\n",
    "        lr = self._get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _get_lr(self):\n",
    "        \"\"\"학습률을 계산합니다.\"\"\"\n",
    "        step = self.num_steps\n",
    "        # 수식: lrate = d_model**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2)\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"스케줄러의 상태를 반환합니다.\"\"\"\n",
    "        return {'num_steps': self.num_steps}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"스케줄러의 상태를 불러옵니다.\"\"\"\n",
    "        self.num_steps = state_dict['num_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 설정이 'Warmup + Cosine Annealing' 스케줄러로 업그레이드되었습니다.\n",
      "최대 학습률(Peak LR): 0.0005, 웜업 스텝: 4000\n",
      "체크포인트가 없습니다. 처음부터 훈련을 시작합니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델, 손실 함수 초기화 (Label Smoothing 포함)\n",
    "model = Transformer(N_LAYERS, D_MODEL, N_HEADS, D_FF, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, DROPOUT).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ko_tokenizer.pad_id(), label_smoothing=0.1)\n",
    "\n",
    "# 옵티마이저: AdamW 사용 및 최대 학습률(peak_lr) 설정\n",
    "peak_lr = 5e-4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=peak_lr, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)\n",
    "\n",
    "warmup_steps = 4000\n",
    "# 전체 훈련 스텝 계산 (CosineAnnealingLR의 T_max에 필요)\n",
    "total_training_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "# 1. Warmup 스케줄러 (LinearLR)\n",
    "# warmup_steps 동안 학습률을 0에서 peak_lr까지 선형적으로 증가\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.001, total_iters=warmup_steps)\n",
    "\n",
    "# 2. Main 스케줄러 (CosineAnnealingLR)\n",
    "# Warmup 이후, 남은 스텝 동안 학습률을 코사인 곡선을 따라 부드럽게 감소시킵니다.\n",
    "main_scheduler = CosineAnnealingLR(optimizer, T_max=total_training_steps - warmup_steps, eta_min=1e-6)\n",
    "\n",
    "# 3. 두 스케줄러를 순차적으로 연결 (SequentialLR)\n",
    "# warmup_steps 이전까지는 warmup_scheduler를, 이후에는 main_scheduler를 사용\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, main_scheduler],\n",
    "    milestones=[warmup_steps]\n",
    ")\n",
    "\n",
    "print(\"훈련 설정이 'Warmup + Cosine Annealing' 스케줄러로 업그레이드되었습니다.\")\n",
    "print(f\"최대 학습률(Peak LR): {peak_lr}, 웜업 스텝: {warmup_steps}\")\n",
    "\n",
    "# 체크포인트 불러오기\n",
    "start_epoch = 0\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"체크포인트를 불러옵니다: {CHECKPOINT_PATH}\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_valid_loss = checkpoint['best_valid_loss']\n",
    "    print(f\"체크포인트 로드 완료. Epoch {start_epoch + 1}부터 훈련을 재개합니다.\")\n",
    "    model.to(device) # 모델을 device로 이동\n",
    "else:\n",
    "    print(\"체크포인트가 없습니다. 처음부터 훈련을 시작합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 / 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaba3bc7a3ef4d11b3fe8daec7cd9ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db66c15016c46bcadf696355c2d63dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved. Checkpoint saved to transformer-2.0-checkpoint.pth\n",
      "Time: 17m 47s\n",
      "\tTrain Loss: 25.006 | Train PPL: 72430073612.781\n",
      "\t Val. Loss: 8.043 |  Val. PPL: 3110.826\n",
      "------------------------------\n",
      "Epoch 02 / 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbaf78765e444084918d307db7198c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9976b70a6e44242961664c269beeed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved. Checkpoint saved to transformer-2.0-checkpoint.pth\n",
      "Time: 17m 54s\n",
      "\tTrain Loss: 6.150 | Train PPL: 468.926\n",
      "\t Val. Loss: 6.018 |  Val. PPL: 410.712\n",
      "------------------------------\n",
      "Epoch 03 / 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43235f6df414831b3b0346091f8d926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d70291857214ebdb5691fc50d8dc6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved. Checkpoint saved to transformer-2.0-checkpoint.pth\n",
      "Time: 17m 47s\n",
      "\tTrain Loss: 5.570 | Train PPL: 262.424\n",
      "\t Val. Loss: 5.679 |  Val. PPL: 292.611\n",
      "------------------------------\n",
      "Epoch 04 / 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9f997b554d433b8ce4f9fb4f642709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3725c6cf8f8d4d16a91141bc92acd3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved. Checkpoint saved to transformer-2.0-checkpoint.pth\n",
      "Time: 17m 42s\n",
      "\tTrain Loss: 5.331 | Train PPL: 206.711\n",
      "\t Val. Loss: 5.542 |  Val. PPL: 255.168\n",
      "------------------------------\n",
      "Epoch 05 / 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04637c0a99c45bea84fc8f3694f2487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac12d847c90e406c8afd8a0fe72d2158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved. Checkpoint saved to transformer-2.0-checkpoint.pth\n",
      "Time: 17m 36s\n",
      "\tTrain Loss: 5.176 | Train PPL: 176.886\n",
      "\t Val. Loss: 5.433 |  Val. PPL: 228.728\n",
      "------------------------------\n",
      "Epoch 06 / 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dd8007ec6047cfb6809def7d5649c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31753fa5837a44009e33540e71e2d3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved. Checkpoint saved to transformer-2.0-checkpoint.pth\n",
      "Time: 17m 34s\n",
      "\tTrain Loss: 5.056 | Train PPL: 156.992\n",
      "\t Val. Loss: 5.364 |  Val. PPL: 213.505\n",
      "------------------------------\n",
      "Epoch 07 / 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d11a4c13509478c9c2ceddbb34639b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b52471f6e3b42899b88519b6027037e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved. Checkpoint saved to transformer-2.0-checkpoint.pth\n",
      "Time: 17m 30s\n",
      "\tTrain Loss: 4.927 | Train PPL: 137.982\n",
      "\t Val. Loss: 5.326 |  Val. PPL: 205.614\n",
      "------------------------------\n",
      "Epoch 08 / 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5402261a2d4d7084b5fee77b9b1b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, scheduler, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(iterator, desc=\"Training\", mininterval=0.5, leave=False)\n",
    "\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        src = batch[0].to(device)\n",
    "        tgt = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _, _, _ = model(src, tgt[:,:-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item(), lr=scheduler.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(iterator, desc=\"Evaluating\", mininterval=0.5, leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            src = batch[0].to(device)\n",
    "            tgt = batch[1].to(device)\n",
    "\n",
    "            output, _, _, _ = model(src, tgt[:,:-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# --- 학습 루프 ---\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02} / {EPOCHS:02}\")\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, scheduler, criterion, 1)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_valid_loss': best_valid_loss,\n",
    "        }, CHECKPOINT_PATH)\n",
    "        print(f\"Validation loss improved. Checkpoint saved to {CHECKPOINT_PATH}\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"Validation loss did not improve. Counter: {early_stopping_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "    print(f'Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')\n",
    "    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"조기 종료: {EARLY_STOPPING_PATIENCE} 에폭 동안 검증 손실이 개선되지 않았습니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 번역 (greedy Search, Beam Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 새로운 모델 객체를 만들고 저장된 가중치를 불러옴\n",
    "inference_model = Transformer(N_LAYERS, D_MODEL, N_HEADS, D_FF, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, DROPOUT).to(device)\n",
    "\n",
    "# 체크포인트는 딕셔너리 형태이므로, model_state_dict를 직접 로드해야 합니다.\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "inference_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# 2. 모델을 평가 모드로 설정\n",
    "inference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_tokenizer, tgt_tokenizer, model, device, max_len=50):\n",
    "\n",
    "    src_tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    tgt_tokens = [tgt_tokenizer.bos_id()]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        tgt_tensor = torch.LongTensor(tgt_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, _, _, dec_enc_attns = model(src_tensor, tgt_tensor)\n",
    "\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        tgt_tokens.append(pred_token)\n",
    "\n",
    "        if pred_token == tgt_tokenizer.eos_id():\n",
    "            break\n",
    "\n",
    "    tgt_sentence = tgt_tokenizer.decode_ids(tgt_tokens)\n",
    "    return tgt_sentence, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence_beam_search(sentence, src_tokenizer, tgt_tokenizer, model, device, max_len=50, beam_size=5):\n",
    "    \"\"\"\n",
    "    빔 서치(Beam Search)를 사용하여 문장을 번역하고, 최종 번역문에 대한 어텐션 맵을 반환하는 함수입니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    src_tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    beams = [(torch.LongTensor([tgt_tokenizer.bos_id()]).to(device), 0)]\n",
    "    completed_hypotheses = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, score in beams:\n",
    "            if seq[-1].item() == tgt_tokenizer.eos_id():\n",
    "                completed_hypotheses.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output, _, _, _ = model(src_tensor, seq.unsqueeze(0))\n",
    "\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            next_token_log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "            top_next_tokens = torch.topk(next_token_log_probs, beam_size, dim=-1)\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                token_id = top_next_tokens.indices[0][i].item()\n",
    "                log_prob = top_next_tokens.values[0][i].item()\n",
    "\n",
    "                new_seq = torch.cat([seq, torch.LongTensor([token_id]).to(device)])\n",
    "                new_score = score + log_prob\n",
    "                new_beams.append((new_seq, new_score))\n",
    "\n",
    "        if not new_beams:\n",
    "            break\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "        if all(b[0][-1].item() == tgt_tokenizer.eos_id() for b in beams):\n",
    "            completed_hypotheses.extend(beams)\n",
    "            break\n",
    "\n",
    "    if not completed_hypotheses:\n",
    "        completed_hypotheses.extend(beams)\n",
    "\n",
    "    best_hypothesis = sorted(completed_hypotheses, key=lambda x: x[1] / len(x[0]), reverse=True)[0]\n",
    "    best_sequence = best_hypothesis[0]\n",
    "\n",
    "    translated_sentence = tgt_tokenizer.decode_ids(best_sequence.tolist())\n",
    "\n",
    "    # 최종 선택된 시퀀스에 대한 어텐션 맵을 얻기 위해 모델을 한 번 더 실행\n",
    "    with torch.no_grad():\n",
    "        # </s> 토큰은 어텐션 계산에 필요 없으므로, 있다면 제외\n",
    "        input_seq = best_sequence.unsqueeze(0)\n",
    "        if input_seq[0, -1].item() == tgt_tokenizer.eos_id():\n",
    "            input_seq = input_seq[:, :-1]\n",
    "\n",
    "        _, _, _, final_attentions = model(src_tensor, input_seq)\n",
    "\n",
    "    return translated_sentence, final_attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역할 문장 선택\n",
    "example_idx = 1\n",
    "src = test_kor_corpus[example_idx]\n",
    "trg = test_eng_corpus[example_idx]\n",
    "\n",
    "# 빔 서치로 번역 실행 (beam_size=5)\n",
    "beam_translation, beam_attention = translate_sentence_beam_search(src, ko_tokenizer, en_tokenizer, inference_model, device, beam_size=5)\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')\n",
    "print(f'predicted trg (beam search) = {beam_translation}')\n",
    "\n",
    "# 기존 Greedy 방식과 비교\n",
    "greedy_translation, _ = translate_sentence(src, ko_tokenizer, en_tokenizer, inference_model, device)\n",
    "print(f'predicted trg (greedy)      = {greedy_translation}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 어텐션 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2):\n",
    "    \"\"\"어텐션 맵을 시각화합니다.\"\"\"\n",
    "    assert n_rows * n_cols == n_heads\n",
    "\n",
    "    font_path = './NanumBarunGothic.ttf'\n",
    "    font_prop = fm.FontProperties(fname=font_path, size=8)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 28))  # x축을 조금 넓혀서 압축 줄임 (10->12)\n",
    "\n",
    "    # 번역된 문장과 원본 문장을 토큰 단위로 분리\n",
    "    sentence_tokens = sentence.split()\n",
    "    translation_tokens = translation.split()\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "\n",
    "        # attention shape: (head_idx, tgt_len, src_len)\n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        # extent 명시: (-0.5, src_len-0.5, tgt_len-0.5, -0.5)로 ticks와 맞춤\n",
    "        src_len = len(sentence_tokens)\n",
    "        tgt_len = len(translation_tokens)\n",
    "        cax = ax.matshow(_attention, cmap='viridis', extent=[-0.5, src_len - 0.5, tgt_len - 0.5, -0.5])\n",
    "\n",
    "        # 눈금 위치 설정\n",
    "        ax.set_xticks(range(src_len))\n",
    "        ax.set_yticks(range(tgt_len))\n",
    "\n",
    "        # 라벨 설정: ha/va로 중앙 정렬\n",
    "        # 다른분꺼 보니까 45도가 좋아보이더만\n",
    "        ax.set_xticklabels(sentence_tokens, rotation=45, fontproperties=font_prop, ha='center', va='center')\n",
    "        ax.set_yticklabels(translation_tokens, fontproperties=font_prop, ha='right', va='center')\n",
    "\n",
    "        ax.tick_params(labelsize=8, pad=15)  # pad로 텍스트와 tick 간격 미세 조정\n",
    "\n",
    "    plt.tight_layout()  # subplot 간 여백 자동 조정 (밀림 방지)\n",
    "    plt.show()\n",
    "\n",
    "display_attention(src, beam_translation, beam_attention[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 최종 모델 성능 종합 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_scorer\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_comprehensively(model, src_corpus, tgt_corpus, src_tokenizer, tgt_tokenizer, device, translate_function, beam_size=5):\n",
    "    \"\"\"\n",
    "    테스트 데이터셋 전체에 대해 번역을 수행하고,\n",
    "    BLEU, METEOR, ROUGE, BERTScore를 포함한\n",
    "    종합적인 평가지표를 계산하여 출력\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1. 전체 테스트 데이터셋에 대해 번역 생성\n",
    "    print(\"테스트 데이터셋 전체에 대한 번역을 시작합니다...\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for src_sentence, ref_sentence in tqdm(zip(src_corpus, tgt_corpus), total=len(src_corpus), desc=\"Translating\"):\n",
    "        pred_sentence, _ = translate_function(\n",
    "            src_sentence, src_tokenizer, tgt_tokenizer, model, device, beam_size=beam_size\n",
    "        )\n",
    "        predictions.append(pred_sentence)\n",
    "        references.append(ref_sentence)\n",
    "\n",
    "    print(\"번역 완료. 평가지표 계산을 시작합니다...\")\n",
    "\n",
    "    # 2. N-gram 기반 평가지표 계산 (BLEU, METEOR, ROUGE)\n",
    "    print(\"Calculating BLEU, METEOR, ROUGE scores...\")\n",
    "    pred_tokens = [p.split() for p in predictions]\n",
    "    ref_tokens = [[r.split()] for r in references]\n",
    "\n",
    "    # BLEU\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    bleu_score = np.mean([sentence_bleu(r, p, smoothing_function=smooth_fn) for r, p in zip(ref_tokens, pred_tokens)])\n",
    "\n",
    "    # METEOR\n",
    "    meteor_score_avg = np.mean([meteor_score(r, p) for r, p in zip(ref_tokens, pred_tokens)])\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_calculator = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_f1, rougeL_f1 = [], []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        scores = rouge_calculator.score(ref, pred)\n",
    "        rouge1_f1.append(scores['rouge1'].fmeasure)\n",
    "        rougeL_f1.append(scores['rougeL'].fmeasure)\n",
    "    rouge1_avg = np.mean(rouge1_f1)\n",
    "    rougeL_avg = np.mean(rougeL_f1)\n",
    "\n",
    "    # 3. 의미 기반 평가지표 계산 (BERTScore)\n",
    "    print(\"Calculating BERTScore...\")\n",
    "    P, R, F1 = bert_scorer(predictions, references, lang=\"en\", device=device, verbose=True)\n",
    "    bert_f1_score = F1.mean().item()\n",
    "\n",
    "    # 4. 최종 결과 종합 출력\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"      종합 번역 성능 평가 결과      \")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"  BLEU Score   : {bleu_score * 100:.2f}\")\n",
    "    print(f\"  METEOR Score : {meteor_score_avg * 100:.2f}\")\n",
    "    print(f\"  ROUGE-1 (F1) : {rouge1_avg * 100:.2f}\")\n",
    "    print(f\"  ROUGE-L (F1) : {rougeL_avg * 100:.2f}\")\n",
    "    print(f\"  BERTScore (F1): {bert_f1_score * 100:.2f}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "# 함수 호출하여 종합 평가 실행\n",
    "# 이전에 로드한 inference_model과 Mecab 처리된 test_kor_mecab을 사용\n",
    "evaluate_model_comprehensively(\n",
    "    inference_model,\n",
    "    test_kor_mecab,\n",
    "    test_eng_corpus,\n",
    "    ko_tokenizer,\n",
    "    en_tokenizer,\n",
    "    device,\n",
    "    translate_sentence_beam_search, # 빔 서치 함수 사용\n",
    "    beam_size=5\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

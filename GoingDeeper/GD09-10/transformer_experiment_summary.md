# Transformer 번역 모델 개선 과정 분석 보고서

## 1. 초기 접근: Transformer 베이스라인 (v0.0 ~ v1.1)
- **초기 모델**: "Attention Is All You Need" 논문 구조를 기반으로 한 베이스라인 모델로 실험을 시작.
- **초기 시도**: 데이터 증강(Lexical Substitution), 학습률 스케줄러(역제곱근 감쇠) 등을 도입하며 PPL을 **62.18 (v0.0)** 에서 **48.01 (v1.1)** 까지 성공적으로 개선.
- **명확한 한계 봉착**: PPL 수치는 개선되었으나, 생성된 번역문은 `germany , germany cnn americans are being sold in the united states , germany , , japan and germany , japan , ` 처럼 단어를 반복하는 등, 문법 적으로도 맞지 않는 여전히 **사용 불가능한 수준**에 머물렀다. 이는 단순한 파라미터 튜닝만으로는 넘을 수 없는 근본적인 한계가 있음을 시사했다.

---

## 2. 근본 원인 탐색: 데이터 품질의 재발견

- **가설**: 모델의 성능이 특정 PPL(약 40~50) 이하로 떨어지지 않는 현상은, 모델 구조나 하이퍼파라미터의 한계가 아닌 **학습 데이터 자체의 품질 문제**일 것이라는 가설을 세웠다.
- **근거**: 데이터셋을 수동으로 탐색하는 과정에서, 모델에게 '독'으로 작용할 수 있는 다수의 노이즈 데이터를 발견했다.
    - **오정렬 데이터**: `한글: walk 신분; 직업;` / `영어: Learn how to talk the walk` 처럼 의미가 전혀 다른 문장 쌍.
    - **사전 형식 데이터**: `high-tech 고도 기술의 jeopardy 위험...` 처럼 단어와 뜻풀이가 나열된 문장.
    - **스크래핑 아티팩트**: `번역 :`, `어휘 :`, URL, `▶` 등 웹페이지에서 잘못 수집된 문자열.
    - **인코딩 오류**: `&#51922;` 와 같이 깨진 문자(HTML 엔티티) 포함.

- **핵심 깨달음**: **"Garbage In, Garbage Out"**. 아무리 좋은 모델과 훈련 기법을 사용해도, 데이터가 깨끗하지 않으면 모델은 그 데이터의 한계를 절대 넘어설 수 없음을 확인했다. **데이터 정제야말로 성능 향상의 가장 중요한 선결 과제**임을 다시한번 깨달았고, 그새 `데이터 전처리가 중요하구나` 생각했던 이전의 나를 까먹었던 나를 다시 발견했다.

---

## 3. 실험 과정 및 분석

### 3.1. 실험 1: 아키텍처 탐색 (v1.3)
- **목표**: 데이터의 영향과 모델 구조의 영향을 분리해서 보기 위해, 데이터 증강을 잠시 배제하고 아키텍처 변경의 효과를 테스트.
- **방법**: 모델을 '깊고 좁은' 구조(`N_LAYERS=6, D_FF=2048`)에서 **'얕고 넓은' 구조(`N_LAYERS=4, D_FF=4096`)**로 변경.
- **결과**: 최고 Validation PPL **43.08** (`Val. Loss: 3.763`) 달성. 기존의 깊은 모델(v1.2, PPL 49.10)보다 더 좋은 성능을 기록.
- **분석**: 이 데이터셋에서는 레이어의 깊이보다 각 레이어의 표현력(너비)이 성능에 더 긍정적인 영향을 미친다는 인사이트를 얻었다.

#### 심층 분석: 왜 빔 서치(Beam Search)가 더 나쁜 결과를 냈는가?
- v1.3 모델(PPL 43.08)로 번역을 생성했을 때, 이론적으로 더 우월한 빔 서치가 오히려 Greedy Search보다 품질이 낮은 문장을 생성하는 현상이 관찰되었다.
    - **입력**: `그러나 어소시에이션 오브 아메리칸 메디칼 콜리지...`
    - **Greedy**: `and consumer protection agency...` (엉뚱하지만 단어는 다양함)
    - **Beam Search**: `, americans think that the most influential americans think...` (단어 반복의 함정에 빠짐)
- **원인 고찰**: 이는 **모델이 아직 '멍청하기' 때문**이다. PPL 43은 모델이 다음 단어를 예측할 때마다 평균적으로 43개의 단어 사이에서 헷갈리고 있다는 의미다. 이런 불확실한 상태에서, 빔 서치는 누적 확률을 높이기 위해 가장 '안전한' 선택, 즉 이전에 등장했던 고빈도 단어('americans')를 반복하는 잘못된 경로에 갇히기 쉽다. 모델이 멍청하기 때문에 더 안좋은 선택지만 더 확률을 높여주는 오작동이 났던 것. 반면, Greedy Search는 근시안적으로 다음 단어만 선택하기에 우연히 이 반복의 함정을 피해갈 수 있었던 것이다. 이 현상은 **"알고리즘의 우수함이 모델 자체의 성능 부족을 해결해주지는 못한다"**는 교훈이 생김.

### 3.2. 실험 2: 최종 모델 구축 (v2.0)
- **목표**: 위 모든 실험에서 얻은 교훈을 모두 머지하여, 최대한 성능을 내는 최종 모델을 구축
- **적용 기술**: 
    1.  **데이터 클리닝**: 위에서 발견된 노이즈들을 제거하는 다단계 필터링 파이프라인을 최우선으로 적용.
    2.  **고품질 데이터 증강**: **역번역(Back-translation)** 기법 도입.
    3.  **토크나이저 고도화**: **Mecab + SentencePiece** 조합 사용.
    4.  **최신 훈련 레시피**: **AdamW, Label Smoothing, 코사인 어닐링 스케줄러** 적용.

#### **v2.0 최종 결과 및 분석**
- **결과**: 최고 Validation PPL **178.51** (`Val. Loss: 5.185`) 달성 후, 15 에폭에서 조기 종료. **오히려 이전 v1.3 모델(PPL 43.08)보다 성능이 크게 하락했다.**
- **심층 분석: 무엇이 문제였나?**
    - 모든 최신 기법을 적용했음에도 성능이 크게 하락한 것은 매우 당혹스러운 결과이다. 이는 **'너무 많은 변수를 한 번에 변경'**했을 때 발생하는 복합적인 문제로 분석된다.
    - **가장 유력한 원인**은 **하이퍼파라미터 비궁합(Mismatch)**이다. 데이터셋의 크기(역번역으로 2배), 분포(Mecab으로 변경), 옵티마이저, 스케줄러가 모두 바뀌면서, 우리가 실험적으로 찾았던 `peak_lr`이 이 복잡한 조합에는 전혀 맞지 않았을 가능성이 높다. 실제로 `peak_lr`을 `5e-5`에서 `5e-4`로 올렸을 때 학습이 더 불안정해지는 등, 안정적인 최적점을 찾지 못하고 높은 Loss 값 주변에서 방황하다 학습이 정체된 것으로 보인다.
    - 이 경험은 최신 기술의 '맹목적인' 적용이 아니라, **새로운 환경에서는 반드시 그에 맞는 하이퍼파라미터를 다시 탐색해야 한다**는 중요한 교훈을 남겼다.

---

## 4. 프로젝트 최종 결론

이번 프로젝트는 성공적인 번역기 모델을 만드는 것만큼이나, **체계적인 실험의 중요성**을 깨닫는 과정이었다.

베이스라인에서 시작해 데이터 정제, 아키텍처 변경, 훈련 기법 고도화를 거치며 점진적으로 성능을 개선했다. 특히 'Garbage In, Garbage Out' 원칙을 체감하며 데이터 품질의 중요성을 확인한 것과, v1.3에서 '얕고 넓은' 아키텍처의 가능성을 발견한 것은 큰 수확이었지만,,

모든 개선안을 집대성한 최종 모델이 오히려 성능이 하락한 것은 가장 중요한 교훈을 남겼다. 이는 **'한 번에 너무 많은 변수를 바꾸는 것'의 위험성**과, 새로운 환경에서는 **반드시 최적의 하이퍼파라미터를 다시 탐색해야 한다**는 점을 명확히 보여준다. 이젠 1.3 까지 잘 왔으니, 최신기술을 덕지덕지 붙이면 잘될거라고 생각한 것 부터 실패가 예견된 일이었을지도.

비록 최종 모델의 성능은 아쉬웠지만, 이 실패를 분석하는 과정을 통해 **'좋은 모델을 만드는 방법론'** 자체에 대해 배울 수 있었다. 결국, 성공적인 딥러닝 모델 개발은 화려한 기술의 적용 이전에, 데이터를 의심하고, 변화를 통제하며, 결과를 집요하게 분석해야 함을 알았다.

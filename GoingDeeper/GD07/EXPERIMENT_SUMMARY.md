# GD07. Mini BERT Pre-training 과정 및 실험 보고서

---

## 1. 프로젝트 목표 및 개요

- **목표:** 약 1.4M개의 파라미터를 갖는 Mini-BERT 모델을 직접 구현하고, 한국어 위키피디아 데이터셋으로 Pre-training을 진행하여 모델의 성능을 확인하고 개선한다.
- **핵심 과제:**
    1. BERT의 핵심 Task인 MLM(Masked Language Model)과 NSP(Next Sentence Prediction)를 위한 데이터셋 파이프라인 구축.
    2. Transformer의 Encoder 구조를 기반으로 BERT 모델 및 Pre-training을 위한 헤드(Head) 구현.
    3. 하이퍼파라미터 튜닝 등 추가 실험을 통해 모델의 성능 변화를 관찰하고 분석.

## 2. 실험: Learning Rate 튜닝

- **동기:** 초기 학습 후, 모델의 성능이 기대에 미치지 못하여 성능 개선을 위한 하이퍼파라미터 튜닝을 계획함
- **실험 설계:** 모델 성능에 가장 큰 영향을 주는 `Learning Rate`를 우선적으로 튜닝하기로 결정. 배치 사이즈(16), 모델 구조 등 다른 조건은 모두 고정하고 `max_lr` 값만 변경하며 최적점을 탐색함

- **실험 결과 요약:**

| Learning Rate | 최종 Loss (Epoch 10) | 비고 |
| :---: | :---: | :--- |
| `1e-4` | 6.1548 | Baseline. Loss 감소가 상대적으로 더딤. |
| `2e-4` | 5.4585 | Baseline 대비 Loss가 유의미하게 감소. |
| `3e-4` | 5.2515 | `2e-4`보다 더 낮은 Loss 값으로 수렴. |
| `4e-4` | 5.1681 | `3e-4`보다 더 낮은 Loss를 기록. |
| `5e-4` | 5.1083 | `4e-4`보다 더 낮은 Loss를 기록. |
| `6e-4` | 5.0763 | `5e-4`보다 더 낮은 Loss를 기록. |
| `9e-4` | 5.0424 | `6e-4`보다 더 낮은 Loss를 기록. |
| `1e-3` | **5.0192** | `9e-4`보다 더 낮은 Loss를 기록. **현재까지 가장 좋은 성능.** |
| `1.5e-3`| 5.3551 | Epoch 3부터 Loss가 다시 증가. 발산 시작. |
| `2e-3` | 5.1276 | Epoch 3부터 Loss가 다시 증가. 발산 시작. |
| `3e-3` | 5.3551 | Epoch 2부터 Loss가 급격히 증가. |
| `5e-3` | 8.0888 | Epoch 2부터 Loss가 급격히 증가. |

- **결론:** 현재 모델 구조에서는 `max_lr`을 **`1e-3`**으로 설정했을 때 가장 좋은 성능을 보였다. `1.5e-3`부터는 학습이 불안정해지며 Loss가 다시 증가하는 현상을 통해, 최적의 Learning Rate가 `1e-3` 근처에 있음을 확인

### 2.1. 추가 실험: 모델 구조 탐색 (깊이 vs 너비)

- **동기:** 최적의 Learning Rate(`1e-3`)를 찾은 후, 동일한 파라미터 예산(~1.4M) 내에서 모델의 구조(깊이와 너비)가 성능에 미치는 영향을 확인하고자 함
- **실험 설계:** 세 가지 다른 구조의 모델을 동일한 조건(learning rate, epoch 등)으로 학습시키고 최종 Loss를 비교함
    1.  **Standard (Baseline):** 2-layer, 128-dim
    2.  **Deep & Narrow:** 6-layer, 96-dim
    3.  **Shallow & Wide:** 1-layer, 144-dim

- **실험 결과 요약:**

| 모델 | 구조 (Layers, Dim) | 파라미터 수 | 최종 Loss (Epoch 10) | 비고 |
| :--- | :---: | :---: | :---: | :--- |
| Standard | 2-layer, 128-dim | 약 1.45M | 5.0192 | Baseline |
| Shallow & Wide | 1-layer, 144-dim | 약 1.41M | 5.2425 | Baseline 대비 성능 하락 |
| **Deep & Narrow** | **6-layer, 96-dim** | **약 1.46M** | **4.8832** | **가장 좋은 성능** |

- **결론:** 파라미터 수가 비슷한 조건에서는, 모델이 얕고 넓어지는 것보다 **더 깊고 좁아지는 것**이 Pre-training 성능에 더 효과적임을 확인함 이는 모델이 여러 레이어를 거치며 더 복잡하고 추상적인 피처를 학습할 수 있기 때문으로 추정

### 2.2. 추가 실험: 전체 데이터셋 학습 및 과적합 분석

- **동기:** 가장 성능이 좋았던 Deep & Narrow 모델(6-layer, 96-dim)을 전체 데이터셋으로 학습시켜 성능의 극한을 확인하고자 함
- **실험 설계:** `bert_deep_6L_96D` 모델을 기존 128,000건이 아닌 전체 Pre-training 데이터(약 918,000건)로 10 epoch 학습을 진행함
- **실험 결과:**
    - **Training Loss:** Epoch 10 최종 Loss가 **4.3526**으로, 일부 데이터로 학습했을 때의 4.8832보다 유의미하게 감소함
    - **일반 상식 테스트:** 하지만, "대한민국의 수도는?"과 같은 간단한 테스트에서는 오히려 성능이 크게 하락함 NSP(문장 순서 예측) 성능은 거의 정반대로 동작하는 등, 모델이 일반 상식을 잃어버린 듯한 모습을 보였습니다.
- **결론 및 분석:**
    - 이 현상은 모델이 **학습 데이터의 특성에 과적합**되었기 때문으로 분석됩니다. 모델은 대화나 일반 상식 문장이 아닌, 위키피디아 특유의 건조하고 정보 나열적인 문체에만 학습이 된 것으로 보임
    - 이는 **학습 데이터의 편향(bias)이 모델의 일반화 성능에 미치는 영향**을 명확히 보여주는 사례

### 3.5. 최종 가설 검증: 위키피디아 데이터 검증

- **동기:** '전체 데이터셋으로 학습한 모델이 일반 상식 테스트에서 실패한 이유가 위키피디아 데이터셋의 특성에 과적합되었기 때문'이라는 가설을 최종적으로 검증하고자 함
- **실험 설계:** 전체 데이터셋을 90%의 훈련 데이터와 10%의 검증 데이터로 분리함 6-layer, 96-dim 모델을 훈련 데이터로 학습시킨 후, 학습에 전혀 사용되지 않은 10%의 검증 데이터로 성능을 평가함
- **실험 결과:**
    - **Training Loss (on 90% data):** 약 4.35 (이전 전체 데이터 학습 Loss와 유사)
    - **Validation Loss (on 10% held-out data):** **4.2926**
- **결론 및 분석:**
    - **가설 입증:** 검증 데이터(held-out Wikipedia)에 대한 손실(Loss)이 훈련 데이터의 손실과 거의 동일하게 낮게 측정되었습니다. 이는 모델이 훈련 데이터의 특정 샘플을 단순히 암기한 것이 아니라, **위키피디아 도메인 자체의 언어적 패턴을 성공적으로 학습하고 일반화**했음을 의미합니다.
    - **문제의 본질 규명:** 따라서, 일반 상식 테스트의 실패 원인은 모델의 학습 실패가 아닌, **'학습된 도메인(위키피디아)'과 '테스트 문장의 도메인(일반 상식/대화)' 사이의 불일치** 때문임이 명확해짐.
    - **프로젝트 최종 결론:** 이 Mini-BERT Pre-training 프로젝트를 통해, 우리는 성공적으로 언어 모델을 구축하고 그 성능을 개선하는 과정을 경험함 특히, 모델의 성능은 학습 데이터의 양과 질, 그리고 특성에 크게 의존한다는 점을 실험적으로 확인함 현재 모델은 '파인튜닝(Finetuning)'을 통해 특정 도메인의 Task를 해결할 준비가 된, 성공적인 Pre-trained 모델이라고 결론

## 4. 회고

### 배운 점
- **BERT의 전체 파이프라인 경험:** 데이터 수집 및 가공부터, `nn.Module`을 이용한 모델 밑바닥 구현, Pre-training, 그리고 테스트 및 디버깅까지의 전 과정을 직접 수행하며 BERT의 작동 원리를 깊이 있게 이해할 수 있었습니다. 특히, MLM과 NSP라는 두 가지 Task가 어떻게 하나의 모델에서 동시에 학습되는지 명확히 알게 되었습니다.
- **하이퍼파라미터의 중요성:** 동일한 모델과 데이터라도 Learning Rate 같은 하이퍼파라미터 설정에 따라 학습 양상과 최종 성능이 크게 달라짐을 체감함 작은 데이터셋으로 빠르게 여러 설정을 테스트하고 최적의 값을 찾아나가는 '튜닝' 과정의 효율성과 필요성을 깨달았습니다.
- **이론과 실제의 간극:** `memmap`의 원리, GPU와 CPU 간의 데이터 이동 등 이론으로만 알던 개념들이 실제 코드에서 어떤 에러를 발생시키고 어떻게 해결해야 하는지 직접 부딪히며 배우는 경험

### 아쉬운 점
- **모델의 절대적 성능:** Mini-BERT라는 이름에 걸맞게, 작은 모델과 제한된 데이터로 학습한 결과, '서울'과 같은 고유명사를 맞추는 데는 실패함 이를 통해 현대의 거대 언어 모델들이 얼마나 방대한 컴퓨팅 자원과 데이터를 기반으로 하는지 다시 한번 느낄 수 있었음.

### 느낀 점
단순히 만들어진 모델을 가져다 쓰는 것을 넘어, 모델의 내부 구조를 직접 구현하고 학습의 전 과정을 통제해보는 경험은 문제 해결 능력에 큰 자신감을 주었습니다. 성능이 낮은 'AI 아기'를 직접 만들어보니, 역설적으로 왜 우리가 잘 만들어진 'AI 성인(Pre-trained Model)'을 가져와 '파인튜닝'해야 하는지에 대한 당위성을 어느정도 체험 해 본 것 같음
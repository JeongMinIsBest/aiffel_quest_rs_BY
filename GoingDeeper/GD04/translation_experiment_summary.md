# 한영 번역기 모델 개선 과정 분석 보고서

## 1. 초기 접근: 단어 기반 Seq2Seq 모델
- **모델**: `Seq2Seq + Attention` ([v3](./translator_ko_en_v3.ipynb), [v4](./translator_ko_en_v4.0.ipynb))
- **토큰화**: 한국어(`Mecab`), 영어(`split()`) 단어 기반 토큰화.
- **결과**: PPL 577. 수치상으로 유의미한 번역 성능 확보 실패.
- **번역 품질**: `obama is , , president bush is a .` 등 문법적 오류 및 의미 왜곡 발생.

#### **한계 분석: 왜 성능이 더 이상 오르지 않는가?**
- **근본 원인**: 단어 기반 토큰화 방식의 명확한 한계 봉착.
    1.  **OOV (Out-of-Vocabulary) 문제**: 훈련 데이터에 없는 단어 등장 시, 의미 정보 완전 손실.
    2.  **한국어 형태소 복잡성**: '대통령', '대통령은', '대통령께서'를 완전히 다른 단어로 취급. 모델이 단어의 핵심 의미(stem)를 학습하지 못하고 파생형에 따라 분산 학습하여 효율 저하.

---

## 2. 대안 탐색: 서브워드 토큰화 (SentencePiece)
- **가설**: OOV 문제를 해결하고, 단어를 의미 단위(subword)로 분리하여 모델의 언어 구조 학습 능력을 향상시킬 수 있을 것.
- **시도**: `SentencePiece` 라이브러리를 활용, 한국어와 영어 코퍼스를 합쳐 **공유 어휘(Shared Vocabulary)** 모델 생성 및 적용 ([v5](./translator_ko_en_v4.0.1.ipynb)).
- **결과**: PPL 868. 오히려 성능이 이전보다 크게 하락.

#### **한계 분석: 왜 공유 어휘는 실패했는가?**
- **근본 원인**: 두 언어의 이질적인 특성을 하나의 토크나이저로 처리하려 한 전략의 실패.
    1.  **언어적 이질성**: 한국어(교착어)와 영어(굴절어)의 판이한 형태론적 구조를 단일 모델로 일반화하는 것의 한계.
    2.  **비효율적 토큰 생성**: 각 언어의 고유 특성을 제대로 반영하지 못해, 의미 없는 서브워드가 다수 생성되어 모델 학습을 직접적으로 방해.
- **추가 고찰**: Mecab으로 한국어 형태소를 먼저 분석한 뒤, SentencePiece를 적용했다면 어땠을까? 이 접근법은 각 언어의 특성을 존중하면서 서브워드 토큰화의 이점을 취하는 절충안이 될 수 있었을 것이다. 즉, Mecab이 1차적으로 의미 단위(형태소)를 보존해주고, SentencePiece가 그 형태소를 바탕으로 OOV 문제에 대응하는 서브워드를 생성하는 하이브리드 방식이다. 이는 공유 어휘 모델의 근본적 한계를 일부 보완할 수 있는 대안적 전략으로 고려해볼 수 있다.

---

## 3. 전략 수정: Seq2Seq 모델의 점진적 개선
- **방향**: `SentencePiece`의 실패를 교훈 삼아, 가장 안정적이었던 v4 모델을 기반으로 다른 변수를 통제하며 점진적 개선 추구.

- **시도 1 (v4.1 - 모델 확장)**: 영어 토크나이저 개선(`spaCy`) 및 모델 용량 증설 (`HID_DIM`, `N_LAYERS` ↑) ([v4.1](./translator_ko_en_v4.1.ipynb)).
    - **결과**: PPL 774. 과적합(Overfitting) 발생.
    - **한계 분석 (왜 과적합되었나?)**: 제한된 데이터셋(약 9만 건)의 복잡도에 비해 모델의 표현력(Capacity)이 과도하게 컸음. 일반화된 번역 패턴 학습 대신 훈련 데이터 자체를 암기하는 현상 발생. **모델을 무조건 키우는 것이 정답이 아님을 확인.**

- **시도 2 (v4.2 - RNN 유닛 변경)**: 안정적인 v4 하이퍼파라미터 기준, RNN 유닛을 GRU에서 **LSTM**으로 교체 ([v4.2](./translator_ko_en_v4.2_lstm.ipynb)).
    - **결과**: PPL 567. GRU 모델(PPL 577) 대비 소폭의 의미 있는 성능 향상.
    - **분석**: LSTM의 더 복잡한 게이트(Forget, Input, Output) 구조가 GRU보다 장기 의존성(Long-term dependency) 정보를 조금 더 효과적으로 제어하고 보존했음을 시사.

- **시도 3 (v4.4 - 최종 조합)**: 검증된 최고 요소(LSTM + `spaCy`) 결합 ([v4.4](./translator_ko_en_v4.4_lstm_spacy.ipynb)).
    - **결과**: **PPL 562.5.** Seq2Seq 프레임워크 내에서 달성한 **최고 성능 기록.**

#### **Seq2Seq 모델 성능 요약**

| 모델 버전 | RNN 유닛 | 토크나이저 (Eng) | Test PPL | 주요 변경점 및 결과 |
| :--- | :---: | :---: | :---: | :--- |
| [v4.0](./translator_ko_en_v4.0.ipynb) | GRU | `split()` | 577.4 | 베이스라인 모델 |
| [v4.2](./translator_ko_en_v4.2_lstm.ipynb) | **LSTM** | `split()` | 567.7 | GRU -> LSTM 변경으로 성능 소폭 향상 |
| [v4.4](./translator_ko_en_v4.4_lstm_spacy.ipynb) | **LSTM** | **`spaCy`** | **562.5** | 영어 토크나이저 개선으로 최고 성능 달성 |

---

## 4. 최종 결론: Seq2Seq 아키텍처의 본질적 한계
- **종합**: 체계적 실험을 통해 PPL을 577에서 562까지 개선했으나, 여전히 사용 불가 수준.
#### **한계 분석: 왜 PPL 500의 벽을 넘지 못했는가?**
- **근본 원인**: RNN 기반 Seq2Seq 아키텍처의 **정보 병목(Information Bottleneck)** 현상.
    1.  **순차적 처리의 한계**: 인코더는 입력 문장의 모든 정보를 고정된 크기의 컨텍스트 벡터 하나에 압축해야 함. 이 과정에서 필연적으로 정보 손실 발생.
    2.  **장기 의존성 문제**: 문장이 길어질수록, 문장 앞부분의 정보가 뒤로 전달되며 점차 희석됨. 어텐션으로 이를 일부 보완했지만, 컨텍스트 벡터 자체가 이미 손실된 정보를 담고 있다는 근본적 한계 존재.

- **최종 가설**: 점진적 개선이 아닌, 아키텍처의 근본적 전환(Transformer)만이 성능의 비약적인 도약을 만들 수 있음.
- **최종 실험 ([v5.1](./translator_ko_en_v5.1_transformer.ipynb))**: `Mecab` + `spaCy` 전처리 방식은 유지, 모델 엔진을 **트랜스포머**로 교체.
- **결과**: **Test PPL 69.7 달성.** Seq2Seq(562.5) 대비 **압도적인 성능 향상.**

#### **최종 모델 성능 비교**

| 아키텍처 | 모델 버전 | Test PPL | 성능 향상률 (PPL 기준) |
| :--- | :---: | :---: | :---: |
| Seq2Seq (LSTM) | [v4.4](./translator_ko_en_v4.4_lstm_spacy.ipynb) | 562.5 | - |
| **Transformer** | [v5.1](./translator_ko_en_v5.1_transformer.ipynb) | **69.7** | **87.6% 감소** |

### 4.1. 트랜스포머 모델의 실제 번역 결과
- **번역 예시 (입력: `오바마는 대통령이다.`):**
    - **Seq2Seq**: `obama is , , president bush is a .`
    - **Transformer**: `obama is the president .` (완벽한 번역)
- **분석**: 다른 문장에서 일부 한계(단어 반복, `<unk>` 토큰 등)가 보였으나, 이는 데이터셋의 한계로 판단. 핵심은, 트랜스포머가 **문법적으로 완결된 문장을 생성**하며 Seq2Seq와는 질적으로 다른 결과를 보여준다는 점.

---

## 5. 최종 고찰: 왜 트랜스포머는 달랐는가?
- **핵심 질문**: 무엇이 RNN의 정보 병목 현상을 해결하고 압도적인 성능 차이를 만들었는가?
- **답변**: **셀프-어텐션(Self-Attention) 메커니즘.**
    - **RNN의 한계점 재확인**: 순차적 처리로 인한 정보 손실 및 장기 의존성 문제.
    - **트랜스포머의 혁신**:
        1.  **전체 문맥 동시 파악**: 문장 내 모든 단어 간의 관계를 **한 번에, 직접적으로** 계산. "그"라는 대명사가 문맥상 누구를 지칭하는지 거리에 상관없이 파악 가능. 정보 병목 현상 없음.
        2.  **병렬 처리**: 순차적 의존성이 없어 대규모 병렬 학습 가능. 학습 효율성 극대화.

- **프로젝트 최종 결론**: 셀프-어텐션을 통한 뛰어난 문맥 이해 능력과 학습 효율성이 Seq2Seq의 근본적 한계를 돌파하고 번역 성능을 비약적으로 향상시킨 핵심 원인.

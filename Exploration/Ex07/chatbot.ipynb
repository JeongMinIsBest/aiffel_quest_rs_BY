{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5ef24311",
      "metadata": {},
      "source": [
        "# 트랜스포머 모델 기반 한국어 챗봇 만들기\n",
        "\n",
        "**진행 단계:**\n",
        "1. **모델 구성요소 정의**: 어텐션, 인코더, 디코더 등 모델 각 부분 코드 구현\n",
        "2. **데이터 준비 및 토크나이저**: 챗봇 데이터 전처리, SentencePiece 이용한 토크나이저를 학습\n",
        "3. **데이터셋 및 데이터로더 구축**: PyTorch Dataset 및 DataLoader 생성\n",
        "4. **모델 학습**: 정의한 모델과 데이터로더를 이용한 학습 진행, 검증 데이터셋으로 성능 측정\n",
        "5. **챗봇 성능 테스트 (추론)**: 학습된 모델로 새로운 문장에 대한 답변을 생성, 다양한 디코딩 전략을 비교\n",
        "\n",
        "**오늘의 회고:**\n",
        "1. LMS에서 기본 블럭들은 다 제공해줘서 조합정도만 했지만 이것저것 보면서 트랜스포머 모델을 만져본건 재밋었다.\n",
        "2. 확실히 에폭이 늘수록 로스는 줄기는 하지만 데이터 양이 절대적으로 부족해서 인지 학습을 많이 늘리더라도 비약적인 성능의 향상은 없었다.\n",
        "3. 디코딩 전략에 따라 같은 모델로도 다양한 결과? 가 나오는게 신기한 점"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bf974946",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import sentencepiece as spm\n",
        "\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc30cb1",
      "metadata": {},
      "source": [
        "## 1. 모델 구성요소 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7d2104b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 단어의 순서 정보를 모델에 알려주기 위한 클래스\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.position = position\n",
        "\n",
        "        self.pos_encoding = self._build_pos_encoding(position, d_model)\n",
        "\n",
        "    def _get_angles(self, position, i, d_model):\n",
        "        return 1.0 / (10000.0 ** ((2.0 * (i // 2)) / d_model)) * position\n",
        "\n",
        "    def _build_pos_encoding(self, position, d_model):\n",
        "        pos = torch.arange(position, dtype=torch.float32).unsqueeze(1)\n",
        "        i = torch.arange(d_model, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        angle_rads = self._get_angles(pos, i, d_model)\n",
        "        sines = torch.sin(angle_rads[:, 0::2])\n",
        "        cosines = torch.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = torch.zeros(position, d_model)\n",
        "        pos_encoding[:, 0::2] = sines\n",
        "        pos_encoding[:, 1::2] = cosines\n",
        "\n",
        "        pos_encoding = pos_encoding.unsqueeze(0)  # shape: [1, position, d_model]\n",
        "        return pos_encoding\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 입력 텐서(x)에 위치 정보를 더해 반환\n",
        "        return x + self.pos_encoding[:, :x.size(1), :].to(x.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "359e28fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 어텐션\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    # 1) Q와 K의 내적을 통해 score(유사도) 계산\n",
        "    matmul_qk = torch.matmul(query, key.transpose(-1, -2))\n",
        "\n",
        "    # 2) key의 차원 수(depth)의 제곱근으로 나누어 스케일링\n",
        "    depth = key.size(-1)\n",
        "    logits = matmul_qk / math.sqrt(depth)\n",
        "\n",
        "    # 3) 마스킹: 어텐션에서 특정 토큰을 무시하도록 처리\n",
        "    if mask is not None:\n",
        "        logits = logits + (mask * -1e9) # 아주 작은 값을 더해 softmax 결과가 0이 되도록 함\n",
        "\n",
        "    # 4) 소프트맥스 함수로 어텐션 가중치(확률 분포) 생성\n",
        "    attention_weights = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # 5) 어텐션 가중치와 V를 내적하여 최종 어텐션 값 계산\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e13ea1ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 멀티 헤드 어텐션: 어텐션을 여러 번 수행하여 다른 관점의 정보를 종합하는 클래스\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        # Q, K, V를 위한 Linear 레이어\n",
        "        self.query_dense = nn.Linear(d_model, d_model)\n",
        "        self.key_dense = nn.Linear(d_model, d_model)\n",
        "        self.value_dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 최종 출력을 위한 Linear 레이어\n",
        "        self.out_dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # d_model 차원을 num_heads와 depth로 분할\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Q, K, V에 각각 Linear 레이어 적용\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # 헤드 분할\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # 스케일드 닷 프로덕트 어텐션 수행\n",
        "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        # 헤드 다시 합치기\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()\n",
        "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # 최종 Linear 레이어 적용\n",
        "        output = self.out_dense(concat_attention)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9d3c59c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_padding_mask(x):\n",
        "    # x == 0 위치를 찾아 float형 1로 변환\n",
        "    mask = (x == 0).float()\n",
        "    # (batch_size, seq_len) -> (batch_size, 1, 1, seq_len)\n",
        "    mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ec929f64",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    seq_len = x.size(1)\n",
        "\n",
        "    # (seq_len, seq_len) 크기의 하삼각 행렬(tril) 생성 후 1에서 빼서\n",
        "    # 상삼각이 1, 하삼각(자기 자신 포함)이 0이 되도록 설정\n",
        "    # => 미래 토큰(자신 인덱스보다 큰 위치) 마스킹\n",
        "    look_ahead_mask = 1 - torch.tril(torch.ones((seq_len, seq_len)))\n",
        "\n",
        "    # 패딩 마스크 생성 (shape: (batch_size, 1, 1, seq_len))\n",
        "    padding_mask = create_padding_mask(x)\n",
        "\n",
        "    # look_ahead_mask: (seq_len, seq_len) -> (1, seq_len, seq_len)\n",
        "    look_ahead_mask = look_ahead_mask.unsqueeze(0)\n",
        "    # -> (1, seq_len, seq_len) -> (1, 1, seq_len, seq_len)\n",
        "    look_ahead_mask = look_ahead_mask.unsqueeze(1)\n",
        "    look_ahead_mask = look_ahead_mask.to(x.device)\n",
        "\n",
        "    # look-ahead 마스크와 패딩 마스크를 합성 (둘 중 하나라도 1이면 마스킹)\n",
        "    # 최종 shape은 브로드캐스팅으로 (batch_size, 1, seq_len, seq_len)\n",
        "    combined_mask = torch.max(look_ahead_mask, padding_mask)\n",
        "    return combined_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "47565600",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)  # 이전에 구현한 MHA\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        # 피드포워드 부분 (Dense -> ReLU -> Dense)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # (1) 멀티 헤드 어텐션 (셀프 어텐션)\n",
        "        attn_output = self.mha(x, x, x, mask)  # (batch_size, seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)     # 잔차 연결 + LayerNorm\n",
        "\n",
        "        # (2) 피드포워드 신경망\n",
        "        ffn_output = self.ffn(out1)            # (batch_size, seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.norm2(out1 + ffn_output)   # 잔차 연결 + LayerNorm\n",
        "\n",
        "        return out2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "31d2d4e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 num_layers,\n",
        "                 ff_dim,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # (1) 임베딩 레이어\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # (2) 포지셔널 인코딩\n",
        "        self.pos_encoding = PositionalEncoding(position=vocab_size, d_model=d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # (3) EncoderLayer 쌓기\n",
        "        self.enc_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # (1) 임베딩 & sqrt(d_model)로 스케일링\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # (2) 포지셔널 인코딩 적용 + 드롭아웃\n",
        "        x = self.pos_encoding(x)  # shape: (batch_size, seq_len, d_model)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # (3) num_layers만큼 쌓아올린 EncoderLayer 통과\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9c851975",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # 첫 번째 서브 레이어 (디코더 내부 셀프 어텐션)\n",
        "        self.self_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        # 두 번째 서브 레이어 (인코더-디코더 어텐션)\n",
        "        self.encdec_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        # 세 번째 서브 레이어 (피드포워드 네트워크)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),  # Dense(units=ff_dim)\n",
        "            nn.ReLU(),                   # activation='relu'\n",
        "            nn.Linear(ff_dim, d_model)   # Dense(units=d_model)\n",
        "        )\n",
        "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_outputs, look_ahead_mask=None, padding_mask=None):\n",
        "        # 1) 셀프 어텐션 (디코더 내부)\n",
        "        self_attn_out = self.self_mha(x, x, x, mask=look_ahead_mask)\n",
        "        self_attn_out = self.dropout1(self_attn_out)\n",
        "        out1 = self.norm1(x + self_attn_out)  # 잔차 연결 + LayerNorm\n",
        "\n",
        "        # 2) 인코더-디코더 어텐션\n",
        "        encdec_attn_out = self.encdec_mha(out1, enc_outputs, enc_outputs, mask=padding_mask)\n",
        "        encdec_attn_out = self.dropout2(encdec_attn_out)\n",
        "        out2 = self.norm2(out1 + encdec_attn_out)  # 잔차 연결 + LayerNorm\n",
        "\n",
        "        # 3) 피드포워드 (Dense -> ReLU -> Dense)\n",
        "        ffn_out = self.ffn(out2)\n",
        "        ffn_out = self.dropout3(ffn_out)\n",
        "        out3 = self.norm3(out2 + ffn_out)  # 잔차 연결 + LayerNorm\n",
        "\n",
        "        return out3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c11bc11f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 num_layers,\n",
        "                 ff_dim,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # (1) 임베딩 레이어\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # (2) 포지셔널 인코딩\n",
        "        # 실제 학습 시에는 최대 시퀀스 길이에 맞추어 쓰기도 함\n",
        "        self.pos_encoding = PositionalEncoding(position=vocab_size, d_model=d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # (3) DecoderLayer 쌓기\n",
        "        self.dec_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, enc_outputs, look_ahead_mask=None, padding_mask=None):\n",
        "        # (1) 임베딩 + sqrt(d_model)로 스케일링\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # (2) 포지셔널 인코딩 + 드롭아웃\n",
        "        x = self.pos_encoding(x)    # (batch_size, tgt_seq_len, d_model)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # (3) num_layers만큼 쌓인 DecoderLayer 통과\n",
        "        for layer in self.dec_layers:\n",
        "            x = layer(x, enc_outputs, look_ahead_mask, padding_mask)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a86fc89b",
      "metadata": {},
      "source": [
        "## 2. 데이터 준비 및 토크나이저"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eb19bd0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "    # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
        "    sentence = sentence.lower().strip()\n",
        "\n",
        "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
        "    # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
        "    # student와 온점 사이에 거리를 만듭니다.\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "    # (한글, 알파벳, 숫자, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
        "    sentence = re.sub(r\"[^가-힣a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "62a01022",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV 파일 읽어서 전처리 후, 텍스트 파일로 저장\n",
        "input_file = 'data/ChatbotData.csv'\n",
        "output_file = 'data/chatbot_corpus.txt'\n",
        "with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
        "        open(output_file, 'w', encoding='utf-8') as f_out:\n",
        "\n",
        "    reader = csv.reader(f_in)\n",
        "    next(reader)  # 헤더 행 건너뛰기\n",
        "\n",
        "    for row in reader:\n",
        "        if len(row) > 1:\n",
        "            question = row[0]\n",
        "            answer = row[1]\n",
        "\n",
        "            if question and isinstance(question, str):\n",
        "                processed_q = preprocess_sentence(question)\n",
        "                f_out.write(processed_q + \"\\n\")\n",
        "                count += 1\n",
        "\n",
        "            if answer and isinstance(answer, str):\n",
        "                processed_a = preprocess_sentence(answer)\n",
        "                f_out.write(processed_a + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5b947433",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: /Users/bychoi/develop/aiffel_quest_rs/Exploration/Ex07/data/chatbot_corpus.txt\n",
            "  input_format: \n",
            "  model_prefix: spm_chatbot\n",
            "  model_type: BPE\n",
            "  vocab_size: 8000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 1\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 999999\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 3\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: 0\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: /Users/bychoi/develop/aiffel_quest_rs/Exploration/Ex07/data/chatbot_corpus.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 23646 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=369459\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=1225\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 23646 sentences.\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 23646\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 20648\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13942 min_freq=42\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1055 size=20 all=16544 active=1772 piece=▁거예요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=776 size=40 all=17277 active=2505 piece=▁여\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=619 size=60 all=17948 active=3176 piece=▁좋아하는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=427 size=80 all=18555 active=3783 piece=▁모\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=358 size=100 all=19286 active=4514 piece=▁전\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=357 min_freq=37\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=307 size=120 all=19722 active=1402 piece=▁뭐\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=262 size=140 all=20320 active=2000 piece=▁소\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=227 size=160 all=20745 active=2425 piece=▁비\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=206 size=180 all=21106 active=2786 piece=▁행\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=200 all=21475 active=3155 piece=▁관\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=188 min_freq=31\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=172 size=220 all=21830 active=1422 piece=▁재\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155 size=240 all=22157 active=1749 piece=▁데\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=144 size=260 all=22480 active=2072 piece=▁자꾸\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=280 all=22863 active=2455 piece=▁고백\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=130 size=300 all=23218 active=2810 piece=▁먼저\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=130 min_freq=26\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=320 all=23552 active=1491 piece=▁헤어진지\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=340 all=23883 active=1822 piece=▁버\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=360 all=24101 active=2040 piece=▁돈\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=104 size=380 all=24428 active=2367 piece=▁하루\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99 size=400 all=24825 active=2764 piece=▁함\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=99 min_freq=22\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=420 all=25109 active=1524 piece=▁남자친구\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=440 all=25330 active=1745 piece=다가\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=85 size=460 all=25586 active=2001 piece=▁사람은\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=480 all=25827 active=2242 piece=하네\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=500 all=26100 active=2515 piece=▁설\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=77 min_freq=20\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=75 size=520 all=26325 active=1518 piece=▁어떨까요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72 size=540 all=26553 active=1746 piece=▁해도\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=560 all=26828 active=2021 piece=▁그만\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=580 all=26994 active=2187 piece=▁휴\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=600 all=27181 active=2374 piece=▁항상\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=64 min_freq=17\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=620 all=27347 active=1526 piece=▁문제\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=640 all=27613 active=1792 piece=▁상황\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=660 all=27861 active=2040 piece=▁개\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=680 all=28068 active=2247 piece=▁가보세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=700 all=28190 active=2369 piece=▁충분히\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=53 min_freq=16\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=720 all=28342 active=1562 piece=▁열심히\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=740 all=28482 active=1702 piece=▁종\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=760 all=28645 active=1865 piece=▁만남\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=780 all=28773 active=1993 piece=으니까요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=800 all=28918 active=2138 piece=▁부모님\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=47 min_freq=14\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=820 all=29024 active=1543 piece=▁헤어지고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=840 all=29157 active=1676 piece=더니\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=860 all=29298 active=1817 piece=▁살아\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=880 all=29392 active=1911 piece=▁잊혀\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=900 all=29510 active=2029 piece=▁재밌\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=41 min_freq=13\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=920 all=29653 active=1604 piece=▁소개팅\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=940 all=29769 active=1720 piece=▁끊\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=960 all=29964 active=1915 piece=▁잘하는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=980 all=30116 active=2067 piece=▁정도\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=1000 all=30262 active=2213 piece=▁붙잡\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=36 min_freq=12\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1020 all=30386 active=1626 piece=같아\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1040 all=30526 active=1766 piece=▁익\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1060 all=30661 active=1901 piece=드릴게요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1080 all=30810 active=2050 piece=▁힘내세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1100 all=30916 active=2156 piece=▁아닌데\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=32 min_freq=12\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1120 all=31008 active=1638 piece=▁무엇\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1140 all=31099 active=1729 piece=▁관리\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1160 all=31198 active=1828 piece=어야\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1180 all=31312 active=1942 piece=이었을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1200 all=31421 active=2051 piece=지고\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28 min_freq=11\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1220 all=31489 active=1625 piece=▁지금은\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1240 all=31575 active=1711 piece=▁부러\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1260 all=31612 active=1748 piece=▁필요해요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1280 all=31705 active=1841 piece=▁부분\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1300 all=31789 active=1925 piece=▁남편이\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26 min_freq=10\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1320 all=31845 active=1646 piece=밖에\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1340 all=31986 active=1787 piece=▁안나\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1360 all=32054 active=1855 piece=▁해봐요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1380 all=32134 active=1935 piece=▁맞춰\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1400 all=32219 active=2020 piece=▁생각만\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24 min_freq=10\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1420 all=32266 active=1657 piece=▁되네\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1440 all=32376 active=1767 piece=줬으면\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1460 all=32437 active=1828 piece=▁가야\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1480 all=32486 active=1877 piece=▁조언\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1500 all=32549 active=1940 piece=더라고요\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=22 min_freq=9\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1520 all=32665 active=1735 piece=한게\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1540 all=32757 active=1827 piece=▁태어\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1560 all=32801 active=1871 piece=▁받아들이는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1580 all=32896 active=1966 piece=식을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1600 all=33008 active=2078 piece=▁적응\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20 min_freq=9\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1620 all=33052 active=1687 piece=▁잊어버\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1640 all=33061 active=1696 piece=시켜\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1660 all=33184 active=1819 piece=▁어색\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1680 all=33274 active=1909 piece=▁���사람\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1700 all=33287 active=1922 piece=▁각\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=8\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1720 all=33408 active=1784 piece=었다\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1740 all=33502 active=1878 piece=▁안타\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1760 all=33560 active=1936 piece=▁성격이\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1780 all=33574 active=1950 piece=▁판\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1800 all=33715 active=2091 piece=▁끝은\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17 min_freq=8\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1820 all=33771 active=1742 piece=▁일도\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1840 all=33823 active=1794 piece=▁연습을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1860 all=33863 active=1834 piece=냐고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1880 all=33969 active=1940 piece=▁다니\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1900 all=34023 active=1994 piece=▁없애\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16 min_freq=8\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1920 all=34101 active=1773 piece=▁가자고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1940 all=34117 active=1789 piece=▁사랑한다고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1960 all=34174 active=1846 piece=어서\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1980 all=34264 active=1936 piece=▁되면\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2000 all=34341 active=2013 piece=▁있지\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=7\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2020 all=34380 active=1757 piece=▁쉬세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2040 all=34386 active=1763 piece=▁연락하지\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2060 all=34429 active=1806 piece=간이\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2080 all=34563 active=1940 piece=▁내는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2100 all=34603 active=1980 piece=▁조급\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14 min_freq=7\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2120 all=34634 active=1757 piece=▁상황이\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2140 all=34626 active=1749 piece=▁썸남한테\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2160 all=34612 active=1735 piece=▁잊어버리세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2180 all=34702 active=1825 piece=어진\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2200 all=34825 active=1948 piece=▁남의\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=7\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2220 all=34868 active=1785 piece=▁선톡\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2240 all=34939 active=1856 piece=▁줘도\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2260 all=34997 active=1914 piece=▁기념일\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2280 all=35005 active=1922 piece=▁연애를\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2300 all=35021 active=1938 piece=▁남자들은\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=7\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2320 all=35016 active=1747 piece=▁헤어졌는데\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2340 all=35076 active=1807 piece=렸어\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2360 all=35150 active=1881 piece=��다들\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2380 all=35178 active=1909 piece=▁신고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2400 all=35222 active=1953 piece=▁편한\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=6\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2420 all=35253 active=1793 piece=▁만난지\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2440 all=35248 active=1788 piece=▁운동을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2460 all=35231 active=1771 piece=▁안됐는데\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2480 all=35216 active=1756 piece=▁정리하는게\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2500 all=35266 active=1806 piece=가봐\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=6\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2520 all=35362 active=1852 piece=하러\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2540 all=35398 active=1888 piece=▁달달\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2560 all=35430 active=1920 piece=▁설렘\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2580 all=35477 active=1967 piece=▁전공\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2600 all=35546 active=2036 piece=하지만\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=6\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2620 all=35554 active=1775 piece=▁부족한\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2640 all=35545 active=1766 piece=▁집들이\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2660 all=35553 active=1774 piece=▁일이네요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2680 all=35538 active=1759 piece=▁김\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2700 all=35606 active=1827 piece=대폰\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2720 all=35686 active=1859 piece=하던\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2740 all=35725 active=1898 piece=▁마실\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2760 all=35761 active=1934 piece=▁부자\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2780 all=35802 active=1975 piece=▁죽고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2800 all=35863 active=2036 piece=하다가\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2820 all=35881 active=1804 piece=▁바쁘게\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2840 all=35876 active=1799 piece=▁연애가\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2860 all=35865 active=1788 piece=셔보세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2880 all=35874 active=1797 piece=▁여자들은\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2900 all=35862 active=1785 piece=▁지금이라도\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2920 all=35902 active=1834 piece=거죠\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2940 all=35997 active=1929 piece=보내\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2960 all=36094 active=2026 piece=▁곁에\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2980 all=36125 active=2057 piece=▁모른\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3000 all=36147 active=2079 piece=▁술마\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=5\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3020 all=36165 active=1821 piece=▁입을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3040 all=36207 active=1863 piece=르세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3060 all=36258 active=1914 piece=▁꽃다발\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3080 all=36259 active=1915 piece=▁보내는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3100 all=36248 active=1904 piece=▁위로봇\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=5\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3120 all=36244 active=1807 piece=▁차라리\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3140 all=36236 active=1799 piece=▁마련이죠\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3160 all=36227 active=1790 piece=▁짝사랑은\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3180 all=36220 active=1783 piece=▁마지막으로\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3200 all=36217 active=1780 piece=▁얻\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3220 all=36266 active=1854 piece=만은\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3240 all=36342 active=1930 piece=활동\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3260 all=36364 active=1952 piece=▁다닐\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3280 all=36397 active=1985 piece=▁반응\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3300 all=36434 active=2022 piece=▁시키\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3320 all=36471 active=1855 piece=▁이직\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3340 all=36492 active=1876 piece=▁쳐다\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3360 all=36534 active=1918 piece=겼는데\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3380 all=36612 active=1996 piece=▁같아서\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3400 all=36605 active=1989 piece=▁늘어요\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3420 all=36597 active=1823 piece=▁뭐할까\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3440 all=36588 active=1814 piece=▁아이돌\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3460 all=36587 active=1813 piece=▁찾기를\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3480 all=36600 active=1826 piece=▁고백하면\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3500 all=36582 active=1808 piece=▁싫어하는\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3520 all=36580 active=1828 piece=▁헤어지자\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3540 all=36564 active=1812 piece=▁취해보세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3560 all=36572 active=1820 piece=▁색\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3580 all=36633 active=1881 piece=년을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3600 all=36695 active=1943 piece=순간\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3620 all=36752 active=1886 piece=콜릿\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3640 all=36768 active=1902 piece=▁길어\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3660 all=36781 active=1915 piece=▁동성\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3680 all=36805 active=1939 piece=▁사내\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3700 all=36822 active=1956 piece=▁어필\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3720 all=36835 active=1852 piece=▁종일\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3740 all=36864 active=1881 piece=▁했나\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3760 all=36919 active=1936 piece=할거라\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3780 all=36925 active=1942 piece=▁끝났네\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3800 all=36910 active=1927 piece=▁말해도\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3820 all=36909 active=1845 piece=▁상대가\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3840 all=36898 active=1834 piece=▁앞두고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3860 all=36900 active=1836 piece=▁자세히\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3880 all=36895 active=1831 piece=▁하려고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3900 all=36895 active=1831 piece=▁당연하죠\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3920 all=36880 active=1830 piece=▁신혼여행\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3940 all=36870 active=1820 piece=▁카페에서\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3960 all=36858 active=1808 piece=▁만남이었길\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3980 all=36842 active=1792 piece=▁풀어보세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4000 all=36849 active=1799 piece=▁롱\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4020 all=36891 active=1883 piece=곰히\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4040 all=36949 active=1941 piece=면증\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4060 all=37005 active=1997 piece=젯밤\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4080 all=37053 active=2045 piece=화가\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4100 all=37075 active=2067 piece=▁귀를\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4120 all=37096 active=1875 piece=▁대중\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4140 all=37111 active=1890 piece=▁며칠\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4160 all=37129 active=1908 piece=▁성적\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4180 all=37142 active=1921 piece=▁썼어\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4200 all=37155 active=1934 piece=▁왕따\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4220 all=37168 active=1866 piece=▁자취\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4240 all=37187 active=1885 piece=▁차갑\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4260 all=37218 active=1916 piece=▁평온\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4280 all=37238 active=1936 piece=근차근\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4300 all=37290 active=1988 piece=적이고\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4320 all=37317 active=1889 piece=▁것들이\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4340 all=37305 active=1877 piece=▁나가기\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4360 all=37293 active=1865 piece=▁떠보는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4380 all=37283 active=1855 piece=▁미래에\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4400 all=37272 active=1844 piece=▁서로가\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4420 all=37259 active=1851 piece=▁알았어\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4440 all=37245 active=1837 piece=▁이해할\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4460 all=37240 active=1832 piece=▁집착해\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4480 all=37235 active=1827 piece=▁하거나\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4500 all=37240 active=1832 piece=▁고민하게\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4520 all=37227 active=1849 piece=▁만들려고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4540 all=37212 active=1834 piece=▁소개시켜\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4560 all=37206 active=1828 piece=▁인정받고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4580 all=37194 active=1816 piece=▁학교에서\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4600 all=37182 active=1804 piece=▁사랑한다는\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4620 all=37164 active=1841 piece=▁준비하세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4640 all=37147 active=1824 piece=▁피곤한가봐요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4660 all=37159 active=1836 piece=▁씌\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4680 all=37202 active=1879 piece=난거\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4700 all=37236 active=1913 piece=문이\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4720 all=37293 active=1915 piece=오를\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4740 all=37352 active=1974 piece=증을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4760 all=37382 active=2004 piece=▁거는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4780 all=37391 active=2013 piece=▁끓여\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4800 all=37404 active=2026 piece=▁답은\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4820 all=37396 active=1863 piece=▁모를\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4840 all=37410 active=1877 piece=▁번씩\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4860 all=37413 active=1880 piece=▁설렜\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4880 all=37433 active=1900 piece=▁아마\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4900 all=37447 active=1914 piece=▁이모\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4920 all=37466 active=1890 piece=▁지낼\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4940 all=37483 active=1907 piece=▁타다\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4960 all=37507 active=1931 piece=끄러워\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4980 all=37556 active=1980 piece=정적인\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5000 all=37572 active=1996 piece=▁공부할\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5020 all=37568 active=1874 piece=▁다르죠\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5040 all=37565 active=1871 piece=▁마셔도\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5060 all=37557 active=1863 piece=▁못했던\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5080 all=37545 active=1851 piece=▁봤는데\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5100 all=37536 active=1842 piece=▁습관이\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5120 all=37525 active=1866 piece=▁여행은\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5140 all=37520 active=1861 piece=▁재밌다\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5160 all=37508 active=1849 piece=▁찾아온\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5180 all=37499 active=1840 piece=▁한숨만\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5200 all=37502 active=1843 piece=해야겠다\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5220 all=37502 active=1871 piece=▁도전하는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5240 all=37487 active=1856 piece=▁보고싶네\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5260 all=37472 active=1841 piece=▁아는데도\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5280 all=37460 active=1829 piece=��이어나가\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5300 all=37446 active=1815 piece=▁좋았으면\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5320 all=37432 active=1859 piece=▁후회하기\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5340 all=37420 active=1847 piece=▁맡겨보세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5360 all=37402 active=1829 piece=▁잃어버렸어\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5380 all=37388 active=1815 piece=▁좋아해줬으면\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5400 all=37404 active=1831 piece=▁민\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5420 all=37417 active=1882 piece=▁핫\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5440 all=37458 active=1923 piece=기에\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5460 all=37491 active=1956 piece=력에\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5480 all=37519 active=1984 piece=받을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5500 all=37561 active=2026 piece=양제\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5520 all=37599 active=1916 piece=조건\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5540 all=37640 active=1957 piece=하진\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5560 all=37655 active=1972 piece=▁갠톡\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5580 all=37656 active=1973 piece=▁꼬박\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5600 all=37649 active=1966 piece=▁농구\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5620 all=37655 active=1886 piece=▁뒤에\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5640 all=37649 active=1880 piece=▁맞아\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5660 all=37657 active=1888 piece=▁믿지\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5680 all=37671 active=1902 piece=▁봐주\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5700 all=37693 active=1924 piece=▁선풍\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5720 all=37708 active=1897 piece=▁안고\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5740 all=37712 active=1901 piece=▁예상\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5760 all=37715 active=1904 piece=▁웨딩\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5780 all=37727 active=1916 piece=▁잠들\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5800 all=37740 active=1929 piece=▁증명\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5820 all=37756 active=1902 piece=▁청춘\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5840 all=37763 active=1909 piece=▁톡이\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5860 all=37768 active=1914 piece=▁현타\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5880 all=37783 active=1929 piece=렵니다\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5900 all=37816 active=1962 piece=하기로\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5920 all=37844 active=1915 piece=▁가야돼\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5940 all=37829 active=1900 piece=▁고시원\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5960 all=37814 active=1885 piece=▁기억은\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5980 all=37805 active=1876 piece=▁내일을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6000 all=37791 active=1862 piece=▁되려고\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6020 all=37781 active=1880 piece=▁레시피\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6040 all=37772 active=1871 piece=▁매력을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6060 all=37757 active=1856 piece=▁미련을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6080 all=37745 active=1844 piece=▁부족해\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6100 all=37731 active=1830 piece=▁생각도\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6120 all=37725 active=1881 piece=▁시선을\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6140 all=37710 active=1866 piece=▁아이디\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6160 all=37702 active=1858 piece=▁어떡함\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6180 all=37689 active=1845 piece=▁예뻐질\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6200 all=37677 active=1833 piece=▁의심해\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6220 all=37670 active=1876 piece=▁있거나\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6240 all=37664 active=1870 piece=▁접는게\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6260 all=37650 active=1856 piece=▁즐기는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6280 all=37634 active=1840 piece=▁커피는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6300 all=37617 active=1823 piece=▁함께도\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6320 all=37607 active=1871 piece=드시거나\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6340 all=37631 active=1895 piece=▁가리세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6360 all=37620 active=1884 piece=▁관심사에\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6380 all=37604 active=1868 piece=▁낭만적인\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6400 all=37586 active=1850 piece=▁동생한테\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6420 all=37567 active=1860 piece=▁무한리필\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6440 all=37556 active=1849 piece=▁뿌린대로\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6460 all=37539 active=1832 piece=▁쉬어가도\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6480 all=37524 active=1817 piece=▁안타깝게\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6500 all=37506 active=1799 piece=▁연애세포\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6520 all=37491 active=1860 piece=▁이왕이면\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6540 all=37478 active=1847 piece=▁저마다의\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6560 all=37460 active=1829 piece=▁친구인데\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6580 all=37444 active=1813 piece=▁해봤는데\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6600 all=37435 active=1804 piece=▁가족들이랑\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6620 all=37419 active=1856 piece=▁놀러가세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6640 all=37399 active=1836"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " piece=▁받아들여야\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6660 all=37381 active=1818 piece=▁세워보세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6680 all=37361 active=1798 piece=▁연락하는게\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6700 all=37342 active=1779 piece=▁정리하세요\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6720 all=37325 active=1851 piece=▁표현해보는\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6740 all=37307 active=1833 piece=▁되짚어보세요\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6760 all=37287 active=1813 piece=▁필요해보여요\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: spm_chatbot.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_chatbot.vocab\n"
          ]
        }
      ],
      "source": [
        "# 이전에 생성한 텍스트 파일 경로\n",
        "corpus_file = '/Users/bychoi/develop/aiffel_quest_rs/Exploration/Ex07/data/chatbot_corpus.txt'\n",
        "\n",
        "# SentencePiece 모델 학습\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=corpus_file,\n",
        "    model_prefix='spm_chatbot',\n",
        "    vocab_size=8000,\n",
        "    character_coverage=1.0,\n",
        "    model_type='bpe',\n",
        "    max_sentence_length=999999,\n",
        "    bos_id=1,\n",
        "    eos_id=2,\n",
        "    pad_id=0,\n",
        "    unk_id=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "272283dc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"spm_chatbot.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b42aa39b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전처리 후의 문장: 안녕하세요 , 저는 챗봇입니다 . 반갑습니다 ! 벌써 12시네요 .\n",
            "Tokenized: ['▁안녕하세요', '▁,', '▁저는', '▁', '챗', '봇', '입니다', '▁.', '▁반갑', '습니다', '▁!', '▁벌써', '▁12', '시', '네요', '▁.']\n",
            "Encoded: [4615, 422, 617, 6775, 7969, 7411, 405, 4, 4838, 151, 108, 1033, 5550, 6817, 33, 4]\n",
            "Decoded: 안녕하세요 , 저는 챗봇입니다 . 반갑습니다 ! 벌써 12시네요 .\n"
          ]
        }
      ],
      "source": [
        "# 예제 문장\n",
        "sentence = \"안녕하세요, 저는 챗봇입니다. 반갑습니다! 벌써 12시네요.\"\n",
        "\n",
        "sentence = preprocess_sentence(sentence)\n",
        "print(\"전처리 후의 문장:\", sentence)\n",
        "\n",
        "# 1. 토크나이징 (subword 단위로 분할)\n",
        "tokens = sp.encode(sentence, out_type=str)\n",
        "print(\"Tokenized:\", tokens)\n",
        "\n",
        "# 2. 인코딩 (서브워드를 정수 ID로 변환)\n",
        "encoded = sp.encode(sentence, out_type=int)\n",
        "print(\"Encoded:\", encoded)\n",
        "\n",
        "# 3. 디코딩 (정수 ID → 원본 문장 복원)\n",
        "decoded = sp.decode(encoded)\n",
        "print(\"Decoded:\", decoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee78a2a",
      "metadata": {},
      "source": [
        "## 3. 데이터셋 및 데이터로더 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cafc2969",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chatbot 데이터셋 클래스 정의\n",
        "class ChatbotDataset(Dataset):\n",
        "    def __init__(self, csv_file, sp_model, max_length=40):\n",
        "        super().__init__()\n",
        "        self.sp = sp_model\n",
        "        self.max_length = max_length\n",
        "        self.data = []\n",
        "\n",
        "        # CSV 파일에서 (질문, 답변) 쌍을 읽어옵니다.\n",
        "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader)  # 헤더 행 건너뛰기\n",
        "            pairs = [(row[0], row[1]) for row in reader]\n",
        "\n",
        "        for q_text, a_text in pairs:\n",
        "            # 1) SentencePiece 모델로 문장을 토큰 ID 배열로 변환\n",
        "            q_ids = self.sp.EncodeAsIds(q_text)\n",
        "            a_ids = self.sp.EncodeAsIds(a_text)\n",
        "\n",
        "            # 2) 문장 시작/끝을 나타내는 스페셜 토큰 ID 추가\n",
        "            bos_id = self.sp.bos_id()\n",
        "            eos_id = self.sp.eos_id()\n",
        "            pad_id = self.sp.pad_id()\n",
        "\n",
        "            enc_input = [bos_id] + q_ids + [eos_id]\n",
        "            dec_input = [bos_id] + a_ids\n",
        "            target = a_ids + [eos_id]\n",
        "\n",
        "            # 3) 최대 길이를 초과하는 샘플은 제외\n",
        "            if len(enc_input) > max_length or len(target) > max_length:\n",
        "                continue\n",
        "\n",
        "            # 4) 패딩(Padding) 추가하여 모든 문장의 길이를 통일\n",
        "            enc_input += [pad_id] * (max_length - len(enc_input))\n",
        "            dec_input += [pad_id] * (max_length - len(dec_input))\n",
        "            target += [pad_id] * (max_length - len(target))\n",
        "\n",
        "            # 전처리 완료된 데이터를 self.data에 저장\n",
        "            self.data.append({\n",
        "                \"enc_input\": enc_input,\n",
        "                \"dec_input\": dec_input,\n",
        "                \"target\": target\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        # 데이터를 파이토치 텐서로 변환\n",
        "        enc_input = torch.tensor(sample[\"enc_input\"], dtype=torch.long)\n",
        "        dec_input = torch.tensor(sample[\"dec_input\"], dtype=torch.long)\n",
        "        target = torch.tensor(sample[\"target\"], dtype=torch.long)\n",
        "        return enc_input, dec_input, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b74db4e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 문장 길이 토큰 분포 통계 ---\n",
            "최대 길이: 29\n",
            "최소 길이: 1\n",
            "평균 길이: 5.59\n",
            "중앙값: 5.0\n",
            "--------------------\n",
            "90% 지점: 9.00\n",
            "95% 지점: 10.00\n",
            "99% 지점: 13.00\n",
            "99.9% 지점: 19.00\n",
            "--------------------\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# CSV 파일을 읽어 모든 문장의 토큰 길이를 계산\n",
        "csv_file_path = 'data/ChatbotData.csv'\n",
        "token_lengths = []\n",
        "\n",
        "with open(csv_file_path, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # 헤더 건너뛰기\n",
        "    for row in reader:\n",
        "        if len(row) > 1:\n",
        "            # 질문과 답변 모두에 대해 처리\n",
        "            for sentence in [row[0], row[1]]:\n",
        "                if sentence and isinstance(sentence, str):\n",
        "                    processed = preprocess_sentence(sentence)\n",
        "                    ids = sp.EncodeAsIds(processed)\n",
        "                    token_lengths.append(len(ids))\n",
        "\n",
        "# 길이 분포 통계 계산\n",
        "token_lengths = np.array(token_lengths)\n",
        "\n",
        "print(\"--- 문장 길이 토큰 분포 통계 ---\")\n",
        "print(f\"최대 길이: {np.max(token_lengths)}\")\n",
        "print(f\"최소 길이: {np.min(token_lengths)}\")\n",
        "print(f\"평균 길이: {np.mean(token_lengths):.2f}\")\n",
        "print(f\"중앙값: {np.median(token_lengths)}\")\n",
        "print(\"-\" * 20)\n",
        "# 백분위수(Percentile) 계산\n",
        "print(f\"90% 지점: {np.percentile(token_lengths, 90):.2f}\")\n",
        "print(f\"95% 지점: {np.percentile(token_lengths, 95):.2f}\")\n",
        "print(f\"99% 지점: {np.percentile(token_lengths, 99):.2f}\")\n",
        "print(f\"99.9% 지점: {np.percentile(token_lengths, 99.9):.2f}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# 히스토그램으로 시각화\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(token_lengths, bins=50, alpha=0.7, color='blue')\n",
        "plt.title('Sentence Token Length Distribution')\n",
        "plt.xlabel('Token Length')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# 99% 지점에 수직선 추가\n",
        "percentile_99 = np.percentile(token_lengths, 99)\n",
        "plt.axvline(percentile_99, color='red', linestyle='dashed', linewidth=2)\n",
        "plt.text(percentile_99 + 1, plt.ylim()[1] * 0.9, f'99th percentile: {percentile_99:.0f}', color='red')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ee9b5cbb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 샘플 수: 11638개\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 및 데이터로더 생성\n",
        "csv_file_path = 'data/ChatbotData.csv'\n",
        "BATCH_SIZE = 64 \n",
        "\n",
        "dataset = ChatbotDataset(csv_file_path, sp, max_length=15)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(f\"총 샘플 수: {len(dataset)}개\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e119f1a4",
      "metadata": {},
      "source": [
        "## 4. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "700d3464",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 num_layers,      # 인코더/디코더 층 수\n",
        "                 units,           # feed-forward 네트워크의 중간 차원(ff_dim)\n",
        "                 d_model,         # 임베딩 및 내부 표현 차원\n",
        "                 num_heads,       # 멀티헤드 어텐션의 헤드 수\n",
        "                 dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # 인코더\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=vocab_size,\n",
        "            num_layers=num_layers,\n",
        "            ff_dim=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # 디코더\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size=vocab_size,\n",
        "            num_layers=num_layers,\n",
        "            ff_dim=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # 최종 출력층: (d_model) -> (vocab_size)\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # 참고: 텐서플로우 코드의 `name=\"transformer\"`는 파이토치에선 보통 사용 안 함\n",
        "\n",
        "    def forward(self, inputs, dec_inputs):\n",
        "        # 1) 인코더 패딩 마스크 생성\n",
        "        enc_padding_mask = create_padding_mask(inputs)     # shape (batch_size, 1, 1, src_seq_len)\n",
        "\n",
        "        # 2) 디코더 look-ahead + 패딩 마스크\n",
        "        look_ahead_mask = create_look_ahead_mask(dec_inputs)  # shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "\n",
        "        # 3) 디코더에서 인코더 출력 쪽을 마스킹할 때 쓸 패딩 마스크\n",
        "        dec_padding_mask = create_padding_mask(inputs)        # shape (batch_size, 1, 1, src_seq_len)\n",
        "\n",
        "        # 4) 인코더 수행\n",
        "        enc_outputs = self.encoder(\n",
        "            x=inputs,\n",
        "            mask=enc_padding_mask\n",
        "        )  # shape: (batch_size, src_seq_len, d_model)\n",
        "\n",
        "        # 5) 디코더 수행\n",
        "        dec_outputs = self.decoder(\n",
        "            x=dec_inputs,           # (batch_size, tgt_seq_len)\n",
        "            enc_outputs=enc_outputs,# (batch_size, src_seq_len, d_model)\n",
        "            look_ahead_mask=look_ahead_mask,\n",
        "            padding_mask=dec_padding_mask\n",
        "        )  # shape: (batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "        # 6) 최종 Dense (vocab_size)\n",
        "        logits = self.final_linear(dec_outputs)  # (batch_size, tgt_seq_len, vocab_size)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c810f565",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(8000, 512)\n",
            "    (pos_encoding): PositionalEncoding()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (enc_layers): ModuleList(\n",
            "      (0-3): 4 x EncoderLayer(\n",
            "        (mha): MultiHeadAttention(\n",
            "          (query_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (key_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (value_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=1024, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(8000, 512)\n",
            "    (pos_encoding): PositionalEncoding()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (dec_layers): ModuleList(\n",
            "      (0-3): 4 x DecoderLayer(\n",
            "        (self_mha): MultiHeadAttention(\n",
            "          (query_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (key_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (value_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (encdec_mha): MultiHeadAttention(\n",
            "          (query_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (key_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (value_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=1024, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (final_linear): Linear(in_features=512, out_features=8000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# 하이퍼파라미터 설정\n",
        "NUM_LAYERS = 4     # 인코더/디코더 층 수\n",
        "D_MODEL = 512      # 임베딩 및 내부 표현 차원\n",
        "NUM_HEADS = 8      # 멀티헤드 어텐션에서의 헤드 수\n",
        "UNITS = 1024        # 피드포워드 신경망의 은닉 차원\n",
        "DROPOUT = 0.1      # 드롭아웃 비율\n",
        "VOCAB_SIZE = 8000  # 단어 집합 크기(예시)\n",
        "\n",
        "# 모델 생성\n",
        "model = Transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2e56ac37",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = nn.CrossEntropyLoss(ignore_index=sp.pad_id())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "801a4063",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lr_lambda(d_model, warmup_steps=4000):\n",
        "    d_model = float(d_model)\n",
        "    def lr_lambda(step):\n",
        "        # step은 0부터 시작하므로 +1로 보정\n",
        "        step = step + 1\n",
        "        return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
        "    return lr_lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6a115c28",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 하이퍼파라미터 설정\n",
        "d_model = 512\n",
        "warmup_steps = 4000\n",
        "total_steps = 200000  # 총 학습 스텝\n",
        "\n",
        "# 학습률 스케줄 시각화\n",
        "steps = np.arange(1, total_steps + 1)\n",
        "learning_rates = [get_lr_lambda(d_model, warmup_steps)(step) for step in steps]\n",
        "\n",
        "# 그래프 출력\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps, learning_rates, label=\"Learning Rate\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Transformer Learning Rate Schedule\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7cab19dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer 정의\n",
        "optimizer = optim.AdamW(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Scheduler 정의\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda(D_MODEL, warmup_steps=4000))\n",
        "\n",
        "def accuracy_function(y_pred, y_true, pad_id=0):\n",
        "    \"\"\"\n",
        "    y_pred: (batch_size, seq_len, vocab_size)\n",
        "    y_true: (batch_size, seq_len)\n",
        "    \"\"\"\n",
        "    preds = y_pred.argmax(dim=-1)  # (batch_size, seq_len)\n",
        "    mask = (y_true != pad_id)\n",
        "    correct = (preds == y_true) & mask\n",
        "    acc = correct.float().sum() / mask.float().sum()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "021d7a1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1992025c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_step(model, batch, optimizer, loss_function, device):\n",
        "    model.train()\n",
        "    enc_input, dec_input, target = [x.to(device) for x in batch]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 모델 포워드 패스\n",
        "    logits = model(enc_input, dec_input)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "    # Loss 계산 (패딩 토큰 무시)\n",
        "    loss = loss_function(logits.permute(0, 2, 1), target)  # (batch_size, vocab_size, seq_len) 필요\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), accuracy_function(logits, target, pad_id=sp.pad_id())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5f91cf51",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, loss_function, scheduler, num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss, total_acc = 0, 0\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            loss, acc = train_step(model, batch, optimizer, loss_function, device)\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "\n",
        "            # 일정 스텝마다 로그 출력\n",
        "            if step % 100 == 0:\n",
        "                print(f\"[Epoch {epoch+1}, Step {step}] Loss: {loss:.4f}, Acc: {acc:.4f}\")\n",
        "\n",
        "            # 학습률 스케줄러 업데이트\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        avg_acc = total_acc / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1} Completed - Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "00557583",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1, Step 0] Loss: 9.1905, Acc: 0.0000\n",
            "[Epoch 1, Step 100] Loss: 9.1116, Acc: 0.0000\n",
            "Epoch 1 Completed - Avg Loss: 9.1510, Avg Acc: 0.0001\n",
            "[Epoch 2, Step 0] Loss: 9.1543, Acc: 0.0000\n",
            "[Epoch 2, Step 100] Loss: 9.0896, Acc: 0.0000\n",
            "Epoch 2 Completed - Avg Loss: 9.1056, Avg Acc: 0.0001\n",
            "[Epoch 3, Step 0] Loss: 9.0767, Acc: 0.0000\n",
            "[Epoch 3, Step 100] Loss: 9.0173, Acc: 0.0000\n",
            "Epoch 3 Completed - Avg Loss: 9.0157, Avg Acc: 0.0002\n",
            "[Epoch 4, Step 0] Loss: 8.9663, Acc: 0.0000\n",
            "[Epoch 4, Step 100] Loss: 8.8303, Acc: 0.0072\n",
            "Epoch 4 Completed - Avg Loss: 8.8811, Avg Acc: 0.0009\n",
            "[Epoch 5, Step 0] Loss: 8.8055, Acc: 0.0023\n",
            "[Epoch 5, Step 100] Loss: 8.7343, Acc: 0.0224\n",
            "Epoch 5 Completed - Avg Loss: 8.6997, Avg Acc: 0.0386\n",
            "[Epoch 6, Step 0] Loss: 8.5830, Acc: 0.1077\n",
            "[Epoch 6, Step 100] Loss: 8.4536, Acc: 0.1609\n",
            "Epoch 6 Completed - Avg Loss: 8.4757, Avg Acc: 0.1578\n",
            "[Epoch 7, Step 0] Loss: 8.2634, Acc: 0.2260\n",
            "[Epoch 7, Step 100] Loss: 8.2127, Acc: 0.2005\n",
            "Epoch 7 Completed - Avg Loss: 8.2103, Avg Acc: 0.2140\n",
            "[Epoch 8, Step 0] Loss: 8.1231, Acc: 0.2215\n",
            "[Epoch 8, Step 100] Loss: 7.8545, Acc: 0.2311\n",
            "Epoch 8 Completed - Avg Loss: 7.9154, Avg Acc: 0.2290\n",
            "[Epoch 9, Step 0] Loss: 7.6863, Acc: 0.2382\n",
            "[Epoch 9, Step 100] Loss: 7.6192, Acc: 0.2240\n",
            "Epoch 9 Completed - Avg Loss: 7.6071, Avg Acc: 0.2342\n",
            "[Epoch 10, Step 0] Loss: 7.4737, Acc: 0.2244\n",
            "[Epoch 10, Step 100] Loss: 7.3412, Acc: 0.2348\n",
            "Epoch 10 Completed - Avg Loss: 7.3266, Avg Acc: 0.2468\n",
            "[Epoch 11, Step 0] Loss: 7.2759, Acc: 0.2533\n",
            "[Epoch 11, Step 100] Loss: 7.0282, Acc: 0.2806\n",
            "Epoch 11 Completed - Avg Loss: 7.1038, Avg Acc: 0.2698\n",
            "[Epoch 12, Step 0] Loss: 7.0867, Acc: 0.2676\n",
            "[Epoch 12, Step 100] Loss: 6.9485, Acc: 0.2909\n",
            "Epoch 12 Completed - Avg Loss: 6.9594, Avg Acc: 0.2863\n",
            "[Epoch 13, Step 0] Loss: 6.9819, Acc: 0.2886\n",
            "[Epoch 13, Step 100] Loss: 6.7532, Acc: 0.3092\n",
            "Epoch 13 Completed - Avg Loss: 6.8696, Avg Acc: 0.2912\n",
            "[Epoch 14, Step 0] Loss: 6.8910, Acc: 0.2838\n",
            "[Epoch 14, Step 100] Loss: 6.7258, Acc: 0.3046\n",
            "Epoch 14 Completed - Avg Loss: 6.8009, Avg Acc: 0.2922\n",
            "[Epoch 15, Step 0] Loss: 6.9176, Acc: 0.2677\n",
            "[Epoch 15, Step 100] Loss: 6.7673, Acc: 0.2864\n",
            "Epoch 15 Completed - Avg Loss: 6.7428, Avg Acc: 0.2924\n",
            "[Epoch 16, Step 0] Loss: 6.6866, Acc: 0.3000\n",
            "[Epoch 16, Step 100] Loss: 6.6521, Acc: 0.2972\n",
            "Epoch 16 Completed - Avg Loss: 6.6900, Avg Acc: 0.2924\n",
            "[Epoch 17, Step 0] Loss: 6.6357, Acc: 0.2926\n",
            "[Epoch 17, Step 100] Loss: 6.7258, Acc: 0.2806\n",
            "Epoch 17 Completed - Avg Loss: 6.6420, Avg Acc: 0.2924\n",
            "[Epoch 18, Step 0] Loss: 6.3878, Acc: 0.3177\n",
            "[Epoch 18, Step 100] Loss: 6.7569, Acc: 0.2739\n",
            "Epoch 18 Completed - Avg Loss: 6.5979, Avg Acc: 0.2925\n",
            "[Epoch 19, Step 0] Loss: 6.7354, Acc: 0.2681\n",
            "[Epoch 19, Step 100] Loss: 6.6694, Acc: 0.2838\n",
            "Epoch 19 Completed - Avg Loss: 6.5564, Avg Acc: 0.2925\n",
            "[Epoch 20, Step 0] Loss: 6.5133, Acc: 0.3012\n",
            "[Epoch 20, Step 100] Loss: 6.5765, Acc: 0.2870\n",
            "Epoch 20 Completed - Avg Loss: 6.5178, Avg Acc: 0.2923\n",
            "[Epoch 21, Step 0] Loss: 6.5017, Acc: 0.2948\n",
            "[Epoch 21, Step 100] Loss: 6.5626, Acc: 0.2851\n",
            "Epoch 21 Completed - Avg Loss: 6.4810, Avg Acc: 0.2924\n",
            "[Epoch 22, Step 0] Loss: 6.3459, Acc: 0.2981\n",
            "[Epoch 22, Step 100] Loss: 6.5404, Acc: 0.2812\n",
            "Epoch 22 Completed - Avg Loss: 6.4453, Avg Acc: 0.2925\n",
            "[Epoch 23, Step 0] Loss: 6.3673, Acc: 0.2962\n",
            "[Epoch 23, Step 100] Loss: 6.4023, Acc: 0.2880\n",
            "Epoch 23 Completed - Avg Loss: 6.4118, Avg Acc: 0.2924\n",
            "[Epoch 24, Step 0] Loss: 6.4089, Acc: 0.2905\n",
            "[Epoch 24, Step 100] Loss: 6.3854, Acc: 0.2927\n",
            "Epoch 24 Completed - Avg Loss: 6.3804, Avg Acc: 0.2925\n",
            "[Epoch 25, Step 0] Loss: 6.5033, Acc: 0.2860\n",
            "[Epoch 25, Step 100] Loss: 6.2511, Acc: 0.3036\n",
            "Epoch 25 Completed - Avg Loss: 6.3524, Avg Acc: 0.2924\n",
            "[Epoch 26, Step 0] Loss: 6.4628, Acc: 0.2785\n",
            "[Epoch 26, Step 100] Loss: 6.2430, Acc: 0.3014\n",
            "Epoch 26 Completed - Avg Loss: 6.3255, Avg Acc: 0.2925\n",
            "[Epoch 27, Step 0] Loss: 6.1484, Acc: 0.3106\n",
            "[Epoch 27, Step 100] Loss: 6.0309, Acc: 0.3282\n",
            "Epoch 27 Completed - Avg Loss: 6.3015, Avg Acc: 0.2924\n",
            "[Epoch 28, Step 0] Loss: 6.3310, Acc: 0.2847\n",
            "[Epoch 28, Step 100] Loss: 6.0874, Acc: 0.3084\n",
            "Epoch 28 Completed - Avg Loss: 6.2788, Avg Acc: 0.2924\n",
            "[Epoch 29, Step 0] Loss: 6.4003, Acc: 0.2709\n",
            "[Epoch 29, Step 100] Loss: 6.4714, Acc: 0.2658\n",
            "Epoch 29 Completed - Avg Loss: 6.2566, Avg Acc: 0.2925\n",
            "[Epoch 30, Step 0] Loss: 6.3196, Acc: 0.2890\n",
            "[Epoch 30, Step 100] Loss: 6.1963, Acc: 0.2871\n",
            "Epoch 30 Completed - Avg Loss: 6.2344, Avg Acc: 0.2926\n",
            "[Epoch 31, Step 0] Loss: 6.0456, Acc: 0.3045\n",
            "[Epoch 31, Step 100] Loss: 6.2889, Acc: 0.2719\n",
            "Epoch 31 Completed - Avg Loss: 6.2154, Avg Acc: 0.2926\n",
            "[Epoch 32, Step 0] Loss: 6.3471, Acc: 0.2673\n",
            "[Epoch 32, Step 100] Loss: 6.0558, Acc: 0.2988\n",
            "Epoch 32 Completed - Avg Loss: 6.1978, Avg Acc: 0.2928\n",
            "[Epoch 33, Step 0] Loss: 6.1176, Acc: 0.2918\n",
            "[Epoch 33, Step 100] Loss: 6.0805, Acc: 0.3014\n",
            "Epoch 33 Completed - Avg Loss: 6.1795, Avg Acc: 0.2931\n",
            "[Epoch 34, Step 0] Loss: 6.2012, Acc: 0.2962\n",
            "[Epoch 34, Step 100] Loss: 6.1104, Acc: 0.2926\n",
            "Epoch 34 Completed - Avg Loss: 6.1616, Avg Acc: 0.2934\n",
            "[Epoch 35, Step 0] Loss: 6.3301, Acc: 0.2822\n",
            "[Epoch 35, Step 100] Loss: 6.1838, Acc: 0.2857\n",
            "Epoch 35 Completed - Avg Loss: 6.1457, Avg Acc: 0.2937\n",
            "[Epoch 36, Step 0] Loss: 6.1098, Acc: 0.2909\n",
            "[Epoch 36, Step 100] Loss: 6.1624, Acc: 0.2940\n",
            "Epoch 36 Completed - Avg Loss: 6.1279, Avg Acc: 0.2941\n",
            "[Epoch 37, Step 0] Loss: 6.2378, Acc: 0.2752\n",
            "[Epoch 37, Step 100] Loss: 5.9897, Acc: 0.3012\n",
            "Epoch 37 Completed - Avg Loss: 6.1121, Avg Acc: 0.2943\n",
            "[Epoch 38, Step 0] Loss: 6.1455, Acc: 0.2934\n",
            "[Epoch 38, Step 100] Loss: 6.2509, Acc: 0.2834\n",
            "Epoch 38 Completed - Avg Loss: 6.0972, Avg Acc: 0.2945\n",
            "[Epoch 39, Step 0] Loss: 6.0447, Acc: 0.2964\n",
            "[Epoch 39, Step 100] Loss: 6.2318, Acc: 0.2755\n",
            "Epoch 39 Completed - Avg Loss: 6.0825, Avg Acc: 0.2949\n",
            "[Epoch 40, Step 0] Loss: 5.8631, Acc: 0.3206\n",
            "[Epoch 40, Step 100] Loss: 6.2499, Acc: 0.2749\n",
            "Epoch 40 Completed - Avg Loss: 6.0688, Avg Acc: 0.2952\n",
            "[Epoch 41, Step 0] Loss: 6.0176, Acc: 0.3025\n",
            "[Epoch 41, Step 100] Loss: 6.0046, Acc: 0.2979\n",
            "Epoch 41 Completed - Avg Loss: 6.0545, Avg Acc: 0.2952\n",
            "[Epoch 42, Step 0] Loss: 6.1063, Acc: 0.2905\n",
            "[Epoch 42, Step 100] Loss: 6.0144, Acc: 0.3095\n",
            "Epoch 42 Completed - Avg Loss: 6.0415, Avg Acc: 0.2954\n",
            "[Epoch 43, Step 0] Loss: 6.0108, Acc: 0.2960\n",
            "[Epoch 43, Step 100] Loss: 5.9917, Acc: 0.2950\n",
            "Epoch 43 Completed - Avg Loss: 6.0291, Avg Acc: 0.2956\n",
            "[Epoch 44, Step 0] Loss: 6.0748, Acc: 0.2912\n",
            "[Epoch 44, Step 100] Loss: 6.0340, Acc: 0.2806\n",
            "Epoch 44 Completed - Avg Loss: 6.0160, Avg Acc: 0.2956\n",
            "[Epoch 45, Step 0] Loss: 6.0328, Acc: 0.2930\n",
            "[Epoch 45, Step 100] Loss: 6.2036, Acc: 0.2835\n",
            "Epoch 45 Completed - Avg Loss: 6.0034, Avg Acc: 0.2959\n",
            "[Epoch 46, Step 0] Loss: 5.8492, Acc: 0.3057\n",
            "[Epoch 46, Step 100] Loss: 5.9584, Acc: 0.2979\n",
            "Epoch 46 Completed - Avg Loss: 5.9905, Avg Acc: 0.2963\n",
            "[Epoch 47, Step 0] Loss: 5.7341, Acc: 0.3206\n",
            "[Epoch 47, Step 100] Loss: 5.8007, Acc: 0.3199\n",
            "Epoch 47 Completed - Avg Loss: 5.9819, Avg Acc: 0.2961\n",
            "[Epoch 48, Step 0] Loss: 5.8134, Acc: 0.3002\n",
            "[Epoch 48, Step 100] Loss: 6.1965, Acc: 0.2645\n",
            "Epoch 48 Completed - Avg Loss: 5.9688, Avg Acc: 0.2965\n",
            "[Epoch 49, Step 0] Loss: 6.1387, Acc: 0.2768\n",
            "[Epoch 49, Step 100] Loss: 5.8741, Acc: 0.3144\n",
            "Epoch 49 Completed - Avg Loss: 5.9595, Avg Acc: 0.2967\n",
            "[Epoch 50, Step 0] Loss: 5.8584, Acc: 0.2920\n",
            "[Epoch 50, Step 100] Loss: 5.9231, Acc: 0.2968\n",
            "Epoch 50 Completed - Avg Loss: 5.9474, Avg Acc: 0.2969\n",
            "[Epoch 51, Step 0] Loss: 6.0375, Acc: 0.2823\n",
            "[Epoch 51, Step 100] Loss: 6.0283, Acc: 0.2993\n",
            "Epoch 51 Completed - Avg Loss: 5.9379, Avg Acc: 0.2971\n",
            "[Epoch 52, Step 0] Loss: 5.7590, Acc: 0.3113\n",
            "[Epoch 52, Step 100] Loss: 5.6742, Acc: 0.3145\n",
            "Epoch 52 Completed - Avg Loss: 5.9274, Avg Acc: 0.2973\n",
            "[Epoch 53, Step 0] Loss: 6.0257, Acc: 0.2908\n",
            "[Epoch 53, Step 100] Loss: 5.8205, Acc: 0.3014\n",
            "Epoch 53 Completed - Avg Loss: 5.9162, Avg Acc: 0.2976\n",
            "[Epoch 54, Step 0] Loss: 5.7811, Acc: 0.2969\n",
            "[Epoch 54, Step 100] Loss: 6.0574, Acc: 0.2944\n",
            "Epoch 54 Completed - Avg Loss: 5.9078, Avg Acc: 0.2977\n",
            "[Epoch 55, Step 0] Loss: 5.9721, Acc: 0.2892\n",
            "[Epoch 55, Step 100] Loss: 5.9482, Acc: 0.2962\n",
            "Epoch 55 Completed - Avg Loss: 5.8991, Avg Acc: 0.2976\n",
            "[Epoch 56, Step 0] Loss: 5.9111, Acc: 0.3084\n",
            "[Epoch 56, Step 100] Loss: 6.0231, Acc: 0.2906\n",
            "Epoch 56 Completed - Avg Loss: 5.8911, Avg Acc: 0.2980\n",
            "[Epoch 57, Step 0] Loss: 5.9911, Acc: 0.2888\n",
            "[Epoch 57, Step 100] Loss: 5.8407, Acc: 0.2910\n",
            "Epoch 57 Completed - Avg Loss: 5.8819, Avg Acc: 0.2983\n",
            "[Epoch 58, Step 0] Loss: 5.9551, Acc: 0.2848\n",
            "[Epoch 58, Step 100] Loss: 6.0460, Acc: 0.2897\n",
            "Epoch 58 Completed - Avg Loss: 5.8721, Avg Acc: 0.2983\n",
            "[Epoch 59, Step 0] Loss: 5.8643, Acc: 0.3007\n",
            "[Epoch 59, Step 100] Loss: 5.8177, Acc: 0.3062\n",
            "Epoch 59 Completed - Avg Loss: 5.8639, Avg Acc: 0.2988\n",
            "[Epoch 60, Step 0] Loss: 5.7391, Acc: 0.3136\n",
            "[Epoch 60, Step 100] Loss: 5.9162, Acc: 0.3002\n",
            "Epoch 60 Completed - Avg Loss: 5.8557, Avg Acc: 0.2990\n",
            "[Epoch 61, Step 0] Loss: 5.8477, Acc: 0.2956\n",
            "[Epoch 61, Step 100] Loss: 5.8593, Acc: 0.2910\n",
            "Epoch 61 Completed - Avg Loss: 5.8468, Avg Acc: 0.2997\n",
            "[Epoch 62, Step 0] Loss: 5.8499, Acc: 0.3002\n",
            "[Epoch 62, Step 100] Loss: 5.8732, Acc: 0.2934\n",
            "Epoch 62 Completed - Avg Loss: 5.8391, Avg Acc: 0.2998\n",
            "[Epoch 63, Step 0] Loss: 5.8947, Acc: 0.2902\n",
            "[Epoch 63, Step 100] Loss: 5.7660, Acc: 0.3041\n",
            "Epoch 63 Completed - Avg Loss: 5.8317, Avg Acc: 0.3004\n",
            "[Epoch 64, Step 0] Loss: 5.6433, Acc: 0.3351\n",
            "[Epoch 64, Step 100] Loss: 5.8106, Acc: 0.3135\n",
            "Epoch 64 Completed - Avg Loss: 5.8245, Avg Acc: 0.3006\n",
            "[Epoch 65, Step 0] Loss: 5.6664, Acc: 0.3220\n",
            "[Epoch 65, Step 100] Loss: 5.8398, Acc: 0.2995\n",
            "Epoch 65 Completed - Avg Loss: 5.8174, Avg Acc: 0.3009\n",
            "[Epoch 66, Step 0] Loss: 5.8450, Acc: 0.2956\n",
            "[Epoch 66, Step 100] Loss: 5.7628, Acc: 0.3108\n",
            "Epoch 66 Completed - Avg Loss: 5.8102, Avg Acc: 0.3011\n",
            "[Epoch 67, Step 0] Loss: 5.9436, Acc: 0.2739\n",
            "[Epoch 67, Step 100] Loss: 5.9789, Acc: 0.2895\n",
            "Epoch 67 Completed - Avg Loss: 5.8028, Avg Acc: 0.3017\n",
            "[Epoch 68, Step 0] Loss: 5.6688, Acc: 0.3194\n",
            "[Epoch 68, Step 100] Loss: 5.8217, Acc: 0.2870\n",
            "Epoch 68 Completed - Avg Loss: 5.7962, Avg Acc: 0.3021\n",
            "[Epoch 69, Step 0] Loss: 5.6588, Acc: 0.3037\n",
            "[Epoch 69, Step 100] Loss: 5.7157, Acc: 0.2981\n",
            "Epoch 69 Completed - Avg Loss: 5.7890, Avg Acc: 0.3023\n",
            "[Epoch 70, Step 0] Loss: 5.6783, Acc: 0.3086\n",
            "[Epoch 70, Step 100] Loss: 5.7086, Acc: 0.2993\n",
            "Epoch 70 Completed - Avg Loss: 5.7818, Avg Acc: 0.3030\n",
            "[Epoch 71, Step 0] Loss: 5.6684, Acc: 0.3248\n",
            "[Epoch 71, Step 100] Loss: 5.8578, Acc: 0.2829\n",
            "Epoch 71 Completed - Avg Loss: 5.7760, Avg Acc: 0.3031\n",
            "[Epoch 72, Step 0] Loss: 5.8850, Acc: 0.2958\n",
            "[Epoch 72, Step 100] Loss: 5.9954, Acc: 0.2809\n",
            "Epoch 72 Completed - Avg Loss: 5.7704, Avg Acc: 0.3032\n",
            "[Epoch 73, Step 0] Loss: 5.6582, Acc: 0.3153\n",
            "[Epoch 73, Step 100] Loss: 5.8207, Acc: 0.2915\n",
            "Epoch 73 Completed - Avg Loss: 5.7644, Avg Acc: 0.3035\n",
            "[Epoch 74, Step 0] Loss: 5.8590, Acc: 0.3052\n",
            "[Epoch 74, Step 100] Loss: 5.4779, Acc: 0.3350\n",
            "Epoch 74 Completed - Avg Loss: 5.7571, Avg Acc: 0.3039\n",
            "[Epoch 75, Step 0] Loss: 5.7872, Acc: 0.2995\n",
            "[Epoch 75, Step 100] Loss: 5.6357, Acc: 0.3228\n",
            "Epoch 75 Completed - Avg Loss: 5.7502, Avg Acc: 0.3041\n",
            "[Epoch 76, Step 0] Loss: 5.8699, Acc: 0.2829\n",
            "[Epoch 76, Step 100] Loss: 5.8062, Acc: 0.2936\n",
            "Epoch 76 Completed - Avg Loss: 5.7460, Avg Acc: 0.3046\n",
            "[Epoch 77, Step 0] Loss: 5.7838, Acc: 0.2923\n",
            "[Epoch 77, Step 100] Loss: 5.6555, Acc: 0.3122\n",
            "Epoch 77 Completed - Avg Loss: 5.7384, Avg Acc: 0.3050\n",
            "[Epoch 78, Step 0] Loss: 5.7081, Acc: 0.3024\n",
            "[Epoch 78, Step 100] Loss: 5.7835, Acc: 0.2897\n",
            "Epoch 78 Completed - Avg Loss: 5.7341, Avg Acc: 0.3050\n",
            "[Epoch 79, Step 0] Loss: 5.6037, Acc: 0.3207\n",
            "[Epoch 79, Step 100] Loss: 5.4735, Acc: 0.3276\n",
            "Epoch 79 Completed - Avg Loss: 5.7273, Avg Acc: 0.3054\n",
            "[Epoch 80, Step 0] Loss: 5.7931, Acc: 0.3059\n",
            "[Epoch 80, Step 100] Loss: 5.7792, Acc: 0.3073\n",
            "Epoch 80 Completed - Avg Loss: 5.7230, Avg Acc: 0.3055\n",
            "[Epoch 81, Step 0] Loss: 5.3669, Acc: 0.3584\n",
            "[Epoch 81, Step 100] Loss: 5.6261, Acc: 0.3135\n",
            "Epoch 81 Completed - Avg Loss: 5.7169, Avg Acc: 0.3057\n",
            "[Epoch 82, Step 0] Loss: 5.7440, Acc: 0.2931\n",
            "[Epoch 82, Step 100] Loss: 5.5556, Acc: 0.3118\n",
            "Epoch 82 Completed - Avg Loss: 5.7118, Avg Acc: 0.3059\n",
            "[Epoch 83, Step 0] Loss: 5.8528, Acc: 0.2896\n",
            "[Epoch 83, Step 100] Loss: 5.7307, Acc: 0.2902\n",
            "Epoch 83 Completed - Avg Loss: 5.7052, Avg Acc: 0.3061\n",
            "[Epoch 84, Step 0] Loss: 5.6867, Acc: 0.3038\n",
            "[Epoch 84, Step 100] Loss: 6.0509, Acc: 0.2590\n",
            "Epoch 84 Completed - Avg Loss: 5.7006, Avg Acc: 0.3062\n",
            "[Epoch 85, Step 0] Loss: 5.7142, Acc: 0.3039\n",
            "[Epoch 85, Step 100] Loss: 5.6557, Acc: 0.3107\n",
            "Epoch 85 Completed - Avg Loss: 5.6979, Avg Acc: 0.3061\n",
            "[Epoch 86, Step 0] Loss: 5.5899, Acc: 0.3170\n",
            "[Epoch 86, Step 100] Loss: 5.5236, Acc: 0.3237\n",
            "Epoch 86 Completed - Avg Loss: 5.6913, Avg Acc: 0.3064\n",
            "[Epoch 87, Step 0] Loss: 5.9164, Acc: 0.2889\n",
            "[Epoch 87, Step 100] Loss: 5.7239, Acc: 0.3077\n",
            "Epoch 87 Completed - Avg Loss: 5.6856, Avg Acc: 0.3067\n",
            "[Epoch 88, Step 0] Loss: 5.7560, Acc: 0.2844\n",
            "[Epoch 88, Step 100] Loss: 5.7597, Acc: 0.3050\n",
            "Epoch 88 Completed - Avg Loss: 5.6803, Avg Acc: 0.3068\n",
            "[Epoch 89, Step 0] Loss: 5.9595, Acc: 0.2741\n",
            "[Epoch 89, Step 100] Loss: 5.6604, Acc: 0.3116\n",
            "Epoch 89 Completed - Avg Loss: 5.6770, Avg Acc: 0.3070\n",
            "[Epoch 90, Step 0] Loss: 5.8846, Acc: 0.2870\n",
            "[Epoch 90, Step 100] Loss: 5.6395, Acc: 0.3113\n",
            "Epoch 90 Completed - Avg Loss: 5.6739, Avg Acc: 0.3070\n",
            "[Epoch 91, Step 0] Loss: 5.5161, Acc: 0.3216\n",
            "[Epoch 91, Step 100] Loss: 5.7623, Acc: 0.3014\n",
            "Epoch 91 Completed - Avg Loss: 5.6678, Avg Acc: 0.3071\n",
            "[Epoch 92, Step 0] Loss: 5.5142, Acc: 0.3141\n",
            "[Epoch 92, Step 100] Loss: 5.5466, Acc: 0.3221\n",
            "Epoch 92 Completed - Avg Loss: 5.6633, Avg Acc: 0.3071\n",
            "[Epoch 93, Step 0] Loss: 5.7349, Acc: 0.3111\n",
            "[Epoch 93, Step 100] Loss: 5.7217, Acc: 0.3138\n",
            "Epoch 93 Completed - Avg Loss: 5.6588, Avg Acc: 0.3072\n",
            "[Epoch 94, Step 0] Loss: 5.4956, Acc: 0.3144\n",
            "[Epoch 94, Step 100] Loss: 5.8583, Acc: 0.2796\n",
            "Epoch 94 Completed - Avg Loss: 5.6539, Avg Acc: 0.3074\n",
            "[Epoch 95, Step 0] Loss: 5.6191, Acc: 0.3052\n",
            "[Epoch 95, Step 100] Loss: 5.5140, Acc: 0.3046\n",
            "Epoch 95 Completed - Avg Loss: 5.6499, Avg Acc: 0.3077\n",
            "[Epoch 96, Step 0] Loss: 5.8242, Acc: 0.2967\n",
            "[Epoch 96, Step 100] Loss: 5.6138, Acc: 0.3109\n",
            "Epoch 96 Completed - Avg Loss: 5.6466, Avg Acc: 0.3077\n",
            "[Epoch 97, Step 0] Loss: 5.5609, Acc: 0.3019\n",
            "[Epoch 97, Step 100] Loss: 5.7566, Acc: 0.3039\n",
            "Epoch 97 Completed - Avg Loss: 5.6425, Avg Acc: 0.3079\n",
            "[Epoch 98, Step 0] Loss: 5.6597, Acc: 0.3016\n",
            "[Epoch 98, Step 100] Loss: 5.7391, Acc: 0.2890\n",
            "Epoch 98 Completed - Avg Loss: 5.6383, Avg Acc: 0.3080\n",
            "[Epoch 99, Step 0] Loss: 5.5485, Acc: 0.3260\n",
            "[Epoch 99, Step 100] Loss: 5.6539, Acc: 0.3227\n",
            "Epoch 99 Completed - Avg Loss: 5.6327, Avg Acc: 0.3080\n",
            "[Epoch 100, Step 0] Loss: 5.4053, Acc: 0.3246\n",
            "[Epoch 100, Step 100] Loss: 5.6841, Acc: 0.3055\n",
            "Epoch 100 Completed - Avg Loss: 5.6299, Avg Acc: 0.3083\n",
            "CPU times: user 43min 33s, sys: 7min 38s, total: 51min 12s\n",
            "Wall time: 1h 12min 9s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_function=loss_function,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=25,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1382ba05",
      "metadata": {},
      "source": [
        "## 5. 챗봇 성능 테스트 (추론)\n",
        "- LMS에 나왔던 그리디 서치 이외에 빔 서치, Top-K 샘플링으로 다른 디코딩 전략 테스트\n",
        "- 레이블 스무딩을 적용해 과적합이나 하나의 답변에 모델이 정착하려고 하는 현상 방지 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7c4c54d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def decoder_inference(model, sentence, tokenizer, device='cpu'):\n",
        "    START_TOKEN = tokenizer.bos_id()\n",
        "    END_TOKEN = tokenizer.eos_id()\n",
        "    MAX_LENGTH = 40\n",
        "\n",
        "\n",
        "    # 전처리\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    # 인코더 입력: [START] + 인코딩 + [END]\n",
        "    enc_input_ids = [START_TOKEN] + tokenizer.encode(sentence) + [END_TOKEN]\n",
        "    # 차원 확장: (batch_size=1, seq_len)\n",
        "    enc_input = torch.tensor([enc_input_ids], dtype=torch.long, device=device)\n",
        "\n",
        "    # 디코더 입력(dec_input)을 START_TOKEN만 포함한 상태로 시작\n",
        "    dec_input = torch.tensor([[START_TOKEN]], dtype=torch.long, device=device)\n",
        "\n",
        "    model.eval()  # 모델 평가 모드\n",
        "    with torch.no_grad():\n",
        "        for i in range(MAX_LENGTH):\n",
        "            # 모델 forward: (enc_input, dec_input) -> (batch_size=1, seq_len, vocab_size)\n",
        "            logits = model(enc_input, dec_input)\n",
        "\n",
        "            # 마지막 타임스텝의 예측만 추출: shape (1, 1, vocab_size)\n",
        "            # logits[:, -1, :] -> (1, vocab_size)\n",
        "            last_step_logits = logits[:, -1, :]\n",
        "\n",
        "            # argmax로 가장 높은 확률의 토큰 선택\n",
        "            predicted_id = torch.argmax(last_step_logits, dim=-1)  # shape: (1,)\n",
        "\n",
        "            # 종료 토큰이면 중단\n",
        "            if predicted_id.item() == END_TOKEN:\n",
        "                break\n",
        "\n",
        "            # 디코더 입력(dec_input)에 예측 토큰을 이어붙임\n",
        "            predicted_id = predicted_id.unsqueeze(0)  # shape (1,1)\n",
        "            dec_input = torch.cat([dec_input, predicted_id], dim=1)\n",
        "\n",
        "    # 최종 시퀀스: dec_input: (1, seq_len)에서 (seq_len,)로\n",
        "    output_sequence = dec_input.squeeze(0).tolist()  # e.g. [START_TOKEN, ..., 토큰들...]\n",
        "\n",
        "    return output_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f949ed31",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentence_generation(model, sentence, tokenizer, device='cpu'):\n",
        "    # 디코더 인퍼런스 -> 예측된 토큰 시퀀스\n",
        "    output_seq = decoder_inference(model, sentence, tokenizer, device=device)\n",
        "\n",
        "    # 토크나이저로 디코딩 (패딩, START/END 토큰 등은 제외하거나 처리)\n",
        "    # 여기서는 단순히 tokenizer.decode() 직접 호출\n",
        "    predicted_sentence = tokenizer.decode(\n",
        "        [token for token in output_seq if token < tokenizer.GetPieceSize()]\n",
        "    )\n",
        "\n",
        "    print(\"입력 :\", sentence)\n",
        "    print(\"출력 :\", predicted_sentence)\n",
        "    return predicted_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1f9ae55e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 벌써 12시네\n",
            "출력 : 좋은 수 있을 거예요.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'좋은 수 있을 거예요.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = '벌써 12시네'\n",
        "sentence_generation(model, sentence, sp, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "91ab7c64",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 배고프다.\n",
            "출력 : 좋은 수 있을 거예요.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'좋은 수 있을 거예요.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = '배고프다.'\n",
        "sentence_generation(model, sentence, sp, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ca112263",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Greedy Search 결과 (비교용) ---\n",
            "입력 : 벌써 12시네\n",
            "출력 : 좋은 수 있을 거예요.\n",
            "입력 : 배고프다.\n",
            "출력 : 좋은 수 있을 거예요.\n",
            "\n",
            "--- Beam Search 결과 (beam_width=5) ---\n",
            "입력 : 벌써 12시네\n",
            "출력 : 좋은 수 있을 거예요.\n",
            "입력 : 배고프다.\n",
            "출력 : 좋은.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'좋은.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def beam_search_decoder(model, sentence, tokenizer, device='cpu', beam_width=5, max_len=15):\n",
        "    \"\"\"\n",
        "    빔 서치 디코딩을 수행하는 함수\n",
        "    :param model: 학습된 트랜스포머 모델\n",
        "    :param sentence: 입력 문장 (string)\n",
        "    :param tokenizer: 학습된 SentencePiece 토크나이저\n",
        "    :param device: 실행 디바이스\n",
        "    :param beam_width: 빔의 크기 (후보 수)\n",
        "    :param max_len: 생성할 문장의 최대 길이\n",
        "    :return: 생성된 문장의 토큰 ID 리스트\n",
        "    \"\"\"\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    # --- 입력 문장 전처리 ---\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    # --- 인코더 준비 ---\n",
        "    enc_input_ids = [tokenizer.bos_id()] + tokenizer.encode(preprocessed_sentence) + [tokenizer.eos_id()]\n",
        "    enc_input = torch.tensor([enc_input_ids], dtype=torch.long, device=device)\n",
        "\n",
        "    # --- 빔 서치 시작 ---\n",
        "    # 빔(beam)은 (시퀀스, 누적 점수)의 튜플 리스트로 구성됩니다.\n",
        "    # 점수는 로그 확률(log probability)을 사용하며, 높을수록 좋습니다.\n",
        "    initial_beam = ([tokenizer.bos_id()], 0.0)\n",
        "    beams = [initial_beam]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            all_candidates = [] # 이번 스텝의 모든 후보군을 저장할 리스트\n",
        "\n",
        "            # 현재 빔에 있는 각 후보 시퀀스에 대해 다음 토큰을 예측\n",
        "            for seq, score in beams:\n",
        "                # 마지막 토큰이 EOS이면, 해당 시퀀스는 완료된 것으로 간주\n",
        "                if seq[-1] == tokenizer.eos_id():\n",
        "                    all_candidates.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # 디코더 입력 준비\n",
        "                dec_input = torch.tensor([seq], dtype=torch.long, device=device)\n",
        "\n",
        "                # 모델 예측\n",
        "                logits = model(enc_input, dec_input)\n",
        "                last_step_logits = logits[:, -1, :] # 마지막 스텝의 로짓만 사용\n",
        "                log_probs = F.log_softmax(last_step_logits, dim=-1) # 로짓을 로그 확률로 변환\n",
        "\n",
        "                # 확률이 높은 상위 beam_width개의 토큰과 그 확률을 가져옴\n",
        "                top_log_probs, top_ids = torch.topk(log_probs, beam_width, dim=-1)\n",
        "\n",
        "                # 새로운 후보 시퀀스 생성\n",
        "                for i in range(beam_width):\n",
        "                    next_token_id = top_ids[0][i].item()\n",
        "                    next_token_log_prob = top_log_probs[0][i].item()\n",
        "\n",
        "                    # 새로운 시퀀스와 누적 점수 계산\n",
        "                    new_seq = seq + [next_token_id]\n",
        "                    new_score = score + next_token_log_prob\n",
        "                    all_candidates.append((new_seq, new_score))\n",
        "\n",
        "            # --- 빔 업데이트 ---\n",
        "            # 모든 후보들을 누적 점수(로그 확률) 기준으로 내림차순 정렬\n",
        "            sorted_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # 가장 점수가 높은 상위 beam_width개의 후보만 새로운 빔으로 선택\n",
        "            beams = sorted_candidates[:beam_width]\n",
        "\n",
        "            # 모든 빔이 EOS로 끝났으면 종료\n",
        "            if all(seq[-1] == tokenizer.eos_id() for seq, _ in beams):\n",
        "                break\n",
        "\n",
        "    # 최종적으로 가장 점수가 높은 시퀀스를 반환\n",
        "    best_seq, _ = beams[0]\n",
        "    return best_seq\n",
        "\n",
        "\n",
        "def sentence_generation_beam_search(model, sentence, tokenizer, device='cpu', beam_width=5):\n",
        "    \"\"\"빔 서치 결과를 사람이 읽을 수 있는 문장으로 변환하는 래퍼 함수\"\"\"\n",
        "\n",
        "    output_seq = beam_search_decoder(\n",
        "        model, sentence, tokenizer, device=device, beam_width=beam_width\n",
        "    )\n",
        "\n",
        "    # 시작 토큰(BOS)은 제외하고 디코딩\n",
        "    predicted_sentence = tokenizer.decode(output_seq[1:])\n",
        "\n",
        "    print(\"입력 :\", sentence)\n",
        "    print(\"출력 :\", predicted_sentence)\n",
        "    return predicted_sentence\n",
        "\n",
        "# --- 테스트 ---\n",
        "print(\"--- Greedy Search 결과 (비교용) ---\")\n",
        "sentence_generation(model, '벌써 12시네', sp, device)\n",
        "sentence_generation(model, '배고프다.', sp, device)\n",
        "\n",
        "print(\"\\n--- Beam Search 결과 (beam_width=5) ---\")\n",
        "sentence_generation_beam_search(model, '벌써 12시네', sp, device, beam_width=5)\n",
        "sentence_generation_beam_search(model, '배고프다.', sp, device, beam_width=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "916ecb81",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 샘플 수: 11638개\n",
            "훈련 샘플 수: 10474개\n",
            "검증 샘플 수: 1164개\n",
            "[Epoch 001] Train Loss: 9.2106, Train Acc: 0.0001 | Val Loss: 9.1978, Val Acc: 0.0000\n",
            "  -> Best model saved with val_loss: 9.1978\n",
            "[Epoch 002] Train Loss: 9.1739, Train Acc: 0.0001 | Val Loss: 9.1386, Val Acc: 0.0000\n",
            "  -> Best model saved with val_loss: 9.1386\n",
            "[Epoch 003] Train Loss: 9.0977, Train Acc: 0.0001 | Val Loss: 9.0400, Val Acc: 0.0000\n",
            "  -> Best model saved with val_loss: 9.0400\n",
            "[Epoch 004] Train Loss: 8.9849, Train Acc: 0.0003 | Val Loss: 8.9025, Val Acc: 0.0002\n",
            "  -> Best model saved with val_loss: 8.9025\n",
            "[Epoch 005] Train Loss: 8.8348, Train Acc: 0.0028 | Val Loss: 8.7265, Val Acc: 0.0117\n",
            "  -> Best model saved with val_loss: 8.7265\n",
            "[Epoch 006] Train Loss: 8.6489, Train Acc: 0.0563 | Val Loss: 8.5136, Val Acc: 0.1592\n",
            "  -> Best model saved with val_loss: 8.5136\n",
            "[Epoch 007] Train Loss: 8.4307, Train Acc: 0.1806 | Val Loss: 8.2672, Val Acc: 0.2312\n",
            "  -> Best model saved with val_loss: 8.2672\n",
            "[Epoch 008] Train Loss: 8.1779, Train Acc: 0.2406 | Val Loss: 7.9939, Val Acc: 0.2505\n",
            "  -> Best model saved with val_loss: 7.9939\n",
            "[Epoch 009] Train Loss: 7.9075, Train Acc: 0.2546 | Val Loss: 7.7096, Val Acc: 0.2569\n",
            "  -> Best model saved with val_loss: 7.7096\n",
            "[Epoch 010] Train Loss: 7.6334, Train Acc: 0.2632 | Val Loss: 7.4367, Val Acc: 0.2644\n",
            "  -> Best model saved with val_loss: 7.4367\n",
            "[Epoch 011] Train Loss: 7.3783, Train Acc: 0.2722 | Val Loss: 7.2064, Val Acc: 0.2742\n",
            "  -> Best model saved with val_loss: 7.2064\n",
            "[Epoch 012] Train Loss: 7.1718, Train Acc: 0.2809 | Val Loss: 7.0431, Val Acc: 0.2844\n",
            "  -> Best model saved with val_loss: 7.0431\n",
            "[Epoch 013] Train Loss: 7.0261, Train Acc: 0.2866 | Val Loss: 6.9416, Val Acc: 0.2894\n",
            "  -> Best model saved with val_loss: 6.9416\n",
            "[Epoch 014] Train Loss: 6.9296, Train Acc: 0.2893 | Val Loss: 6.8687, Val Acc: 0.2907\n",
            "  -> Best model saved with val_loss: 6.8687\n",
            "[Epoch 015] Train Loss: 6.8566, Train Acc: 0.2914 | Val Loss: 6.8044, Val Acc: 0.2914\n",
            "  -> Best model saved with val_loss: 6.8044\n",
            "[Epoch 016] Train Loss: 6.7943, Train Acc: 0.2919 | Val Loss: 6.7472, Val Acc: 0.2917\n",
            "  -> Best model saved with val_loss: 6.7472\n",
            "[Epoch 017] Train Loss: 6.7356, Train Acc: 0.2924 | Val Loss: 6.6971, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.6971\n",
            "[Epoch 018] Train Loss: 6.6861, Train Acc: 0.2925 | Val Loss: 6.6534, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.6534\n",
            "[Epoch 019] Train Loss: 6.6402, Train Acc: 0.2925 | Val Loss: 6.6132, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.6132\n",
            "[Epoch 020] Train Loss: 6.5992, Train Acc: 0.2927 | Val Loss: 6.5769, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.5769\n",
            "[Epoch 021] Train Loss: 6.5621, Train Acc: 0.2925 | Val Loss: 6.5426, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.5426\n",
            "[Epoch 022] Train Loss: 6.5251, Train Acc: 0.2926 | Val Loss: 6.5104, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.5104\n",
            "[Epoch 023] Train Loss: 6.4917, Train Acc: 0.2926 | Val Loss: 6.4798, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.4798\n",
            "[Epoch 024] Train Loss: 6.4585, Train Acc: 0.2926 | Val Loss: 6.4502, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.4502\n",
            "[Epoch 025] Train Loss: 6.4270, Train Acc: 0.2925 | Val Loss: 6.4223, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.4223\n",
            "[Epoch 026] Train Loss: 6.3975, Train Acc: 0.2925 | Val Loss: 6.3965, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.3965\n",
            "[Epoch 027] Train Loss: 6.3712, Train Acc: 0.2925 | Val Loss: 6.3726, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.3726\n",
            "[Epoch 028] Train Loss: 6.3460, Train Acc: 0.2925 | Val Loss: 6.3503, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.3503\n",
            "[Epoch 029] Train Loss: 6.3212, Train Acc: 0.2926 | Val Loss: 6.3292, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.3292\n",
            "[Epoch 030] Train Loss: 6.2986, Train Acc: 0.2926 | Val Loss: 6.3096, Val Acc: 0.2921\n",
            "  -> Best model saved with val_loss: 6.3096\n",
            "[Epoch 031] Train Loss: 6.2770, Train Acc: 0.2927 | Val Loss: 6.2908, Val Acc: 0.2922\n",
            "  -> Best model saved with val_loss: 6.2908\n",
            "[Epoch 032] Train Loss: 6.2568, Train Acc: 0.2928 | Val Loss: 6.2727, Val Acc: 0.2922\n",
            "  -> Best model saved with val_loss: 6.2727\n",
            "[Epoch 033] Train Loss: 6.2381, Train Acc: 0.2928 | Val Loss: 6.2555, Val Acc: 0.2928\n",
            "  -> Best model saved with val_loss: 6.2555\n",
            "[Epoch 034] Train Loss: 6.2196, Train Acc: 0.2930 | Val Loss: 6.2390, Val Acc: 0.2930\n",
            "  -> Best model saved with val_loss: 6.2390\n",
            "[Epoch 035] Train Loss: 6.2007, Train Acc: 0.2935 | Val Loss: 6.2239, Val Acc: 0.2935\n",
            "  -> Best model saved with val_loss: 6.2239\n",
            "[Epoch 036] Train Loss: 6.1835, Train Acc: 0.2938 | Val Loss: 6.2088, Val Acc: 0.2937\n",
            "  -> Best model saved with val_loss: 6.2088\n",
            "[Epoch 037] Train Loss: 6.1683, Train Acc: 0.2940 | Val Loss: 6.1940, Val Acc: 0.2937\n",
            "  -> Best model saved with val_loss: 6.1940\n",
            "[Epoch 038] Train Loss: 6.1519, Train Acc: 0.2944 | Val Loss: 6.1798, Val Acc: 0.2937\n",
            "  -> Best model saved with val_loss: 6.1798\n",
            "[Epoch 039] Train Loss: 6.1358, Train Acc: 0.2949 | Val Loss: 6.1669, Val Acc: 0.2937\n",
            "  -> Best model saved with val_loss: 6.1669\n",
            "[Epoch 040] Train Loss: 6.1219, Train Acc: 0.2949 | Val Loss: 6.1529, Val Acc: 0.2937\n",
            "  -> Best model saved with val_loss: 6.1529\n",
            "[Epoch 041] Train Loss: 6.1070, Train Acc: 0.2952 | Val Loss: 6.1404, Val Acc: 0.2937\n",
            "  -> Best model saved with val_loss: 6.1404\n",
            "[Epoch 042] Train Loss: 6.0925, Train Acc: 0.2954 | Val Loss: 6.1278, Val Acc: 0.2939\n",
            "  -> Best model saved with val_loss: 6.1278\n",
            "[Epoch 043] Train Loss: 6.0779, Train Acc: 0.2957 | Val Loss: 6.1163, Val Acc: 0.2939\n",
            "  -> Best model saved with val_loss: 6.1163\n",
            "[Epoch 044] Train Loss: 6.0655, Train Acc: 0.2957 | Val Loss: 6.1043, Val Acc: 0.2939\n",
            "  -> Best model saved with val_loss: 6.1043\n",
            "[Epoch 045] Train Loss: 6.0522, Train Acc: 0.2958 | Val Loss: 6.0935, Val Acc: 0.2945\n",
            "  -> Best model saved with val_loss: 6.0935\n",
            "[Epoch 046] Train Loss: 6.0399, Train Acc: 0.2961 | Val Loss: 6.0821, Val Acc: 0.2947\n",
            "  -> Best model saved with val_loss: 6.0821\n",
            "[Epoch 047] Train Loss: 6.0294, Train Acc: 0.2963 | Val Loss: 6.0719, Val Acc: 0.2947\n",
            "  -> Best model saved with val_loss: 6.0719\n",
            "[Epoch 048] Train Loss: 6.0162, Train Acc: 0.2965 | Val Loss: 6.0611, Val Acc: 0.2952\n",
            "  -> Best model saved with val_loss: 6.0611\n",
            "[Epoch 049] Train Loss: 6.0045, Train Acc: 0.2969 | Val Loss: 6.0510, Val Acc: 0.2954\n",
            "  -> Best model saved with val_loss: 6.0510\n",
            "[Epoch 050] Train Loss: 5.9942, Train Acc: 0.2968 | Val Loss: 6.0412, Val Acc: 0.2957\n",
            "  -> Best model saved with val_loss: 6.0412\n",
            "[Epoch 051] Train Loss: 5.9828, Train Acc: 0.2971 | Val Loss: 6.0316, Val Acc: 0.2958\n",
            "  -> Best model saved with val_loss: 6.0316\n",
            "[Epoch 052] Train Loss: 5.9724, Train Acc: 0.2975 | Val Loss: 6.0226, Val Acc: 0.2960\n",
            "  -> Best model saved with val_loss: 6.0226\n",
            "[Epoch 053] Train Loss: 5.9624, Train Acc: 0.2976 | Val Loss: 6.0131, Val Acc: 0.2961\n",
            "  -> Best model saved with val_loss: 6.0131\n",
            "[Epoch 054] Train Loss: 5.9508, Train Acc: 0.2982 | Val Loss: 6.0044, Val Acc: 0.2964\n",
            "  -> Best model saved with val_loss: 6.0044\n",
            "[Epoch 055] Train Loss: 5.9431, Train Acc: 0.2978 | Val Loss: 5.9956, Val Acc: 0.2964\n",
            "  -> Best model saved with val_loss: 5.9956\n",
            "[Epoch 056] Train Loss: 5.9321, Train Acc: 0.2981 | Val Loss: 5.9874, Val Acc: 0.2967\n",
            "  -> Best model saved with val_loss: 5.9874\n",
            "[Epoch 057] Train Loss: 5.9220, Train Acc: 0.2987 | Val Loss: 5.9792, Val Acc: 0.2964\n",
            "  -> Best model saved with val_loss: 5.9792\n",
            "[Epoch 058] Train Loss: 5.9141, Train Acc: 0.2984 | Val Loss: 5.9711, Val Acc: 0.2965\n",
            "  -> Best model saved with val_loss: 5.9711\n",
            "[Epoch 059] Train Loss: 5.9049, Train Acc: 0.2989 | Val Loss: 5.9634, Val Acc: 0.2969\n",
            "  -> Best model saved with val_loss: 5.9634\n",
            "[Epoch 060] Train Loss: 5.8938, Train Acc: 0.2990 | Val Loss: 5.9557, Val Acc: 0.2973\n",
            "  -> Best model saved with val_loss: 5.9557\n",
            "[Epoch 061] Train Loss: 5.8871, Train Acc: 0.2993 | Val Loss: 5.9485, Val Acc: 0.2972\n",
            "  -> Best model saved with val_loss: 5.9485\n",
            "[Epoch 062] Train Loss: 5.8781, Train Acc: 0.2994 | Val Loss: 5.9408, Val Acc: 0.2970\n",
            "  -> Best model saved with val_loss: 5.9408\n",
            "[Epoch 063] Train Loss: 5.8680, Train Acc: 0.3001 | Val Loss: 5.9338, Val Acc: 0.2971\n",
            "  -> Best model saved with val_loss: 5.9338\n",
            "[Epoch 064] Train Loss: 5.8614, Train Acc: 0.3001 | Val Loss: 5.9269, Val Acc: 0.2977\n",
            "  -> Best model saved with val_loss: 5.9269\n",
            "[Epoch 065] Train Loss: 5.8521, Train Acc: 0.3002 | Val Loss: 5.9202, Val Acc: 0.2978\n",
            "  -> Best model saved with val_loss: 5.9202\n",
            "[Epoch 066] Train Loss: 5.8446, Train Acc: 0.3005 | Val Loss: 5.9135, Val Acc: 0.2975\n",
            "  -> Best model saved with val_loss: 5.9135\n",
            "[Epoch 067] Train Loss: 5.8385, Train Acc: 0.3004 | Val Loss: 5.9070, Val Acc: 0.2979\n",
            "  -> Best model saved with val_loss: 5.9070\n",
            "[Epoch 068] Train Loss: 5.8301, Train Acc: 0.3009 | Val Loss: 5.9007, Val Acc: 0.2983\n",
            "  -> Best model saved with val_loss: 5.9007\n",
            "[Epoch 069] Train Loss: 5.8234, Train Acc: 0.3012 | Val Loss: 5.8944, Val Acc: 0.2988\n",
            "  -> Best model saved with val_loss: 5.8944\n",
            "[Epoch 070] Train Loss: 5.8170, Train Acc: 0.3011 | Val Loss: 5.8885, Val Acc: 0.2988\n",
            "  -> Best model saved with val_loss: 5.8885\n",
            "[Epoch 071] Train Loss: 5.8095, Train Acc: 0.3015 | Val Loss: 5.8825, Val Acc: 0.2987\n",
            "  -> Best model saved with val_loss: 5.8825\n",
            "[Epoch 072] Train Loss: 5.8016, Train Acc: 0.3018 | Val Loss: 5.8771, Val Acc: 0.2987\n",
            "  -> Best model saved with val_loss: 5.8771\n",
            "[Epoch 073] Train Loss: 5.7939, Train Acc: 0.3018 | Val Loss: 5.8711, Val Acc: 0.2988\n",
            "  -> Best model saved with val_loss: 5.8711\n",
            "[Epoch 074] Train Loss: 5.7885, Train Acc: 0.3022 | Val Loss: 5.8660, Val Acc: 0.2992\n",
            "  -> Best model saved with val_loss: 5.8660\n",
            "[Epoch 075] Train Loss: 5.7826, Train Acc: 0.3021 | Val Loss: 5.8604, Val Acc: 0.2993\n",
            "  -> Best model saved with val_loss: 5.8604\n",
            "[Epoch 076] Train Loss: 5.7764, Train Acc: 0.3020 | Val Loss: 5.8555, Val Acc: 0.2993\n",
            "  -> Best model saved with val_loss: 5.8555\n",
            "[Epoch 077] Train Loss: 5.7704, Train Acc: 0.3023 | Val Loss: 5.8501, Val Acc: 0.2994\n",
            "  -> Best model saved with val_loss: 5.8501\n",
            "[Epoch 078] Train Loss: 5.7616, Train Acc: 0.3024 | Val Loss: 5.8452, Val Acc: 0.2993\n",
            "  -> Best model saved with val_loss: 5.8452\n",
            "[Epoch 079] Train Loss: 5.7570, Train Acc: 0.3030 | Val Loss: 5.8405, Val Acc: 0.2996\n",
            "  -> Best model saved with val_loss: 5.8405\n",
            "[Epoch 080] Train Loss: 5.7516, Train Acc: 0.3026 | Val Loss: 5.8353, Val Acc: 0.2998\n",
            "  -> Best model saved with val_loss: 5.8353\n",
            "[Epoch 081] Train Loss: 5.7473, Train Acc: 0.3031 | Val Loss: 5.8306, Val Acc: 0.3002\n",
            "  -> Best model saved with val_loss: 5.8306\n",
            "[Epoch 082] Train Loss: 5.7410, Train Acc: 0.3031 | Val Loss: 5.8261, Val Acc: 0.3005\n",
            "  -> Best model saved with val_loss: 5.8261\n",
            "[Epoch 083] Train Loss: 5.7350, Train Acc: 0.3033 | Val Loss: 5.8219, Val Acc: 0.3008\n",
            "  -> Best model saved with val_loss: 5.8219\n",
            "[Epoch 084] Train Loss: 5.7291, Train Acc: 0.3033 | Val Loss: 5.8171, Val Acc: 0.3011\n",
            "  -> Best model saved with val_loss: 5.8171\n",
            "[Epoch 085] Train Loss: 5.7231, Train Acc: 0.3037 | Val Loss: 5.8130, Val Acc: 0.3014\n",
            "  -> Best model saved with val_loss: 5.8130\n",
            "[Epoch 086] Train Loss: 5.7180, Train Acc: 0.3037 | Val Loss: 5.8086, Val Acc: 0.3023\n",
            "  -> Best model saved with val_loss: 5.8086\n",
            "[Epoch 087] Train Loss: 5.7133, Train Acc: 0.3038 | Val Loss: 5.8045, Val Acc: 0.3028\n",
            "  -> Best model saved with val_loss: 5.8045\n",
            "[Epoch 088] Train Loss: 5.7086, Train Acc: 0.3039 | Val Loss: 5.8003, Val Acc: 0.3022\n",
            "  -> Best model saved with val_loss: 5.8003\n",
            "[Epoch 089] Train Loss: 5.7050, Train Acc: 0.3040 | Val Loss: 5.7965, Val Acc: 0.3033\n",
            "  -> Best model saved with val_loss: 5.7965\n",
            "[Epoch 090] Train Loss: 5.6980, Train Acc: 0.3043 | Val Loss: 5.7923, Val Acc: 0.3031\n",
            "  -> Best model saved with val_loss: 5.7923\n",
            "[Epoch 091] Train Loss: 5.6935, Train Acc: 0.3045 | Val Loss: 5.7887, Val Acc: 0.3030\n",
            "  -> Best model saved with val_loss: 5.7887\n",
            "[Epoch 092] Train Loss: 5.6887, Train Acc: 0.3048 | Val Loss: 5.7852, Val Acc: 0.3036\n",
            "  -> Best model saved with val_loss: 5.7852\n",
            "[Epoch 093] Train Loss: 5.6847, Train Acc: 0.3043 | Val Loss: 5.7815, Val Acc: 0.3034\n",
            "  -> Best model saved with val_loss: 5.7815\n",
            "[Epoch 094] Train Loss: 5.6812, Train Acc: 0.3052 | Val Loss: 5.7779, Val Acc: 0.3034\n",
            "  -> Best model saved with val_loss: 5.7779\n",
            "[Epoch 095] Train Loss: 5.6762, Train Acc: 0.3051 | Val Loss: 5.7744, Val Acc: 0.3035\n",
            "  -> Best model saved with val_loss: 5.7744\n",
            "[Epoch 096] Train Loss: 5.6710, Train Acc: 0.3051 | Val Loss: 5.7706, Val Acc: 0.3036\n",
            "  -> Best model saved with val_loss: 5.7706\n",
            "[Epoch 097] Train Loss: 5.6657, Train Acc: 0.3055 | Val Loss: 5.7673, Val Acc: 0.3040\n",
            "  -> Best model saved with val_loss: 5.7673\n",
            "[Epoch 098] Train Loss: 5.6613, Train Acc: 0.3058 | Val Loss: 5.7640, Val Acc: 0.3045\n",
            "  -> Best model saved with val_loss: 5.7640\n",
            "[Epoch 099] Train Loss: 5.6568, Train Acc: 0.3059 | Val Loss: 5.7607, Val Acc: 0.3045\n",
            "  -> Best model saved with val_loss: 5.7607\n",
            "[Epoch 100] Train Loss: 5.6549, Train Acc: 0.3057 | Val Loss: 5.7573, Val Acc: 0.3046\n",
            "  -> Best model saved with val_loss: 5.7573\n",
            "[Epoch 101] Train Loss: 5.6498, Train Acc: 0.3058 | Val Loss: 5.7543, Val Acc: 0.3046\n",
            "  -> Best model saved with val_loss: 5.7543\n",
            "[Epoch 102] Train Loss: 5.6447, Train Acc: 0.3063 | Val Loss: 5.7513, Val Acc: 0.3047\n",
            "  -> Best model saved with val_loss: 5.7513\n",
            "[Epoch 103] Train Loss: 5.6440, Train Acc: 0.3065 | Val Loss: 5.7478, Val Acc: 0.3045\n",
            "  -> Best model saved with val_loss: 5.7478\n",
            "[Epoch 104] Train Loss: 5.6384, Train Acc: 0.3066 | Val Loss: 5.7450, Val Acc: 0.3045\n",
            "  -> Best model saved with val_loss: 5.7450\n",
            "[Epoch 105] Train Loss: 5.6349, Train Acc: 0.3070 | Val Loss: 5.7420, Val Acc: 0.3046\n",
            "  -> Best model saved with val_loss: 5.7420\n",
            "[Epoch 106] Train Loss: 5.6305, Train Acc: 0.3070 | Val Loss: 5.7391, Val Acc: 0.3046\n",
            "  -> Best model saved with val_loss: 5.7391\n",
            "[Epoch 107] Train Loss: 5.6268, Train Acc: 0.3069 | Val Loss: 5.7359, Val Acc: 0.3046\n",
            "  -> Best model saved with val_loss: 5.7359\n",
            "[Epoch 108] Train Loss: 5.6213, Train Acc: 0.3074 | Val Loss: 5.7333, Val Acc: 0.3047\n",
            "  -> Best model saved with val_loss: 5.7333\n",
            "[Epoch 109] Train Loss: 5.6173, Train Acc: 0.3075 | Val Loss: 5.7303, Val Acc: 0.3051\n",
            "  -> Best model saved with val_loss: 5.7303\n",
            "[Epoch 110] Train Loss: 5.6155, Train Acc: 0.3076 | Val Loss: 5.7276, Val Acc: 0.3047\n",
            "  -> Best model saved with val_loss: 5.7276\n",
            "[Epoch 111] Train Loss: 5.6120, Train Acc: 0.3075 | Val Loss: 5.7246, Val Acc: 0.3053\n",
            "  -> Best model saved with val_loss: 5.7246\n",
            "[Epoch 112] Train Loss: 5.6093, Train Acc: 0.3076 | Val Loss: 5.7219, Val Acc: 0.3051\n",
            "  -> Best model saved with val_loss: 5.7219\n",
            "[Epoch 113] Train Loss: 5.6045, Train Acc: 0.3081 | Val Loss: 5.7196, Val Acc: 0.3054\n",
            "  -> Best model saved with val_loss: 5.7196\n",
            "[Epoch 114] Train Loss: 5.6016, Train Acc: 0.3080 | Val Loss: 5.7171, Val Acc: 0.3055\n",
            "  -> Best model saved with val_loss: 5.7171\n",
            "[Epoch 115] Train Loss: 5.5995, Train Acc: 0.3082 | Val Loss: 5.7141, Val Acc: 0.3055\n",
            "  -> Best model saved with val_loss: 5.7141\n",
            "[Epoch 116] Train Loss: 5.5957, Train Acc: 0.3083 | Val Loss: 5.7118, Val Acc: 0.3055\n",
            "  -> Best model saved with val_loss: 5.7118\n",
            "[Epoch 117] Train Loss: 5.5914, Train Acc: 0.3084 | Val Loss: 5.7091, Val Acc: 0.3061\n",
            "  -> Best model saved with val_loss: 5.7091\n",
            "[Epoch 118] Train Loss: 5.5869, Train Acc: 0.3087 | Val Loss: 5.7067, Val Acc: 0.3061\n",
            "  -> Best model saved with val_loss: 5.7067\n",
            "[Epoch 119] Train Loss: 5.5867, Train Acc: 0.3086 | Val Loss: 5.7043, Val Acc: 0.3061\n",
            "  -> Best model saved with val_loss: 5.7043\n",
            "[Epoch 120] Train Loss: 5.5817, Train Acc: 0.3090 | Val Loss: 5.7019, Val Acc: 0.3061\n",
            "  -> Best model saved with val_loss: 5.7019\n",
            "[Epoch 121] Train Loss: 5.5782, Train Acc: 0.3088 | Val Loss: 5.6997, Val Acc: 0.3061\n",
            "  -> Best model saved with val_loss: 5.6997\n",
            "[Epoch 122] Train Loss: 5.5753, Train Acc: 0.3089 | Val Loss: 5.6975, Val Acc: 0.3061\n",
            "  -> Best model saved with val_loss: 5.6975\n",
            "[Epoch 123] Train Loss: 5.5721, Train Acc: 0.3092 | Val Loss: 5.6950, Val Acc: 0.3065\n",
            "  -> Best model saved with val_loss: 5.6950\n",
            "[Epoch 124] Train Loss: 5.5700, Train Acc: 0.3095 | Val Loss: 5.6926, Val Acc: 0.3063\n",
            "  -> Best model saved with val_loss: 5.6926\n",
            "[Epoch 125] Train Loss: 5.5672, Train Acc: 0.3093 | Val Loss: 5.6905, Val Acc: 0.3063\n",
            "  -> Best model saved with val_loss: 5.6905\n",
            "[Epoch 126] Train Loss: 5.5635, Train Acc: 0.3092 | Val Loss: 5.6879, Val Acc: 0.3065\n",
            "  -> Best model saved with val_loss: 5.6879\n",
            "[Epoch 127] Train Loss: 5.5603, Train Acc: 0.3095 | Val Loss: 5.6858, Val Acc: 0.3065\n",
            "  -> Best model saved with val_loss: 5.6858\n",
            "[Epoch 128] Train Loss: 5.5583, Train Acc: 0.3095 | Val Loss: 5.6837, Val Acc: 0.3064\n",
            "  -> Best model saved with val_loss: 5.6837\n",
            "[Epoch 129] Train Loss: 5.5544, Train Acc: 0.3101 | Val Loss: 5.6818, Val Acc: 0.3064\n",
            "  -> Best model saved with val_loss: 5.6818\n",
            "[Epoch 130] Train Loss: 5.5506, Train Acc: 0.3098 | Val Loss: 5.6793, Val Acc: 0.3066\n",
            "  -> Best model saved with val_loss: 5.6793\n",
            "[Epoch 131] Train Loss: 5.5466, Train Acc: 0.3101 | Val Loss: 5.6774, Val Acc: 0.3066\n",
            "  -> Best model saved with val_loss: 5.6774\n",
            "[Epoch 132] Train Loss: 5.5461, Train Acc: 0.3106 | Val Loss: 5.6752, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 5.6752\n",
            "[Epoch 133] Train Loss: 5.5427, Train Acc: 0.3109 | Val Loss: 5.6732, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 5.6732\n",
            "[Epoch 134] Train Loss: 5.5402, Train Acc: 0.3103 | Val Loss: 5.6708, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 5.6708\n",
            "[Epoch 135] Train Loss: 5.5361, Train Acc: 0.3107 | Val Loss: 5.6688, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 5.6688\n",
            "[Epoch 136] Train Loss: 5.5337, Train Acc: 0.3103 | Val Loss: 5.6670, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 5.6670\n",
            "[Epoch 137] Train Loss: 5.5317, Train Acc: 0.3107 | Val Loss: 5.6652, Val Acc: 0.3070\n",
            "  -> Best model saved with val_loss: 5.6652\n",
            "[Epoch 138] Train Loss: 5.5296, Train Acc: 0.3107 | Val Loss: 5.6630, Val Acc: 0.3069\n",
            "  -> Best model saved with val_loss: 5.6630\n",
            "[Epoch 139] Train Loss: 5.5253, Train Acc: 0.3110 | Val Loss: 5.6612, Val Acc: 0.3072\n",
            "  -> Best model saved with val_loss: 5.6612\n",
            "[Epoch 140] Train Loss: 5.5241, Train Acc: 0.3113 | Val Loss: 5.6591, Val Acc: 0.3071\n",
            "  -> Best model saved with val_loss: 5.6591\n",
            "[Epoch 141] Train Loss: 5.5216, Train Acc: 0.3110 | Val Loss: 5.6570, Val Acc: 0.3073\n",
            "  -> Best model saved with val_loss: 5.6570\n",
            "[Epoch 142] Train Loss: 5.5203, Train Acc: 0.3111 | Val Loss: 5.6552, Val Acc: 0.3073\n",
            "  -> Best model saved with val_loss: 5.6552\n",
            "[Epoch 143] Train Loss: 5.5175, Train Acc: 0.3111 | Val Loss: 5.6537, Val Acc: 0.3073\n",
            "  -> Best model saved with val_loss: 5.6537\n",
            "[Epoch 144] Train Loss: 5.5134, Train Acc: 0.3112 | Val Loss: 5.6518, Val Acc: 0.3074\n",
            "  -> Best model saved with val_loss: 5.6518\n",
            "[Epoch 145] Train Loss: 5.5095, Train Acc: 0.3115 | Val Loss: 5.6500, Val Acc: 0.3078\n",
            "  -> Best model saved with val_loss: 5.6500\n",
            "[Epoch 146] Train Loss: 5.5084, Train Acc: 0.3117 | Val Loss: 5.6483, Val Acc: 0.3078\n",
            "  -> Best model saved with val_loss: 5.6483\n",
            "[Epoch 147] Train Loss: 5.5056, Train Acc: 0.3116 | Val Loss: 5.6463, Val Acc: 0.3078\n",
            "  -> Best model saved with val_loss: 5.6463\n",
            "[Epoch 148] Train Loss: 5.5036, Train Acc: 0.3119 | Val Loss: 5.6443, Val Acc: 0.3082\n",
            "  -> Best model saved with val_loss: 5.6443\n",
            "[Epoch 149] Train Loss: 5.5009, Train Acc: 0.3117 | Val Loss: 5.6430, Val Acc: 0.3078\n",
            "  -> Best model saved with val_loss: 5.6430\n",
            "[Epoch 150] Train Loss: 5.4997, Train Acc: 0.3120 | Val Loss: 5.6410, Val Acc: 0.3083\n",
            "  -> Best model saved with val_loss: 5.6410\n",
            "[Epoch 151] Train Loss: 5.4961, Train Acc: 0.3122 | Val Loss: 5.6393, Val Acc: 0.3084\n",
            "  -> Best model saved with val_loss: 5.6393\n",
            "[Epoch 152] Train Loss: 5.4929, Train Acc: 0.3119 | Val Loss: 5.6376, Val Acc: 0.3086\n",
            "  -> Best model saved with val_loss: 5.6376\n",
            "[Epoch 153] Train Loss: 5.4910, Train Acc: 0.3123 | Val Loss: 5.6358, Val Acc: 0.3087\n",
            "  -> Best model saved with val_loss: 5.6358\n",
            "[Epoch 154] Train Loss: 5.4868, Train Acc: 0.3124 | Val Loss: 5.6338, Val Acc: 0.3095\n",
            "  -> Best model saved with val_loss: 5.6338\n",
            "[Epoch 155] Train Loss: 5.4872, Train Acc: 0.3122 | Val Loss: 5.6322, Val Acc: 0.3096\n",
            "  -> Best model saved with val_loss: 5.6322\n",
            "[Epoch 156] Train Loss: 5.4835, Train Acc: 0.3123 | Val Loss: 5.6307, Val Acc: 0.3098\n",
            "  -> Best model saved with val_loss: 5.6307\n",
            "[Epoch 157] Train Loss: 5.4810, Train Acc: 0.3124 | Val Loss: 5.6293, Val Acc: 0.3094\n",
            "  -> Best model saved with val_loss: 5.6293\n",
            "[Epoch 158] Train Loss: 5.4789, Train Acc: 0.3122 | Val Loss: 5.6273, Val Acc: 0.3098\n",
            "  -> Best model saved with val_loss: 5.6273\n",
            "[Epoch 159] Train Loss: 5.4772, Train Acc: 0.3126 | Val Loss: 5.6257, Val Acc: 0.3100\n",
            "  -> Best model saved with val_loss: 5.6257\n",
            "[Epoch 160] Train Loss: 5.4745, Train Acc: 0.3129 | Val Loss: 5.6241, Val Acc: 0.3100\n",
            "  -> Best model saved with val_loss: 5.6241\n",
            "[Epoch 161] Train Loss: 5.4732, Train Acc: 0.3130 | Val Loss: 5.6228, Val Acc: 0.3100\n",
            "  -> Best model saved with val_loss: 5.6228\n",
            "[Epoch 162] Train Loss: 5.4689, Train Acc: 0.3125 | Val Loss: 5.6213, Val Acc: 0.3100\n",
            "  -> Best model saved with val_loss: 5.6213\n",
            "[Epoch 163] Train Loss: 5.4682, Train Acc: 0.3128 | Val Loss: 5.6193, Val Acc: 0.3100\n",
            "  -> Best model saved with val_loss: 5.6193\n",
            "[Epoch 164] Train Loss: 5.4645, Train Acc: 0.3130 | Val Loss: 5.6177, Val Acc: 0.3099\n",
            "  -> Best model saved with val_loss: 5.6177\n",
            "[Epoch 165] Train Loss: 5.4648, Train Acc: 0.3129 | Val Loss: 5.6163, Val Acc: 0.3099\n",
            "  -> Best model saved with val_loss: 5.6163\n",
            "[Epoch 166] Train Loss: 5.4600, Train Acc: 0.3131 | Val Loss: 5.6155, Val Acc: 0.3100\n",
            "  -> Best model saved with val_loss: 5.6155\n",
            "[Epoch 167] Train Loss: 5.4601, Train Acc: 0.3129 | Val Loss: 5.6134, Val Acc: 0.3099\n",
            "  -> Best model saved with val_loss: 5.6134\n",
            "[Epoch 168] Train Loss: 5.4559, Train Acc: 0.3133 | Val Loss: 5.6118, Val Acc: 0.3099\n",
            "  -> Best model saved with val_loss: 5.6118\n",
            "[Epoch 169] Train Loss: 5.4547, Train Acc: 0.3134 | Val Loss: 5.6106, Val Acc: 0.3099\n",
            "  -> Best model saved with val_loss: 5.6106\n",
            "[Epoch 170] Train Loss: 5.4530, Train Acc: 0.3132 | Val Loss: 5.6090, Val Acc: 0.3100\n",
            "  -> Best model saved with val_loss: 5.6090\n",
            "[Epoch 171] Train Loss: 5.4512, Train Acc: 0.3133 | Val Loss: 5.6081, Val Acc: 0.3099\n",
            "  -> Best model saved with val_loss: 5.6081\n",
            "[Epoch 172] Train Loss: 5.4489, Train Acc: 0.3134 | Val Loss: 5.6059, Val Acc: 0.3104\n",
            "  -> Best model saved with val_loss: 5.6059\n",
            "[Epoch 173] Train Loss: 5.4477, Train Acc: 0.3132 | Val Loss: 5.6046, Val Acc: 0.3104\n",
            "  -> Best model saved with val_loss: 5.6046\n",
            "[Epoch 174] Train Loss: 5.4447, Train Acc: 0.3135 | Val Loss: 5.6031, Val Acc: 0.3105\n",
            "  -> Best model saved with val_loss: 5.6031\n",
            "[Epoch 175] Train Loss: 5.4433, Train Acc: 0.3137 | Val Loss: 5.6016, Val Acc: 0.3107\n",
            "  -> Best model saved with val_loss: 5.6016\n",
            "[Epoch 176] Train Loss: 5.4406, Train Acc: 0.3133 | Val Loss: 5.6000, Val Acc: 0.3110\n",
            "  -> Best model saved with val_loss: 5.6000\n",
            "[Epoch 177] Train Loss: 5.4394, Train Acc: 0.3139 | Val Loss: 5.5992, Val Acc: 0.3107\n",
            "  -> Best model saved with val_loss: 5.5992\n",
            "[Epoch 178] Train Loss: 5.4355, Train Acc: 0.3136 | Val Loss: 5.5977, Val Acc: 0.3108\n",
            "  -> Best model saved with val_loss: 5.5977\n",
            "[Epoch 179] Train Loss: 5.4339, Train Acc: 0.3135 | Val Loss: 5.5969, Val Acc: 0.3107\n",
            "  -> Best model saved with val_loss: 5.5969\n",
            "[Epoch 180] Train Loss: 5.4316, Train Acc: 0.3142 | Val Loss: 5.5951, Val Acc: 0.3111\n",
            "  -> Best model saved with val_loss: 5.5951\n",
            "[Epoch 181] Train Loss: 5.4304, Train Acc: 0.3143 | Val Loss: 5.5938, Val Acc: 0.3112\n",
            "  -> Best model saved with val_loss: 5.5938\n",
            "[Epoch 182] Train Loss: 5.4289, Train Acc: 0.3137 | Val Loss: 5.5924, Val Acc: 0.3112\n",
            "  -> Best model saved with val_loss: 5.5924\n",
            "[Epoch 183] Train Loss: 5.4261, Train Acc: 0.3140 | Val Loss: 5.5912, Val Acc: 0.3113\n",
            "  -> Best model saved with val_loss: 5.5912\n",
            "[Epoch 184] Train Loss: 5.4237, Train Acc: 0.3138 | Val Loss: 5.5893, Val Acc: 0.3113\n",
            "  -> Best model saved with val_loss: 5.5893\n",
            "[Epoch 185] Train Loss: 5.4240, Train Acc: 0.3141 | Val Loss: 5.5882, Val Acc: 0.3113\n",
            "  -> Best model saved with val_loss: 5.5882\n",
            "[Epoch 186] Train Loss: 5.4203, Train Acc: 0.3146 | Val Loss: 5.5870, Val Acc: 0.3113\n",
            "  -> Best model saved with val_loss: 5.5870\n",
            "[Epoch 187] Train Loss: 5.4203, Train Acc: 0.3143 | Val Loss: 5.5861, Val Acc: 0.3115\n",
            "  -> Best model saved with val_loss: 5.5861\n",
            "[Epoch 188] Train Loss: 5.4170, Train Acc: 0.3145 | Val Loss: 5.5845, Val Acc: 0.3113\n",
            "  -> Best model saved with val_loss: 5.5845\n",
            "[Epoch 189] Train Loss: 5.4153, Train Acc: 0.3146 | Val Loss: 5.5829, Val Acc: 0.3115\n",
            "  -> Best model saved with val_loss: 5.5829\n",
            "[Epoch 190] Train Loss: 5.4128, Train Acc: 0.3148 | Val Loss: 5.5816, Val Acc: 0.3115\n",
            "  -> Best model saved with val_loss: 5.5816\n",
            "[Epoch 191] Train Loss: 5.4097, Train Acc: 0.3142 | Val Loss: 5.5810, Val Acc: 0.3113\n",
            "  -> Best model saved with val_loss: 5.5810\n",
            "[Epoch 192] Train Loss: 5.4087, Train Acc: 0.3145 | Val Loss: 5.5793, Val Acc: 0.3116\n",
            "  -> Best model saved with val_loss: 5.5793\n",
            "[Epoch 193] Train Loss: 5.4068, Train Acc: 0.3143 | Val Loss: 5.5781, Val Acc: 0.3117\n",
            "  -> Best model saved with val_loss: 5.5781\n",
            "[Epoch 194] Train Loss: 5.4063, Train Acc: 0.3148 | Val Loss: 5.5770, Val Acc: 0.3116\n",
            "  -> Best model saved with val_loss: 5.5770\n",
            "[Epoch 195] Train Loss: 5.4043, Train Acc: 0.3147 | Val Loss: 5.5752, Val Acc: 0.3119\n",
            "  -> Best model saved with val_loss: 5.5752\n",
            "[Epoch 196] Train Loss: 5.4018, Train Acc: 0.3143 | Val Loss: 5.5745, Val Acc: 0.3124\n",
            "  -> Best model saved with val_loss: 5.5745\n",
            "[Epoch 197] Train Loss: 5.3997, Train Acc: 0.3149 | Val Loss: 5.5729, Val Acc: 0.3124\n",
            "  -> Best model saved with val_loss: 5.5729\n",
            "[Epoch 198] Train Loss: 5.3968, Train Acc: 0.3146 | Val Loss: 5.5716, Val Acc: 0.3118\n",
            "  -> Best model saved with val_loss: 5.5716\n",
            "[Epoch 199] Train Loss: 5.3964, Train Acc: 0.3151 | Val Loss: 5.5704, Val Acc: 0.3118\n",
            "  -> Best model saved with val_loss: 5.5704\n",
            "[Epoch 200] Train Loss: 5.3957, Train Acc: 0.3149 | Val Loss: 5.5694, Val Acc: 0.3125\n",
            "  -> Best model saved with val_loss: 5.5694\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# 데이터셋 생성\n",
        "full_dataset = ChatbotDataset(csv_file_path, sp, max_length=15)\n",
        "\n",
        "# 데이터셋 90%(훈련)와 10%(검증) 분할\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# 데이터로더 생성\n",
        "BATCH_SIZE = 64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"총 샘플 수: {len(full_dataset)}개\")\n",
        "print(f\"훈련 샘플 수: {len(train_dataset)}개\")\n",
        "print(f\"검증 샘플 수: {len(val_dataset)}개\")\n",
        "\n",
        "\n",
        "# 검증을 위한 함수 정의\n",
        "def evaluate_step(model, batch, loss_function, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    enc_input, dec_input, target = [x.to(device) for x in batch]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(enc_input, dec_input)\n",
        "        loss = loss_function(logits.permute(0, 2, 1), target)\n",
        "        acc = accuracy_function(logits, target, pad_id=sp.pad_id())\n",
        "\n",
        "    return loss.item(), acc\n",
        "\n",
        "\n",
        "# 훈련 및 검증을 수행하는 함수 정의\n",
        "def train_and_validate(model, train_loader, val_loader, optimizer, loss_function, scheduler,\n",
        "num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    best_val_loss = float('inf') # 가장 낮은 검증 손실을 저장하기 위한 변수\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # --- 훈련 루프 ---\n",
        "        model.train() # 훈련 모드\n",
        "        total_train_loss, total_train_acc = 0, 0\n",
        "        for batch in train_loader:\n",
        "            loss, acc = train_step(model, batch, optimizer, loss_function, device)\n",
        "            total_train_loss += loss\n",
        "            total_train_acc += acc\n",
        "            scheduler.step() # 스텝마다 스케줄러 업데이트\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_train_acc = total_train_acc / len(train_loader)\n",
        "\n",
        "        # --- 검증 루프 ---\n",
        "        model.eval() # 평가 모드\n",
        "        total_val_loss, total_val_acc = 0, 0\n",
        "        for batch in val_loader:\n",
        "            loss, acc = evaluate_step(model, batch, loss_function, device)\n",
        "            total_val_loss += loss\n",
        "            total_val_acc += acc\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        avg_val_acc = total_val_acc / len(val_loader)\n",
        "\n",
        "        # --- 에폭 결과 출력 ---\n",
        "        print(f\"[Epoch {epoch+1:03d}] \"\n",
        "            f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f} | \"\n",
        "            f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
        "\n",
        "        # 가장 좋은 성능의 모델 저장\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"  -> Best model saved with val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# 모델 및 옵티마이저 등 재정의 후 학습 시작\n",
        "# 하이퍼파라미터\n",
        "NUM_LAYERS = 4\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "UNITS = 1024\n",
        "DROPOUT = 0.1\n",
        "VOCAB_SIZE = 8000\n",
        "\n",
        "# 모델, 옵티마이저, 스케줄러, 손실함수\n",
        "model = Transformer(VOCAB_SIZE, NUM_LAYERS, UNITS, D_MODEL, NUM_HEADS, DROPOUT)\n",
        "optimizer = optim.AdamW(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda(D_MODEL, warmup_steps=4000\n",
        "))\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
        "\n",
        "# 학습 시작\n",
        "EPOCHS = 200\n",
        "train_and_validate(model, train_dataloader, val_dataloader, optimizer, loss_function, scheduler,\n",
        "EPOCHS, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "477ab76b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/14/wm3n_r1s01jfgw6n8j7pr4740000gn/T/ipykernel_11986/540550193.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Greedy Search 결과 (비교용) ---\n",
            "입력 : 벌써 12시네\n",
            "출력 : 좋은 수 있을 거예요.\n",
            "입력 : 가족있어?\n",
            "출력 : 좋은 수 있을 거예요.\n",
            "입력 : 초조하다.\n",
            "출력 : 좋은 더 많이 거예요.\n",
            "입력 : 놀러가고 싶다.\n",
            "출력 : 좋은.\n",
            "입력 : 나만 사랑하고 있는건 아니겠지?\n",
            "출력 : 좋은 사람 거예요.\n",
            "\n",
            "--- Beam Search 결과 (beam_width=5) ---\n",
            "입력 : 벌써 12시네\n",
            "출력 : 잘 거예요.\n",
            "입력 : 가족있어?\n",
            "출력 : 잘 거예요.\n",
            "입력 : 초조하다.\n",
            "출력 : 잘 거예요.\n",
            "입력 : 놀러가고 싶다.\n",
            "출력 : 잘 거예요.\n",
            "입력 : 나만 사랑하고 있는건 아니겠지?\n",
            "출력 : 잘 거예요.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'잘 거예요.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. 새로운 모델 객체를 생성 (동일한 구조)\n",
        "best_model = Transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "# 2. 저장된 최고 성능의 가중치('best_model.pth')를 불러오기\n",
        "best_model.load_state_dict(torch.load('best_model.pth'))\n",
        "best_model.to(device)\n",
        "\n",
        "# 답변 생성 테스트\n",
        "print(\"--- Greedy Search 결과 (비교용) ---\")\n",
        "sentence_generation(best_model, '벌써 12시네', sp, device)\n",
        "sentence_generation(best_model, '가족있어?', sp, device)\n",
        "sentence_generation(best_model, '초조하다.', sp, device)\n",
        "sentence_generation(best_model, '놀러가고 싶다.', sp, device)\n",
        "sentence_generation(best_model, '나만 사랑하고 있는건 아니겠지?', sp, device)\n",
        "\n",
        "print(\"\\n--- Beam Search 결과 (beam_width=5) ---\")\n",
        "sentence_generation_beam_search(best_model, '벌써 12시네', sp, device, beam_width=5)\n",
        "sentence_generation_beam_search(best_model, '가족있어?', sp, device, beam_width=5)\n",
        "sentence_generation_beam_search(best_model, '초조하다.', sp, device, beam_width=5)\n",
        "sentence_generation_beam_search(best_model, '놀러가고 싶다.', sp, device, beam_width=5)\n",
        "sentence_generation_beam_search(best_model, '나만 사랑하고 있는건 아니겠지?', sp, device, beam_width=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "aa070aab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Top-K Sampling 결과 (top_k=50) ---\n",
            "입력 : 벌써 12시네\n",
            "출력 : 좋은 마세요.\n",
            "입력 : 배고프다.\n",
            "출력 : 충분히가길 생각해요.\n",
            "입력 : 오늘 날씨 어때?\n",
            "출력 : 한 해보세요.\n",
            "입력 : 나만 사랑하고 있는건 아니겠지?\n",
            "출력 : 저도 수 있어요.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'저도 수 있어요.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def top_k_sampling_decoder(model, sentence, tokenizer, device='cpu', top_k=50, max_len=15):\n",
        "    \"\"\"\n",
        "    Top-K 샘플링 디코딩을 수행하는 함수\n",
        "    :param top_k: 샘플링에 사용할 상위 K개 토큰\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 전처리\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    # 인코더 입력: [START] + 인코딩 + [END]\n",
        "    # 시작 토큰(BOS)과 종료 토큰(EOS)을 추가\n",
        "    enc_input_ids = [tokenizer.bos_id()] + tokenizer.encode(preprocessed_sentence) + [tokenizer.eos_id()]\n",
        "    enc_input = torch.tensor([enc_input_ids], dtype=torch.long, device=device)\n",
        "\n",
        "    # 디코더 입력은 시작 토큰(BOS)만 포함\n",
        "    dec_input = torch.tensor([[tokenizer.bos_id()]], dtype=torch.long, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            logits = model(enc_input, dec_input)\n",
        "            last_step_logits = logits[:, -1, :]\n",
        "\n",
        "            # --- Top-K 샘플링 로직 ---\n",
        "            # 1. top_k 이외의 토큰에는 아주 낮은 값(-inf)을 주어 확률을 0으로 만듦\n",
        "            indices_to_remove = last_step_logits < torch.topk(last_step_logits, top_k)[0][..., -1, None]\n",
        "            last_step_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # 2. softmax를 통해 확률 분포로 변환\n",
        "            probs = F.softmax(last_step_logits, dim=-1)\n",
        "\n",
        "            # 3. multinomial 샘플링으로 다음 토큰을 '뽑음'\n",
        "            predicted_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # 종료 토큰이면 중단\n",
        "            if predicted_id.item() == tokenizer.eos_id():\n",
        "                break\n",
        "\n",
        "            # 예측된 토큰을 디코더 입력에 추가\n",
        "            dec_input = torch.cat([dec_input, predicted_id], dim=1)\n",
        "\n",
        "    return dec_input.squeeze(0).tolist()\n",
        "\n",
        "\n",
        "def sentence_generation_sampling(model, sentence, tokenizer, device='cpu', top_k=50):\n",
        "    \"\"\"Top-K 샘플링 결과를 문장으로 변환하는 래퍼 함수\"\"\"\n",
        "\n",
        "    output_seq = top_k_sampling_decoder(\n",
        "        model, sentence, tokenizer, device=device, top_k=top_k\n",
        "    )\n",
        "\n",
        "    predicted_sentence = tokenizer.decode(output_seq[1:])\n",
        "\n",
        "    print(\"입력 :\", sentence)\n",
        "    print(\"출력 :\", predicted_sentence)\n",
        "    return predicted_sentence\n",
        "\n",
        "# --- 테스트 ---\n",
        "print(\"--- Top-K Sampling 결과 (top_k=50) ---\")\n",
        "sentence_generation_sampling(best_model, '벌써 12시네', sp, device, top_k=50)\n",
        "sentence_generation_sampling(best_model, '배고프다.', sp, device, top_k=50)\n",
        "sentence_generation_sampling(best_model, '오늘 날씨 어때?', sp, device, top_k=50)\n",
        "sentence_generation_sampling(best_model, '나만 사랑하고 있는건 아니겠지?', sp, device, top_k=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "de1c45e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 데이터 로딩 시작 ---\n",
            "훈련: 10474개, 검증: 1164개\n",
            "\n",
            "--- 학습 시작 ---\n",
            "[Epoch 001/400] Train Loss: 9.0623, Acc: 0.0001 | Val Loss: 9.0428, Val Acc: 0.0000\n",
            "  -> Best model saved with val_loss: 9.0428\n",
            "[Epoch 002/400] Train Loss: 9.0021, Acc: 0.0000 | Val Loss: 8.9458, Val Acc: 0.0000\n",
            "  -> Best model saved with val_loss: 8.9458\n",
            "[Epoch 003/400] Train Loss: 8.8811, Acc: 0.0004 | Val Loss: 8.7857, Val Acc: 0.0005\n",
            "  -> Best model saved with val_loss: 8.7857\n",
            "[Epoch 004/400] Train Loss: 8.7045, Acc: 0.0194 | Val Loss: 8.5666, Val Acc: 0.0945\n",
            "  -> Best model saved with val_loss: 8.5666\n",
            "[Epoch 005/400] Train Loss: 8.4739, Acc: 0.1510 | Val Loss: 8.2959, Val Acc: 0.2353\n",
            "  -> Best model saved with val_loss: 8.2959\n",
            "[Epoch 006/400] Train Loss: 8.1980, Acc: 0.2442 | Val Loss: 7.9884, Val Acc: 0.2706\n",
            "  -> Best model saved with val_loss: 7.9884\n",
            "[Epoch 007/400] Train Loss: 7.8950, Acc: 0.2702 | Val Loss: 7.6660, Val Acc: 0.2815\n",
            "  -> Best model saved with val_loss: 7.6660\n",
            "[Epoch 008/400] Train Loss: 7.5899, Acc: 0.2817 | Val Loss: 7.3655, Val Acc: 0.2857\n",
            "  -> Best model saved with val_loss: 7.3655\n",
            "[Epoch 009/400] Train Loss: 7.3211, Acc: 0.2849 | Val Loss: 7.1288, Val Acc: 0.2872\n",
            "  -> Best model saved with val_loss: 7.1288\n",
            "[Epoch 010/400] Train Loss: 7.1147, Acc: 0.2856 | Val Loss: 6.9777, Val Acc: 0.2872\n",
            "  -> Best model saved with val_loss: 6.9777\n",
            "[Epoch 011/400] Train Loss: 6.9826, Acc: 0.2865 | Val Loss: 6.8869, Val Acc: 0.2884\n",
            "  -> Best model saved with val_loss: 6.8869\n",
            "[Epoch 012/400] Train Loss: 6.8921, Acc: 0.2890 | Val Loss: 6.8174, Val Acc: 0.2901\n",
            "  -> Best model saved with val_loss: 6.8174\n",
            "[Epoch 013/400] Train Loss: 6.8226, Acc: 0.2914 | Val Loss: 6.7574, Val Acc: 0.2907\n",
            "  -> Best model saved with val_loss: 6.7574\n",
            "[Epoch 014/400] Train Loss: 6.7629, Acc: 0.2922 | Val Loss: 6.7056, Val Acc: 0.2912\n",
            "  -> Best model saved with val_loss: 6.7056\n",
            "[Epoch 015/400] Train Loss: 6.7118, Acc: 0.2924 | Val Loss: 6.6601, Val Acc: 0.2915\n",
            "  -> Best model saved with val_loss: 6.6601\n",
            "[Epoch 016/400] Train Loss: 6.6625, Acc: 0.2927 | Val Loss: 6.6197, Val Acc: 0.2915\n",
            "  -> Best model saved with val_loss: 6.6197\n",
            "[Epoch 017/400] Train Loss: 6.6205, Acc: 0.2925 | Val Loss: 6.5829, Val Acc: 0.2913\n",
            "  -> Best model saved with val_loss: 6.5829\n",
            "[Epoch 018/400] Train Loss: 6.5809, Acc: 0.2926 | Val Loss: 6.5481, Val Acc: 0.2913\n",
            "  -> Best model saved with val_loss: 6.5481\n",
            "[Epoch 019/400] Train Loss: 6.5424, Acc: 0.2925 | Val Loss: 6.5160, Val Acc: 0.2913\n",
            "  -> Best model saved with val_loss: 6.5160\n",
            "[Epoch 020/400] Train Loss: 6.5070, Acc: 0.2927 | Val Loss: 6.4853, Val Acc: 0.2915\n",
            "  -> Best model saved with val_loss: 6.4853\n",
            "[Epoch 021/400] Train Loss: 6.4750, Acc: 0.2926 | Val Loss: 6.4559, Val Acc: 0.2915\n",
            "  -> Best model saved with val_loss: 6.4559\n",
            "[Epoch 022/400] Train Loss: 6.4450, Acc: 0.2924 | Val Loss: 6.4284, Val Acc: 0.2915\n",
            "  -> Best model saved with val_loss: 6.4284\n",
            "[Epoch 023/400] Train Loss: 6.4127, Acc: 0.2925 | Val Loss: 6.4016, Val Acc: 0.2915\n",
            "  -> Best model saved with val_loss: 6.4016\n",
            "[Epoch 024/400] Train Loss: 6.3841, Acc: 0.2925 | Val Loss: 6.3752, Val Acc: 0.2915\n",
            "  -> Best model saved with val_loss: 6.3752\n",
            "[Epoch 025/400] Train Loss: 6.3551, Acc: 0.2926 | Val Loss: 6.3501, Val Acc: 0.2916\n",
            "  -> Best model saved with val_loss: 6.3501\n",
            "[Epoch 026/400] Train Loss: 6.3279, Acc: 0.2927 | Val Loss: 6.3257, Val Acc: 0.2926\n",
            "  -> Best model saved with val_loss: 6.3257\n",
            "[Epoch 027/400] Train Loss: 6.3032, Acc: 0.2931 | Val Loss: 6.3034, Val Acc: 0.2930\n",
            "  -> Best model saved with val_loss: 6.3034\n",
            "[Epoch 028/400] Train Loss: 6.2789, Acc: 0.2936 | Val Loss: 6.2826, Val Acc: 0.2931\n",
            "  -> Best model saved with val_loss: 6.2826\n",
            "[Epoch 029/400] Train Loss: 6.2558, Acc: 0.2942 | Val Loss: 6.2625, Val Acc: 0.2936\n",
            "  -> Best model saved with val_loss: 6.2625\n",
            "[Epoch 030/400] Train Loss: 6.2361, Acc: 0.2944 | Val Loss: 6.2433, Val Acc: 0.2940\n",
            "  -> Best model saved with val_loss: 6.2433\n",
            "[Epoch 031/400] Train Loss: 6.2139, Acc: 0.2951 | Val Loss: 6.2254, Val Acc: 0.2943\n",
            "  -> Best model saved with val_loss: 6.2254\n",
            "[Epoch 032/400] Train Loss: 6.1951, Acc: 0.2953 | Val Loss: 6.2080, Val Acc: 0.2943\n",
            "  -> Best model saved with val_loss: 6.2080\n",
            "[Epoch 033/400] Train Loss: 6.1744, Acc: 0.2959 | Val Loss: 6.1916, Val Acc: 0.2948\n",
            "  -> Best model saved with val_loss: 6.1916\n",
            "[Epoch 034/400] Train Loss: 6.1579, Acc: 0.2963 | Val Loss: 6.1749, Val Acc: 0.2951\n",
            "  -> Best model saved with val_loss: 6.1749\n",
            "[Epoch 035/400] Train Loss: 6.1408, Acc: 0.2966 | Val Loss: 6.1593, Val Acc: 0.2957\n",
            "  -> Best model saved with val_loss: 6.1593\n",
            "[Epoch 036/400] Train Loss: 6.1231, Acc: 0.2967 | Val Loss: 6.1442, Val Acc: 0.2964\n",
            "  -> Best model saved with val_loss: 6.1442\n",
            "[Epoch 037/400] Train Loss: 6.1073, Acc: 0.2972 | Val Loss: 6.1296, Val Acc: 0.2967\n",
            "  -> Best model saved with val_loss: 6.1296\n",
            "[Epoch 038/400] Train Loss: 6.0925, Acc: 0.2971 | Val Loss: 6.1158, Val Acc: 0.2971\n",
            "  -> Best model saved with val_loss: 6.1158\n",
            "[Epoch 039/400] Train Loss: 6.0768, Acc: 0.2973 | Val Loss: 6.1021, Val Acc: 0.2976\n",
            "  -> Best model saved with val_loss: 6.1021\n",
            "[Epoch 040/400] Train Loss: 6.0616, Acc: 0.2971 | Val Loss: 6.0889, Val Acc: 0.2977\n",
            "  -> Best model saved with val_loss: 6.0889\n",
            "[Epoch 041/400] Train Loss: 6.0462, Acc: 0.2977 | Val Loss: 6.0762, Val Acc: 0.2979\n",
            "  -> Best model saved with val_loss: 6.0762\n",
            "[Epoch 042/400] Train Loss: 6.0342, Acc: 0.2973 | Val Loss: 6.0640, Val Acc: 0.2983\n",
            "  -> Best model saved with val_loss: 6.0640\n",
            "[Epoch 043/400] Train Loss: 6.0198, Acc: 0.2977 | Val Loss: 6.0519, Val Acc: 0.2985\n",
            "  -> Best model saved with val_loss: 6.0519\n",
            "[Epoch 044/400] Train Loss: 6.0067, Acc: 0.2982 | Val Loss: 6.0402, Val Acc: 0.2988\n",
            "  -> Best model saved with val_loss: 6.0402\n",
            "[Epoch 045/400] Train Loss: 5.9935, Acc: 0.2985 | Val Loss: 6.0289, Val Acc: 0.2988\n",
            "  -> Best model saved with val_loss: 6.0289\n",
            "[Epoch 046/400] Train Loss: 5.9803, Acc: 0.2986 | Val Loss: 6.0184, Val Acc: 0.2989\n",
            "  -> Best model saved with val_loss: 6.0184\n",
            "[Epoch 047/400] Train Loss: 5.9700, Acc: 0.2986 | Val Loss: 6.0074, Val Acc: 0.2989\n",
            "  -> Best model saved with val_loss: 6.0074\n",
            "[Epoch 048/400] Train Loss: 5.9583, Acc: 0.2989 | Val Loss: 5.9973, Val Acc: 0.2994\n",
            "  -> Best model saved with val_loss: 5.9973\n",
            "[Epoch 049/400] Train Loss: 5.9459, Acc: 0.2993 | Val Loss: 5.9876, Val Acc: 0.2995\n",
            "  -> Best model saved with val_loss: 5.9876\n",
            "[Epoch 050/400] Train Loss: 5.9331, Acc: 0.2995 | Val Loss: 5.9777, Val Acc: 0.2998\n",
            "  -> Best model saved with val_loss: 5.9777\n",
            "[Epoch 051/400] Train Loss: 5.9234, Acc: 0.2997 | Val Loss: 5.9683, Val Acc: 0.2999\n",
            "  -> Best model saved with val_loss: 5.9683\n",
            "[Epoch 052/400] Train Loss: 5.9132, Acc: 0.2998 | Val Loss: 5.9590, Val Acc: 0.3001\n",
            "  -> Best model saved with val_loss: 5.9590\n",
            "[Epoch 053/400] Train Loss: 5.9040, Acc: 0.2999 | Val Loss: 5.9500, Val Acc: 0.3002\n",
            "  -> Best model saved with val_loss: 5.9500\n",
            "[Epoch 054/400] Train Loss: 5.8931, Acc: 0.3001 | Val Loss: 5.9412, Val Acc: 0.3003\n",
            "  -> Best model saved with val_loss: 5.9412\n",
            "[Epoch 055/400] Train Loss: 5.8832, Acc: 0.3005 | Val Loss: 5.9328, Val Acc: 0.3004\n",
            "  -> Best model saved with val_loss: 5.9328\n",
            "[Epoch 056/400] Train Loss: 5.8723, Acc: 0.3007 | Val Loss: 5.9245, Val Acc: 0.3004\n",
            "  -> Best model saved with val_loss: 5.9245\n",
            "[Epoch 057/400] Train Loss: 5.8643, Acc: 0.3010 | Val Loss: 5.9164, Val Acc: 0.3006\n",
            "  -> Best model saved with val_loss: 5.9164\n",
            "[Epoch 058/400] Train Loss: 5.8561, Acc: 0.3009 | Val Loss: 5.9085, Val Acc: 0.3008\n",
            "  -> Best model saved with val_loss: 5.9085\n",
            "[Epoch 059/400] Train Loss: 5.8454, Acc: 0.3014 | Val Loss: 5.9010, Val Acc: 0.3010\n",
            "  -> Best model saved with val_loss: 5.9010\n",
            "[Epoch 060/400] Train Loss: 5.8389, Acc: 0.3012 | Val Loss: 5.8936, Val Acc: 0.3024\n",
            "  -> Best model saved with val_loss: 5.8936\n",
            "[Epoch 061/400] Train Loss: 5.8283, Acc: 0.3016 | Val Loss: 5.8863, Val Acc: 0.3026\n",
            "  -> Best model saved with val_loss: 5.8863\n",
            "[Epoch 062/400] Train Loss: 5.8220, Acc: 0.3019 | Val Loss: 5.8792, Val Acc: 0.3025\n",
            "  -> Best model saved with val_loss: 5.8792\n",
            "[Epoch 063/400] Train Loss: 5.8136, Acc: 0.3022 | Val Loss: 5.8722, Val Acc: 0.3032\n",
            "  -> Best model saved with val_loss: 5.8722\n",
            "[Epoch 064/400] Train Loss: 5.8042, Acc: 0.3027 | Val Loss: 5.8656, Val Acc: 0.3034\n",
            "  -> Best model saved with val_loss: 5.8656\n",
            "[Epoch 065/400] Train Loss: 5.7982, Acc: 0.3030 | Val Loss: 5.8591, Val Acc: 0.3034\n",
            "  -> Best model saved with val_loss: 5.8591\n",
            "[Epoch 066/400] Train Loss: 5.7885, Acc: 0.3032 | Val Loss: 5.8529, Val Acc: 0.3039\n",
            "  -> Best model saved with val_loss: 5.8529\n",
            "[Epoch 067/400] Train Loss: 5.7828, Acc: 0.3031 | Val Loss: 5.8465, Val Acc: 0.3041\n",
            "  -> Best model saved with val_loss: 5.8465\n",
            "[Epoch 068/400] Train Loss: 5.7746, Acc: 0.3039 | Val Loss: 5.8407, Val Acc: 0.3040\n",
            "  -> Best model saved with val_loss: 5.8407\n",
            "[Epoch 069/400] Train Loss: 5.7681, Acc: 0.3041 | Val Loss: 5.8345, Val Acc: 0.3046\n",
            "  -> Best model saved with val_loss: 5.8345\n",
            "[Epoch 070/400] Train Loss: 5.7608, Acc: 0.3046 | Val Loss: 5.8288, Val Acc: 0.3049\n",
            "  -> Best model saved with val_loss: 5.8288\n",
            "[Epoch 071/400] Train Loss: 5.7529, Acc: 0.3049 | Val Loss: 5.8231, Val Acc: 0.3051\n",
            "  -> Best model saved with val_loss: 5.8231\n",
            "[Epoch 072/400] Train Loss: 5.7479, Acc: 0.3048 | Val Loss: 5.8176, Val Acc: 0.3054\n",
            "  -> Best model saved with val_loss: 5.8176\n",
            "[Epoch 073/400] Train Loss: 5.7403, Acc: 0.3049 | Val Loss: 5.8120, Val Acc: 0.3059\n",
            "  -> Best model saved with val_loss: 5.8120\n",
            "[Epoch 074/400] Train Loss: 5.7338, Acc: 0.3054 | Val Loss: 5.8067, Val Acc: 0.3062\n",
            "  -> Best model saved with val_loss: 5.8067\n",
            "[Epoch 075/400] Train Loss: 5.7276, Acc: 0.3058 | Val Loss: 5.8016, Val Acc: 0.3064\n",
            "  -> Best model saved with val_loss: 5.8016\n",
            "[Epoch 076/400] Train Loss: 5.7232, Acc: 0.3057 | Val Loss: 5.7965, Val Acc: 0.3065\n",
            "  -> Best model saved with val_loss: 5.7965\n",
            "[Epoch 077/400] Train Loss: 5.7140, Acc: 0.3062 | Val Loss: 5.7914, Val Acc: 0.3065\n",
            "  -> Best model saved with val_loss: 5.7914\n",
            "[Epoch 078/400] Train Loss: 5.7082, Acc: 0.3067 | Val Loss: 5.7867, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 5.7867\n",
            "[Epoch 079/400] Train Loss: 5.7042, Acc: 0.3064 | Val Loss: 5.7818, Val Acc: 0.3064\n",
            "  -> Best model saved with val_loss: 5.7818\n",
            "[Epoch 080/400] Train Loss: 5.6991, Acc: 0.3068 | Val Loss: 5.7773, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 5.7773\n",
            "[Epoch 081/400] Train Loss: 5.6914, Acc: 0.3069 | Val Loss: 5.7729, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 5.7729\n",
            "[Epoch 082/400] Train Loss: 5.6854, Acc: 0.3073 | Val Loss: 5.7682, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 5.7682\n",
            "[Epoch 083/400] Train Loss: 5.6814, Acc: 0.3074 | Val Loss: 5.7638, Val Acc: 0.3069\n",
            "  -> Best model saved with val_loss: 5.7638\n",
            "[Epoch 084/400] Train Loss: 5.6751, Acc: 0.3074 | Val Loss: 5.7594, Val Acc: 0.3069\n",
            "  -> Best model saved with val_loss: 5.7594\n",
            "[Epoch 085/400] Train Loss: 5.6713, Acc: 0.3074 | Val Loss: 5.7552, Val Acc: 0.3073\n",
            "  -> Best model saved with val_loss: 5.7552\n",
            "[Epoch 086/400] Train Loss: 5.6643, Acc: 0.3077 | Val Loss: 5.7513, Val Acc: 0.3077\n",
            "  -> Best model saved with val_loss: 5.7513\n",
            "[Epoch 087/400] Train Loss: 5.6603, Acc: 0.3078 | Val Loss: 5.7469, Val Acc: 0.3074\n",
            "  -> Best model saved with val_loss: 5.7469\n",
            "[Epoch 088/400] Train Loss: 5.6554, Acc: 0.3082 | Val Loss: 5.7430, Val Acc: 0.3078\n",
            "  -> Best model saved with val_loss: 5.7430\n",
            "[Epoch 089/400] Train Loss: 5.6496, Acc: 0.3086 | Val Loss: 5.7389, Val Acc: 0.3075\n",
            "  -> Best model saved with val_loss: 5.7389\n",
            "[Epoch 090/400] Train Loss: 5.6464, Acc: 0.3087 | Val Loss: 5.7350, Val Acc: 0.3076\n",
            "  -> Best model saved with val_loss: 5.7350\n",
            "[Epoch 091/400] Train Loss: 5.6409, Acc: 0.3084 | Val Loss: 5.7317, Val Acc: 0.3078\n",
            "  -> Best model saved with val_loss: 5.7317\n",
            "[Epoch 092/400] Train Loss: 5.6368, Acc: 0.3086 | Val Loss: 5.7276, Val Acc: 0.3080\n",
            "  -> Best model saved with val_loss: 5.7276\n",
            "[Epoch 093/400] Train Loss: 5.6313, Acc: 0.3089 | Val Loss: 5.7241, Val Acc: 0.3084\n",
            "  -> Best model saved with val_loss: 5.7241\n",
            "[Epoch 094/400] Train Loss: 5.6254, Acc: 0.3092 | Val Loss: 5.7202, Val Acc: 0.3082\n",
            "  -> Best model saved with val_loss: 5.7202\n",
            "[Epoch 095/400] Train Loss: 5.6217, Acc: 0.3088 | Val Loss: 5.7166, Val Acc: 0.3083\n",
            "  -> Best model saved with val_loss: 5.7166\n",
            "[Epoch 096/400] Train Loss: 5.6168, Acc: 0.3097 | Val Loss: 5.7132, Val Acc: 0.3084\n",
            "  -> Best model saved with val_loss: 5.7132\n",
            "[Epoch 097/400] Train Loss: 5.6129, Acc: 0.3093 | Val Loss: 5.7099, Val Acc: 0.3093\n",
            "  -> Best model saved with val_loss: 5.7099\n",
            "[Epoch 098/400] Train Loss: 5.6094, Acc: 0.3096 | Val Loss: 5.7064, Val Acc: 0.3093\n",
            "  -> Best model saved with val_loss: 5.7064\n",
            "[Epoch 099/400] Train Loss: 5.6039, Acc: 0.3099 | Val Loss: 5.7033, Val Acc: 0.3094\n",
            "  -> Best model saved with val_loss: 5.7033\n",
            "[Epoch 100/400] Train Loss: 5.5989, Acc: 0.3103 | Val Loss: 5.7001, Val Acc: 0.3096\n",
            "  -> Best model saved with val_loss: 5.7001\n",
            "[Epoch 101/400] Train Loss: 5.5961, Acc: 0.3101 | Val Loss: 5.6972, Val Acc: 0.3095\n",
            "  -> Best model saved with val_loss: 5.6972\n",
            "[Epoch 102/400] Train Loss: 5.5905, Acc: 0.3103 | Val Loss: 5.6935, Val Acc: 0.3095\n",
            "  -> Best model saved with val_loss: 5.6935\n",
            "[Epoch 103/400] Train Loss: 5.5896, Acc: 0.3101 | Val Loss: 5.6903, Val Acc: 0.3100\n",
            "  -> Best model saved with val_loss: 5.6903\n",
            "[Epoch 104/400] Train Loss: 5.5843, Acc: 0.3107 | Val Loss: 5.6871, Val Acc: 0.3103\n",
            "  -> Best model saved with val_loss: 5.6871\n",
            "[Epoch 105/400] Train Loss: 5.5814, Acc: 0.3107 | Val Loss: 5.6842, Val Acc: 0.3103\n",
            "  -> Best model saved with val_loss: 5.6842\n",
            "[Epoch 106/400] Train Loss: 5.5768, Acc: 0.3109 | Val Loss: 5.6811, Val Acc: 0.3110\n",
            "  -> Best model saved with val_loss: 5.6811\n",
            "[Epoch 107/400] Train Loss: 5.5734, Acc: 0.3111 | Val Loss: 5.6789, Val Acc: 0.3102\n",
            "  -> Best model saved with val_loss: 5.6789\n",
            "[Epoch 108/400] Train Loss: 5.5669, Acc: 0.3115 | Val Loss: 5.6758, Val Acc: 0.3106\n",
            "  -> Best model saved with val_loss: 5.6758\n",
            "[Epoch 109/400] Train Loss: 5.5663, Acc: 0.3111 | Val Loss: 5.6722, Val Acc: 0.3114\n",
            "  -> Best model saved with val_loss: 5.6722\n",
            "[Epoch 110/400] Train Loss: 5.5600, Acc: 0.3120 | Val Loss: 5.6696, Val Acc: 0.3113\n",
            "  -> Best model saved with val_loss: 5.6696\n",
            "[Epoch 111/400] Train Loss: 5.5568, Acc: 0.3117 | Val Loss: 5.6667, Val Acc: 0.3114\n",
            "  -> Best model saved with val_loss: 5.6667\n",
            "[Epoch 112/400] Train Loss: 5.5552, Acc: 0.3119 | Val Loss: 5.6639, Val Acc: 0.3117\n",
            "  -> Best model saved with val_loss: 5.6639\n",
            "[Epoch 113/400] Train Loss: 5.5507, Acc: 0.3123 | Val Loss: 5.6612, Val Acc: 0.3117\n",
            "  -> Best model saved with val_loss: 5.6612\n",
            "[Epoch 114/400] Train Loss: 5.5461, Acc: 0.3120 | Val Loss: 5.6585, Val Acc: 0.3118\n",
            "  -> Best model saved with val_loss: 5.6585\n",
            "[Epoch 115/400] Train Loss: 5.5438, Acc: 0.3119 | Val Loss: 5.6560, Val Acc: 0.3121\n",
            "  -> Best model saved with val_loss: 5.6560\n",
            "[Epoch 116/400] Train Loss: 5.5382, Acc: 0.3125 | Val Loss: 5.6536, Val Acc: 0.3119\n",
            "  -> Best model saved with val_loss: 5.6536\n",
            "[Epoch 117/400] Train Loss: 5.5358, Acc: 0.3125 | Val Loss: 5.6508, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6508\n",
            "[Epoch 118/400] Train Loss: 5.5313, Acc: 0.3128 | Val Loss: 5.6482, Val Acc: 0.3122\n",
            "  -> Best model saved with val_loss: 5.6482\n",
            "[Epoch 119/400] Train Loss: 5.5282, Acc: 0.3125 | Val Loss: 5.6456, Val Acc: 0.3121\n",
            "  -> Best model saved with val_loss: 5.6456\n",
            "[Epoch 120/400] Train Loss: 5.5272, Acc: 0.3130 | Val Loss: 5.6432, Val Acc: 0.3118\n",
            "  -> Best model saved with val_loss: 5.6432\n",
            "[Epoch 121/400] Train Loss: 5.5222, Acc: 0.3129 | Val Loss: 5.6406, Val Acc: 0.3122\n",
            "  -> Best model saved with val_loss: 5.6406\n",
            "[Epoch 122/400] Train Loss: 5.5174, Acc: 0.3138 | Val Loss: 5.6383, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6383\n",
            "[Epoch 123/400] Train Loss: 5.5157, Acc: 0.3132 | Val Loss: 5.6357, Val Acc: 0.3119\n",
            "  -> Best model saved with val_loss: 5.6357\n",
            "[Epoch 124/400] Train Loss: 5.5097, Acc: 0.3134 | Val Loss: 5.6338, Val Acc: 0.3121\n",
            "  -> Best model saved with val_loss: 5.6338\n",
            "[Epoch 125/400] Train Loss: 5.5103, Acc: 0.3131 | Val Loss: 5.6311, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6311\n",
            "[Epoch 126/400] Train Loss: 5.5068, Acc: 0.3132 | Val Loss: 5.6288, Val Acc: 0.3123\n",
            "  -> Best model saved with val_loss: 5.6288\n",
            "[Epoch 127/400] Train Loss: 5.5011, Acc: 0.3136 | Val Loss: 5.6263, Val Acc: 0.3118\n",
            "  -> Best model saved with val_loss: 5.6263\n",
            "[Epoch 128/400] Train Loss: 5.5005, Acc: 0.3135 | Val Loss: 5.6242, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6242\n",
            "[Epoch 129/400] Train Loss: 5.4958, Acc: 0.3137 | Val Loss: 5.6220, Val Acc: 0.3119\n",
            "  -> Best model saved with val_loss: 5.6220\n",
            "[Epoch 130/400] Train Loss: 5.4941, Acc: 0.3141 | Val Loss: 5.6197, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6197\n",
            "[Epoch 131/400] Train Loss: 5.4903, Acc: 0.3138 | Val Loss: 5.6175, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6175\n",
            "[Epoch 132/400] Train Loss: 5.4871, Acc: 0.3139 | Val Loss: 5.6152, Val Acc: 0.3122\n",
            "  -> Best model saved with val_loss: 5.6152\n",
            "[Epoch 133/400] Train Loss: 5.4840, Acc: 0.3142 | Val Loss: 5.6131, Val Acc: 0.3119\n",
            "  -> Best model saved with val_loss: 5.6131\n",
            "[Epoch 134/400] Train Loss: 5.4814, Acc: 0.3135 | Val Loss: 5.6110, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6110\n",
            "[Epoch 135/400] Train Loss: 5.4778, Acc: 0.3144 | Val Loss: 5.6088, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6088\n",
            "[Epoch 136/400] Train Loss: 5.4759, Acc: 0.3146 | Val Loss: 5.6069, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6069\n",
            "[Epoch 137/400] Train Loss: 5.4733, Acc: 0.3145 | Val Loss: 5.6048, Val Acc: 0.3121\n",
            "  -> Best model saved with val_loss: 5.6048\n",
            "[Epoch 138/400] Train Loss: 5.4709, Acc: 0.3142 | Val Loss: 5.6025, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 5.6025\n",
            "[Epoch 139/400] Train Loss: 5.4662, Acc: 0.3147 | Val Loss: 5.6006, Val Acc: 0.3123\n",
            "  -> Best model saved with val_loss: 5.6006\n",
            "[Epoch 140/400] Train Loss: 5.4639, Acc: 0.3149 | Val Loss: 5.5990, Val Acc: 0.3124\n",
            "  -> Best model saved with val_loss: 5.5990\n",
            "[Epoch 141/400] Train Loss: 5.4613, Acc: 0.3148 | Val Loss: 5.5967, Val Acc: 0.3124\n",
            "  -> Best model saved with val_loss: 5.5967\n",
            "[Epoch 142/400] Train Loss: 5.4575, Acc: 0.3153 | Val Loss: 5.5949, Val Acc: 0.3125\n",
            "  -> Best model saved with val_loss: 5.5949\n",
            "[Epoch 143/400] Train Loss: 5.4558, Acc: 0.3148 | Val Loss: 5.5928, Val Acc: 0.3124\n",
            "  -> Best model saved with val_loss: 5.5928\n",
            "[Epoch 144/400] Train Loss: 5.4521, Acc: 0.3147 | Val Loss: 5.5906, Val Acc: 0.3125\n",
            "  -> Best model saved with val_loss: 5.5906\n",
            "[Epoch 145/400] Train Loss: 5.4516, Acc: 0.3145 | Val Loss: 5.5888, Val Acc: 0.3124\n",
            "  -> Best model saved with val_loss: 5.5888\n",
            "[Epoch 146/400] Train Loss: 5.4496, Acc: 0.3150 | Val Loss: 5.5867, Val Acc: 0.3129\n",
            "  -> Best model saved with val_loss: 5.5867\n",
            "[Epoch 147/400] Train Loss: 5.4468, Acc: 0.3149 | Val Loss: 5.5850, Val Acc: 0.3125\n",
            "  -> Best model saved with val_loss: 5.5850\n",
            "[Epoch 148/400] Train Loss: 5.4429, Acc: 0.3151 | Val Loss: 5.5833, Val Acc: 0.3126\n",
            "  -> Best model saved with val_loss: 5.5833\n",
            "[Epoch 149/400] Train Loss: 5.4398, Acc: 0.3151 | Val Loss: 5.5812, Val Acc: 0.3127\n",
            "  -> Best model saved with val_loss: 5.5812\n",
            "[Epoch 150/400] Train Loss: 5.4375, Acc: 0.3153 | Val Loss: 5.5791, Val Acc: 0.3127\n",
            "  -> Best model saved with val_loss: 5.5791\n",
            "[Epoch 151/400] Train Loss: 5.4338, Acc: 0.3154 | Val Loss: 5.5774, Val Acc: 0.3129\n",
            "  -> Best model saved with val_loss: 5.5774\n",
            "[Epoch 152/400] Train Loss: 5.4305, Acc: 0.3151 | Val Loss: 5.5754, Val Acc: 0.3129\n",
            "  -> Best model saved with val_loss: 5.5754\n",
            "[Epoch 153/400] Train Loss: 5.4292, Acc: 0.3156 | Val Loss: 5.5736, Val Acc: 0.3129\n",
            "  -> Best model saved with val_loss: 5.5736\n",
            "[Epoch 154/400] Train Loss: 5.4269, Acc: 0.3154 | Val Loss: 5.5722, Val Acc: 0.3129\n",
            "  -> Best model saved with val_loss: 5.5722\n",
            "[Epoch 155/400] Train Loss: 5.4233, Acc: 0.3161 | Val Loss: 5.5701, Val Acc: 0.3129\n",
            "  -> Best model saved with val_loss: 5.5701\n",
            "[Epoch 156/400] Train Loss: 5.4200, Acc: 0.3155 | Val Loss: 5.5688, Val Acc: 0.3131\n",
            "  -> Best model saved with val_loss: 5.5688\n",
            "[Epoch 157/400] Train Loss: 5.4182, Acc: 0.3164 | Val Loss: 5.5672, Val Acc: 0.3130\n",
            "  -> Best model saved with val_loss: 5.5672\n",
            "[Epoch 158/400] Train Loss: 5.4161, Acc: 0.3161 | Val Loss: 5.5649, Val Acc: 0.3130\n",
            "  -> Best model saved with val_loss: 5.5649\n",
            "[Epoch 159/400] Train Loss: 5.4130, Acc: 0.3157 | Val Loss: 5.5639, Val Acc: 0.3130\n",
            "  -> Best model saved with val_loss: 5.5639\n",
            "[Epoch 160/400] Train Loss: 5.4105, Acc: 0.3159 | Val Loss: 5.5615, Val Acc: 0.3131\n",
            "  -> Best model saved with val_loss: 5.5615\n",
            "[Epoch 161/400] Train Loss: 5.4090, Acc: 0.3165 | Val Loss: 5.5600, Val Acc: 0.3131\n",
            "  -> Best model saved with val_loss: 5.5600\n",
            "[Epoch 162/400] Train Loss: 5.4057, Acc: 0.3167 | Val Loss: 5.5587, Val Acc: 0.3132\n",
            "  -> Best model saved with val_loss: 5.5587\n",
            "[Epoch 163/400] Train Loss: 5.4040, Acc: 0.3162 | Val Loss: 5.5566, Val Acc: 0.3135\n",
            "  -> Best model saved with val_loss: 5.5566\n",
            "[Epoch 164/400] Train Loss: 5.4026, Acc: 0.3159 | Val Loss: 5.5550, Val Acc: 0.3136\n",
            "  -> Best model saved with val_loss: 5.5550\n",
            "[Epoch 165/400] Train Loss: 5.3986, Acc: 0.3160 | Val Loss: 5.5535, Val Acc: 0.3136\n",
            "  -> Best model saved with val_loss: 5.5535\n",
            "[Epoch 166/400] Train Loss: 5.3976, Acc: 0.3164 | Val Loss: 5.5516, Val Acc: 0.3135\n",
            "  -> Best model saved with val_loss: 5.5516\n",
            "[Epoch 167/400] Train Loss: 5.3935, Acc: 0.3166 | Val Loss: 5.5501, Val Acc: 0.3135\n",
            "  -> Best model saved with val_loss: 5.5501\n",
            "[Epoch 168/400] Train Loss: 5.3919, Acc: 0.3166 | Val Loss: 5.5483, Val Acc: 0.3136\n",
            "  -> Best model saved with val_loss: 5.5483\n",
            "[Epoch 169/400] Train Loss: 5.3884, Acc: 0.3168 | Val Loss: 5.5469, Val Acc: 0.3137\n",
            "  -> Best model saved with val_loss: 5.5469\n",
            "[Epoch 170/400] Train Loss: 5.3880, Acc: 0.3167 | Val Loss: 5.5450, Val Acc: 0.3138\n",
            "  -> Best model saved with val_loss: 5.5450\n",
            "[Epoch 171/400] Train Loss: 5.3852, Acc: 0.3169 | Val Loss: 5.5436, Val Acc: 0.3136\n",
            "  -> Best model saved with val_loss: 5.5436\n",
            "[Epoch 172/400] Train Loss: 5.3823, Acc: 0.3171 | Val Loss: 5.5421, Val Acc: 0.3136\n",
            "  -> Best model saved with val_loss: 5.5421\n",
            "[Epoch 173/400] Train Loss: 5.3819, Acc: 0.3165 | Val Loss: 5.5403, Val Acc: 0.3136\n",
            "  -> Best model saved with val_loss: 5.5403\n",
            "[Epoch 174/400] Train Loss: 5.3786, Acc: 0.3168 | Val Loss: 5.5387, Val Acc: 0.3136\n",
            "  -> Best model saved with val_loss: 5.5387\n",
            "[Epoch 175/400] Train Loss: 5.3765, Acc: 0.3169 | Val Loss: 5.5375, Val Acc: 0.3140\n",
            "  -> Best model saved with val_loss: 5.5375\n",
            "[Epoch 176/400] Train Loss: 5.3740, Acc: 0.3168 | Val Loss: 5.5357, Val Acc: 0.3139\n",
            "  -> Best model saved with val_loss: 5.5357\n",
            "[Epoch 177/400] Train Loss: 5.3717, Acc: 0.3171 | Val Loss: 5.5342, Val Acc: 0.3138\n",
            "  -> Best model saved with val_loss: 5.5342\n",
            "[Epoch 178/400] Train Loss: 5.3707, Acc: 0.3171 | Val Loss: 5.5329, Val Acc: 0.3138\n",
            "  -> Best model saved with val_loss: 5.5329\n",
            "[Epoch 179/400] Train Loss: 5.3680, Acc: 0.3174 | Val Loss: 5.5311, Val Acc: 0.3139\n",
            "  -> Best model saved with val_loss: 5.5311\n",
            "[Epoch 180/400] Train Loss: 5.3649, Acc: 0.3175 | Val Loss: 5.5298, Val Acc: 0.3138\n",
            "  -> Best model saved with val_loss: 5.5298\n",
            "[Epoch 181/400] Train Loss: 5.3619, Acc: 0.3176 | Val Loss: 5.5282, Val Acc: 0.3144\n",
            "  -> Best model saved with val_loss: 5.5282\n",
            "[Epoch 182/400] Train Loss: 5.3597, Acc: 0.3174 | Val Loss: 5.5267, Val Acc: 0.3144\n",
            "  -> Best model saved with val_loss: 5.5267\n",
            "[Epoch 183/400] Train Loss: 5.3600, Acc: 0.3172 | Val Loss: 5.5253, Val Acc: 0.3144\n",
            "  -> Best model saved with val_loss: 5.5253\n",
            "[Epoch 184/400] Train Loss: 5.3571, Acc: 0.3175 | Val Loss: 5.5237, Val Acc: 0.3144\n",
            "  -> Best model saved with val_loss: 5.5237\n",
            "[Epoch 185/400] Train Loss: 5.3526, Acc: 0.3176 | Val Loss: 5.5224, Val Acc: 0.3146\n",
            "  -> Best model saved with val_loss: 5.5224\n",
            "[Epoch 186/400] Train Loss: 5.3535, Acc: 0.3176 | Val Loss: 5.5206, Val Acc: 0.3148\n",
            "  -> Best model saved with val_loss: 5.5206\n",
            "[Epoch 187/400] Train Loss: 5.3515, Acc: 0.3178 | Val Loss: 5.5195, Val Acc: 0.3148\n",
            "  -> Best model saved with val_loss: 5.5195\n",
            "[Epoch 188/400] Train Loss: 5.3465, Acc: 0.3176 | Val Loss: 5.5178, Val Acc: 0.3151\n",
            "  -> Best model saved with val_loss: 5.5178\n",
            "[Epoch 189/400] Train Loss: 5.3448, Acc: 0.3183 | Val Loss: 5.5162, Val Acc: 0.3152\n",
            "  -> Best model saved with val_loss: 5.5162\n",
            "[Epoch 190/400] Train Loss: 5.3431, Acc: 0.3182 | Val Loss: 5.5153, Val Acc: 0.3152\n",
            "  -> Best model saved with val_loss: 5.5153\n",
            "[Epoch 191/400] Train Loss: 5.3393, Acc: 0.3187 | Val Loss: 5.5135, Val Acc: 0.3154\n",
            "  -> Best model saved with val_loss: 5.5135\n",
            "[Epoch 192/400] Train Loss: 5.3375, Acc: 0.3187 | Val Loss: 5.5122, Val Acc: 0.3154\n",
            "  -> Best model saved with val_loss: 5.5122\n",
            "[Epoch 193/400] Train Loss: 5.3362, Acc: 0.3187 | Val Loss: 5.5106, Val Acc: 0.3155\n",
            "  -> Best model saved with val_loss: 5.5106\n",
            "[Epoch 194/400] Train Loss: 5.3346, Acc: 0.3181 | Val Loss: 5.5093, Val Acc: 0.3157\n",
            "  -> Best model saved with val_loss: 5.5093\n",
            "[Epoch 195/400] Train Loss: 5.3310, Acc: 0.3191 | Val Loss: 5.5082, Val Acc: 0.3157\n",
            "  -> Best model saved with val_loss: 5.5082\n",
            "[Epoch 196/400] Train Loss: 5.3308, Acc: 0.3186 | Val Loss: 5.5066, Val Acc: 0.3158\n",
            "  -> Best model saved with val_loss: 5.5066\n",
            "[Epoch 197/400] Train Loss: 5.3285, Acc: 0.3183 | Val Loss: 5.5054, Val Acc: 0.3163\n",
            "  -> Best model saved with val_loss: 5.5054\n",
            "[Epoch 198/400] Train Loss: 5.3256, Acc: 0.3188 | Val Loss: 5.5041, Val Acc: 0.3163\n",
            "  -> Best model saved with val_loss: 5.5041\n",
            "[Epoch 199/400] Train Loss: 5.3224, Acc: 0.3188 | Val Loss: 5.5025, Val Acc: 0.3163\n",
            "  -> Best model saved with val_loss: 5.5025\n",
            "[Epoch 200/400] Train Loss: 5.3202, Acc: 0.3189 | Val Loss: 5.5016, Val Acc: 0.3161\n",
            "  -> Best model saved with val_loss: 5.5016\n",
            "[Epoch 201/400] Train Loss: 5.3203, Acc: 0.3193 | Val Loss: 5.4996, Val Acc: 0.3160\n",
            "  -> Best model saved with val_loss: 5.4996\n",
            "[Epoch 202/400] Train Loss: 5.3181, Acc: 0.3191 | Val Loss: 5.4981, Val Acc: 0.3161\n",
            "  -> Best model saved with val_loss: 5.4981\n",
            "[Epoch 203/400] Train Loss: 5.3168, Acc: 0.3192 | Val Loss: 5.4971, Val Acc: 0.3165\n",
            "  -> Best model saved with val_loss: 5.4971\n",
            "[Epoch 204/400] Train Loss: 5.3122, Acc: 0.3198 | Val Loss: 5.4957, Val Acc: 0.3166\n",
            "  -> Best model saved with val_loss: 5.4957\n",
            "[Epoch 205/400] Train Loss: 5.3116, Acc: 0.3192 | Val Loss: 5.4945, Val Acc: 0.3166\n",
            "  -> Best model saved with val_loss: 5.4945\n",
            "[Epoch 206/400] Train Loss: 5.3097, Acc: 0.3191 | Val Loss: 5.4933, Val Acc: 0.3168\n",
            "  -> Best model saved with val_loss: 5.4933\n",
            "[Epoch 207/400] Train Loss: 5.3072, Acc: 0.3192 | Val Loss: 5.4923, Val Acc: 0.3168\n",
            "  -> Best model saved with val_loss: 5.4923\n",
            "[Epoch 208/400] Train Loss: 5.3064, Acc: 0.3196 | Val Loss: 5.4904, Val Acc: 0.3168\n",
            "  -> Best model saved with val_loss: 5.4904\n",
            "[Epoch 209/400] Train Loss: 5.3033, Acc: 0.3198 | Val Loss: 5.4890, Val Acc: 0.3170\n",
            "  -> Best model saved with val_loss: 5.4890\n",
            "[Epoch 210/400] Train Loss: 5.3028, Acc: 0.3199 | Val Loss: 5.4881, Val Acc: 0.3170\n",
            "  -> Best model saved with val_loss: 5.4881\n",
            "[Epoch 211/400] Train Loss: 5.3011, Acc: 0.3197 | Val Loss: 5.4866, Val Acc: 0.3174\n",
            "  -> Best model saved with val_loss: 5.4866\n",
            "[Epoch 212/400] Train Loss: 5.2972, Acc: 0.3198 | Val Loss: 5.4857, Val Acc: 0.3171\n",
            "  -> Best model saved with val_loss: 5.4857\n",
            "[Epoch 213/400] Train Loss: 5.2962, Acc: 0.3199 | Val Loss: 5.4844, Val Acc: 0.3173\n",
            "  -> Best model saved with val_loss: 5.4844\n",
            "[Epoch 214/400] Train Loss: 5.2948, Acc: 0.3201 | Val Loss: 5.4836, Val Acc: 0.3173\n",
            "  -> Best model saved with val_loss: 5.4836\n",
            "[Epoch 215/400] Train Loss: 5.2921, Acc: 0.3202 | Val Loss: 5.4818, Val Acc: 0.3175\n",
            "  -> Best model saved with val_loss: 5.4818\n",
            "[Epoch 216/400] Train Loss: 5.2905, Acc: 0.3200 | Val Loss: 5.4802, Val Acc: 0.3175\n",
            "  -> Best model saved with val_loss: 5.4802\n",
            "[Epoch 217/400] Train Loss: 5.2877, Acc: 0.3204 | Val Loss: 5.4790, Val Acc: 0.3174\n",
            "  -> Best model saved with val_loss: 5.4790\n",
            "[Epoch 218/400] Train Loss: 5.2868, Acc: 0.3204 | Val Loss: 5.4775, Val Acc: 0.3173\n",
            "  -> Best model saved with val_loss: 5.4775\n",
            "[Epoch 219/400] Train Loss: 5.2842, Acc: 0.3209 | Val Loss: 5.4765, Val Acc: 0.3178\n",
            "  -> Best model saved with val_loss: 5.4765\n",
            "[Epoch 220/400] Train Loss: 5.2819, Acc: 0.3207 | Val Loss: 5.4752, Val Acc: 0.3179\n",
            "  -> Best model saved with val_loss: 5.4752\n",
            "[Epoch 221/400] Train Loss: 5.2797, Acc: 0.3212 | Val Loss: 5.4743, Val Acc: 0.3178\n",
            "  -> Best model saved with val_loss: 5.4743\n",
            "[Epoch 222/400] Train Loss: 5.2782, Acc: 0.3208 | Val Loss: 5.4728, Val Acc: 0.3180\n",
            "  -> Best model saved with val_loss: 5.4728\n",
            "[Epoch 223/400] Train Loss: 5.2770, Acc: 0.3210 | Val Loss: 5.4716, Val Acc: 0.3180\n",
            "  -> Best model saved with val_loss: 5.4716\n",
            "[Epoch 224/400] Train Loss: 5.2759, Acc: 0.3210 | Val Loss: 5.4702, Val Acc: 0.3180\n",
            "  -> Best model saved with val_loss: 5.4702\n",
            "[Epoch 225/400] Train Loss: 5.2735, Acc: 0.3213 | Val Loss: 5.4690, Val Acc: 0.3181\n",
            "  -> Best model saved with val_loss: 5.4690\n",
            "[Epoch 226/400] Train Loss: 5.2716, Acc: 0.3212 | Val Loss: 5.4680, Val Acc: 0.3181\n",
            "  -> Best model saved with val_loss: 5.4680\n",
            "[Epoch 227/400] Train Loss: 5.2695, Acc: 0.3210 | Val Loss: 5.4666, Val Acc: 0.3183\n",
            "  -> Best model saved with val_loss: 5.4666\n",
            "[Epoch 228/400] Train Loss: 5.2683, Acc: 0.3208 | Val Loss: 5.4655, Val Acc: 0.3181\n",
            "  -> Best model saved with val_loss: 5.4655\n",
            "[Epoch 229/400] Train Loss: 5.2643, Acc: 0.3217 | Val Loss: 5.4642, Val Acc: 0.3183\n",
            "  -> Best model saved with val_loss: 5.4642\n",
            "[Epoch 230/400] Train Loss: 5.2643, Acc: 0.3215 | Val Loss: 5.4636, Val Acc: 0.3184\n",
            "  -> Best model saved with val_loss: 5.4636\n",
            "[Epoch 231/400] Train Loss: 5.2621, Acc: 0.3214 | Val Loss: 5.4620, Val Acc: 0.3183\n",
            "  -> Best model saved with val_loss: 5.4620\n",
            "[Epoch 232/400] Train Loss: 5.2596, Acc: 0.3220 | Val Loss: 5.4607, Val Acc: 0.3181\n",
            "  -> Best model saved with val_loss: 5.4607\n",
            "[Epoch 233/400] Train Loss: 5.2595, Acc: 0.3214 | Val Loss: 5.4595, Val Acc: 0.3184\n",
            "  -> Best model saved with val_loss: 5.4595\n",
            "[Epoch 234/400] Train Loss: 5.2562, Acc: 0.3214 | Val Loss: 5.4584, Val Acc: 0.3183\n",
            "  -> Best model saved with val_loss: 5.4584\n",
            "[Epoch 235/400] Train Loss: 5.2539, Acc: 0.3220 | Val Loss: 5.4570, Val Acc: 0.3185\n",
            "  -> Best model saved with val_loss: 5.4570\n",
            "[Epoch 236/400] Train Loss: 5.2531, Acc: 0.3219 | Val Loss: 5.4558, Val Acc: 0.3185\n",
            "  -> Best model saved with val_loss: 5.4558\n",
            "[Epoch 237/400] Train Loss: 5.2506, Acc: 0.3225 | Val Loss: 5.4547, Val Acc: 0.3185\n",
            "  -> Best model saved with val_loss: 5.4547\n",
            "[Epoch 238/400] Train Loss: 5.2492, Acc: 0.3225 | Val Loss: 5.4538, Val Acc: 0.3188\n",
            "  -> Best model saved with val_loss: 5.4538\n",
            "[Epoch 239/400] Train Loss: 5.2465, Acc: 0.3221 | Val Loss: 5.4530, Val Acc: 0.3188\n",
            "  -> Best model saved with val_loss: 5.4530\n",
            "[Epoch 240/400] Train Loss: 5.2456, Acc: 0.3225 | Val Loss: 5.4518, Val Acc: 0.3186\n",
            "  -> Best model saved with val_loss: 5.4518\n",
            "[Epoch 241/400] Train Loss: 5.2456, Acc: 0.3221 | Val Loss: 5.4504, Val Acc: 0.3188\n",
            "  -> Best model saved with val_loss: 5.4504\n",
            "[Epoch 242/400] Train Loss: 5.2416, Acc: 0.3221 | Val Loss: 5.4490, Val Acc: 0.3189\n",
            "  -> Best model saved with val_loss: 5.4490\n",
            "[Epoch 243/400] Train Loss: 5.2397, Acc: 0.3227 | Val Loss: 5.4480, Val Acc: 0.3188\n",
            "  -> Best model saved with val_loss: 5.4480\n",
            "[Epoch 244/400] Train Loss: 5.2387, Acc: 0.3228 | Val Loss: 5.4468, Val Acc: 0.3188\n",
            "  -> Best model saved with val_loss: 5.4468\n",
            "[Epoch 245/400] Train Loss: 5.2361, Acc: 0.3225 | Val Loss: 5.4457, Val Acc: 0.3189\n",
            "  -> Best model saved with val_loss: 5.4457\n",
            "[Epoch 246/400] Train Loss: 5.2350, Acc: 0.3229 | Val Loss: 5.4442, Val Acc: 0.3193\n",
            "  -> Best model saved with val_loss: 5.4442\n",
            "[Epoch 247/400] Train Loss: 5.2322, Acc: 0.3227 | Val Loss: 5.4433, Val Acc: 0.3189\n",
            "  -> Best model saved with val_loss: 5.4433\n",
            "[Epoch 248/400] Train Loss: 5.2309, Acc: 0.3234 | Val Loss: 5.4421, Val Acc: 0.3195\n",
            "  -> Best model saved with val_loss: 5.4421\n",
            "[Epoch 249/400] Train Loss: 5.2291, Acc: 0.3233 | Val Loss: 5.4409, Val Acc: 0.3194\n",
            "  -> Best model saved with val_loss: 5.4409\n",
            "[Epoch 250/400] Train Loss: 5.2297, Acc: 0.3231 | Val Loss: 5.4401, Val Acc: 0.3194\n",
            "  -> Best model saved with val_loss: 5.4401\n",
            "[Epoch 251/400] Train Loss: 5.2261, Acc: 0.3233 | Val Loss: 5.4387, Val Acc: 0.3199\n",
            "  -> Best model saved with val_loss: 5.4387\n",
            "[Epoch 252/400] Train Loss: 5.2240, Acc: 0.3229 | Val Loss: 5.4378, Val Acc: 0.3199\n",
            "  -> Best model saved with val_loss: 5.4378\n",
            "[Epoch 253/400] Train Loss: 5.2222, Acc: 0.3237 | Val Loss: 5.4370, Val Acc: 0.3199\n",
            "  -> Best model saved with val_loss: 5.4370\n",
            "[Epoch 254/400] Train Loss: 5.2213, Acc: 0.3236 | Val Loss: 5.4356, Val Acc: 0.3199\n",
            "  -> Best model saved with val_loss: 5.4356\n",
            "[Epoch 255/400] Train Loss: 5.2193, Acc: 0.3230 | Val Loss: 5.4348, Val Acc: 0.3200\n",
            "  -> Best model saved with val_loss: 5.4348\n",
            "[Epoch 256/400] Train Loss: 5.2179, Acc: 0.3230 | Val Loss: 5.4333, Val Acc: 0.3203\n",
            "  -> Best model saved with val_loss: 5.4333\n",
            "[Epoch 257/400] Train Loss: 5.2159, Acc: 0.3232 | Val Loss: 5.4323, Val Acc: 0.3202\n",
            "  -> Best model saved with val_loss: 5.4323\n",
            "[Epoch 258/400] Train Loss: 5.2160, Acc: 0.3234 | Val Loss: 5.4310, Val Acc: 0.3199\n",
            "  -> Best model saved with val_loss: 5.4310\n",
            "[Epoch 259/400] Train Loss: 5.2120, Acc: 0.3235 | Val Loss: 5.4299, Val Acc: 0.3202\n",
            "  -> Best model saved with val_loss: 5.4299\n",
            "[Epoch 260/400] Train Loss: 5.2105, Acc: 0.3243 | Val Loss: 5.4294, Val Acc: 0.3203\n",
            "  -> Best model saved with val_loss: 5.4294\n",
            "[Epoch 261/400] Train Loss: 5.2083, Acc: 0.3244 | Val Loss: 5.4281, Val Acc: 0.3204\n",
            "  -> Best model saved with val_loss: 5.4281\n",
            "[Epoch 262/400] Train Loss: 5.2069, Acc: 0.3244 | Val Loss: 5.4269, Val Acc: 0.3202\n",
            "  -> Best model saved with val_loss: 5.4269\n",
            "[Epoch 263/400] Train Loss: 5.2054, Acc: 0.3243 | Val Loss: 5.4259, Val Acc: 0.3204\n",
            "  -> Best model saved with val_loss: 5.4259\n",
            "[Epoch 264/400] Train Loss: 5.2029, Acc: 0.3245 | Val Loss: 5.4247, Val Acc: 0.3203\n",
            "  -> Best model saved with val_loss: 5.4247\n",
            "[Epoch 265/400] Train Loss: 5.2034, Acc: 0.3240 | Val Loss: 5.4237, Val Acc: 0.3203\n",
            "  -> Best model saved with val_loss: 5.4237\n",
            "[Epoch 266/400] Train Loss: 5.2009, Acc: 0.3245 | Val Loss: 5.4222, Val Acc: 0.3204\n",
            "  -> Best model saved with val_loss: 5.4222\n",
            "[Epoch 267/400] Train Loss: 5.1988, Acc: 0.3244 | Val Loss: 5.4213, Val Acc: 0.3207\n",
            "  -> Best model saved with val_loss: 5.4213\n",
            "[Epoch 268/400] Train Loss: 5.1964, Acc: 0.3249 | Val Loss: 5.4203, Val Acc: 0.3206\n",
            "  -> Best model saved with val_loss: 5.4203\n",
            "[Epoch 269/400] Train Loss: 5.1942, Acc: 0.3252 | Val Loss: 5.4194, Val Acc: 0.3206\n",
            "  -> Best model saved with val_loss: 5.4194\n",
            "[Epoch 270/400] Train Loss: 5.1937, Acc: 0.3251 | Val Loss: 5.4181, Val Acc: 0.3209\n",
            "  -> Best model saved with val_loss: 5.4181\n",
            "[Epoch 271/400] Train Loss: 5.1918, Acc: 0.3245 | Val Loss: 5.4171, Val Acc: 0.3209\n",
            "  -> Best model saved with val_loss: 5.4171\n",
            "[Epoch 272/400] Train Loss: 5.1896, Acc: 0.3252 | Val Loss: 5.4160, Val Acc: 0.3209\n",
            "  -> Best model saved with val_loss: 5.4160\n",
            "[Epoch 273/400] Train Loss: 5.1893, Acc: 0.3250 | Val Loss: 5.4151, Val Acc: 0.3210\n",
            "  -> Best model saved with val_loss: 5.4151\n",
            "[Epoch 274/400] Train Loss: 5.1870, Acc: 0.3251 | Val Loss: 5.4138, Val Acc: 0.3209\n",
            "  -> Best model saved with val_loss: 5.4138\n",
            "[Epoch 275/400] Train Loss: 5.1836, Acc: 0.3251 | Val Loss: 5.4138, Val Acc: 0.3213\n",
            "  -> Best model saved with val_loss: 5.4138\n",
            "[Epoch 276/400] Train Loss: 5.1855, Acc: 0.3251 | Val Loss: 5.4120, Val Acc: 0.3213\n",
            "  -> Best model saved with val_loss: 5.4120\n",
            "[Epoch 277/400] Train Loss: 5.1820, Acc: 0.3255 | Val Loss: 5.4111, Val Acc: 0.3214\n",
            "  -> Best model saved with val_loss: 5.4111\n",
            "[Epoch 278/400] Train Loss: 5.1811, Acc: 0.3248 | Val Loss: 5.4098, Val Acc: 0.3214\n",
            "  -> Best model saved with val_loss: 5.4098\n",
            "[Epoch 279/400] Train Loss: 5.1793, Acc: 0.3254 | Val Loss: 5.4091, Val Acc: 0.3213\n",
            "  -> Best model saved with val_loss: 5.4091\n",
            "[Epoch 280/400] Train Loss: 5.1784, Acc: 0.3253 | Val Loss: 5.4077, Val Acc: 0.3215\n",
            "  -> Best model saved with val_loss: 5.4077\n",
            "[Epoch 281/400] Train Loss: 5.1761, Acc: 0.3252 | Val Loss: 5.4068, Val Acc: 0.3214\n",
            "  -> Best model saved with val_loss: 5.4068\n",
            "[Epoch 282/400] Train Loss: 5.1745, Acc: 0.3256 | Val Loss: 5.4055, Val Acc: 0.3217\n",
            "  -> Best model saved with val_loss: 5.4055\n",
            "[Epoch 283/400] Train Loss: 5.1717, Acc: 0.3259 | Val Loss: 5.4048, Val Acc: 0.3216\n",
            "  -> Best model saved with val_loss: 5.4048\n",
            "[Epoch 284/400] Train Loss: 5.1688, Acc: 0.3259 | Val Loss: 5.4039, Val Acc: 0.3214\n",
            "  -> Best model saved with val_loss: 5.4039\n",
            "[Epoch 285/400] Train Loss: 5.1687, Acc: 0.3253 | Val Loss: 5.4026, Val Acc: 0.3217\n",
            "  -> Best model saved with val_loss: 5.4026\n",
            "[Epoch 286/400] Train Loss: 5.1682, Acc: 0.3256 | Val Loss: 5.4017, Val Acc: 0.3217\n",
            "  -> Best model saved with val_loss: 5.4017\n",
            "[Epoch 287/400] Train Loss: 5.1663, Acc: 0.3257 | Val Loss: 5.4005, Val Acc: 0.3219\n",
            "  -> Best model saved with val_loss: 5.4005\n",
            "[Epoch 288/400] Train Loss: 5.1657, Acc: 0.3265 | Val Loss: 5.3994, Val Acc: 0.3217\n",
            "  -> Best model saved with val_loss: 5.3994\n",
            "[Epoch 289/400] Train Loss: 5.1642, Acc: 0.3262 | Val Loss: 5.3983, Val Acc: 0.3219\n",
            "  -> Best model saved with val_loss: 5.3983\n",
            "[Epoch 290/400] Train Loss: 5.1618, Acc: 0.3264 | Val Loss: 5.3976, Val Acc: 0.3219\n",
            "  -> Best model saved with val_loss: 5.3976\n",
            "[Epoch 291/400] Train Loss: 5.1603, Acc: 0.3264 | Val Loss: 5.3965, Val Acc: 0.3219\n",
            "  -> Best model saved with val_loss: 5.3965\n",
            "[Epoch 292/400] Train Loss: 5.1583, Acc: 0.3263 | Val Loss: 5.3956, Val Acc: 0.3220\n",
            "  -> Best model saved with val_loss: 5.3956\n",
            "[Epoch 293/400] Train Loss: 5.1564, Acc: 0.3266 | Val Loss: 5.3946, Val Acc: 0.3220\n",
            "  -> Best model saved with val_loss: 5.3946\n",
            "[Epoch 294/400] Train Loss: 5.1552, Acc: 0.3264 | Val Loss: 5.3937, Val Acc: 0.3220\n",
            "  -> Best model saved with val_loss: 5.3937\n",
            "[Epoch 295/400] Train Loss: 5.1533, Acc: 0.3264 | Val Loss: 5.3924, Val Acc: 0.3219\n",
            "  -> Best model saved with val_loss: 5.3924\n",
            "[Epoch 296/400] Train Loss: 5.1521, Acc: 0.3265 | Val Loss: 5.3916, Val Acc: 0.3220\n",
            "  -> Best model saved with val_loss: 5.3916\n",
            "[Epoch 297/400] Train Loss: 5.1514, Acc: 0.3266 | Val Loss: 5.3904, Val Acc: 0.3219\n",
            "  -> Best model saved with val_loss: 5.3904\n",
            "[Epoch 298/400] Train Loss: 5.1500, Acc: 0.3267 | Val Loss: 5.3896, Val Acc: 0.3223\n",
            "  -> Best model saved with val_loss: 5.3896\n",
            "[Epoch 299/400] Train Loss: 5.1482, Acc: 0.3264 | Val Loss: 5.3885, Val Acc: 0.3220\n",
            "  -> Best model saved with val_loss: 5.3885\n",
            "[Epoch 300/400] Train Loss: 5.1455, Acc: 0.3267 | Val Loss: 5.3873, Val Acc: 0.3223\n",
            "  -> Best model saved with val_loss: 5.3873\n",
            "[Epoch 301/400] Train Loss: 5.1443, Acc: 0.3271 | Val Loss: 5.3866, Val Acc: 0.3223\n",
            "  -> Best model saved with val_loss: 5.3866\n",
            "[Epoch 302/400] Train Loss: 5.1425, Acc: 0.3268 | Val Loss: 5.3856, Val Acc: 0.3223\n",
            "  -> Best model saved with val_loss: 5.3856\n",
            "[Epoch 303/400] Train Loss: 5.1401, Acc: 0.3270 | Val Loss: 5.3847, Val Acc: 0.3223\n",
            "  -> Best model saved with val_loss: 5.3847\n",
            "[Epoch 304/400] Train Loss: 5.1409, Acc: 0.3268 | Val Loss: 5.3838, Val Acc: 0.3223\n",
            "  -> Best model saved with val_loss: 5.3838\n",
            "[Epoch 305/400] Train Loss: 5.1388, Acc: 0.3277 | Val Loss: 5.3833, Val Acc: 0.3223\n",
            "  -> Best model saved with val_loss: 5.3833\n",
            "[Epoch 306/400] Train Loss: 5.1382, Acc: 0.3272 | Val Loss: 5.3820, Val Acc: 0.3223\n",
            "  -> Best model saved with val_loss: 5.3820\n",
            "[Epoch 307/400] Train Loss: 5.1354, Acc: 0.3277 | Val Loss: 5.3809, Val Acc: 0.3223\n",
            "  -> Best model saved with val_loss: 5.3809\n",
            "[Epoch 308/400] Train Loss: 5.1330, Acc: 0.3281 | Val Loss: 5.3797, Val Acc: 0.3226\n",
            "  -> Best model saved with val_loss: 5.3797\n",
            "[Epoch 309/400] Train Loss: 5.1318, Acc: 0.3280 | Val Loss: 5.3790, Val Acc: 0.3224\n",
            "  -> Best model saved with val_loss: 5.3790\n",
            "[Epoch 310/400] Train Loss: 5.1306, Acc: 0.3278 | Val Loss: 5.3780, Val Acc: 0.3225\n",
            "  -> Best model saved with val_loss: 5.3780\n",
            "[Epoch 311/400] Train Loss: 5.1308, Acc: 0.3273 | Val Loss: 5.3768, Val Acc: 0.3229\n",
            "  -> Best model saved with val_loss: 5.3768\n",
            "[Epoch 312/400] Train Loss: 5.1271, Acc: 0.3278 | Val Loss: 5.3760, Val Acc: 0.3225\n",
            "  -> Best model saved with val_loss: 5.3760\n",
            "[Epoch 313/400] Train Loss: 5.1248, Acc: 0.3280 | Val Loss: 5.3750, Val Acc: 0.3229\n",
            "  -> Best model saved with val_loss: 5.3750\n",
            "[Epoch 314/400] Train Loss: 5.1252, Acc: 0.3280 | Val Loss: 5.3739, Val Acc: 0.3232\n",
            "  -> Best model saved with val_loss: 5.3739\n",
            "[Epoch 315/400] Train Loss: 5.1242, Acc: 0.3278 | Val Loss: 5.3731, Val Acc: 0.3229\n",
            "  -> Best model saved with val_loss: 5.3731\n",
            "[Epoch 316/400] Train Loss: 5.1216, Acc: 0.3279 | Val Loss: 5.3723, Val Acc: 0.3228\n",
            "  -> Best model saved with val_loss: 5.3723\n",
            "[Epoch 317/400] Train Loss: 5.1206, Acc: 0.3279 | Val Loss: 5.3713, Val Acc: 0.3228\n",
            "  -> Best model saved with val_loss: 5.3713\n",
            "[Epoch 318/400] Train Loss: 5.1181, Acc: 0.3282 | Val Loss: 5.3704, Val Acc: 0.3228\n",
            "  -> Best model saved with val_loss: 5.3704\n",
            "[Epoch 319/400] Train Loss: 5.1181, Acc: 0.3278 | Val Loss: 5.3693, Val Acc: 0.3230\n",
            "  -> Best model saved with val_loss: 5.3693\n",
            "[Epoch 320/400] Train Loss: 5.1163, Acc: 0.3281 | Val Loss: 5.3686, Val Acc: 0.3230\n",
            "  -> Best model saved with val_loss: 5.3686\n",
            "[Epoch 321/400] Train Loss: 5.1151, Acc: 0.3287 | Val Loss: 5.3682, Val Acc: 0.3233\n",
            "  -> Best model saved with val_loss: 5.3682\n",
            "[Epoch 322/400] Train Loss: 5.1114, Acc: 0.3282 | Val Loss: 5.3669, Val Acc: 0.3233\n",
            "  -> Best model saved with val_loss: 5.3669\n",
            "[Epoch 323/400] Train Loss: 5.1110, Acc: 0.3287 | Val Loss: 5.3655, Val Acc: 0.3233\n",
            "  -> Best model saved with val_loss: 5.3655\n",
            "[Epoch 324/400] Train Loss: 5.1107, Acc: 0.3287 | Val Loss: 5.3648, Val Acc: 0.3233\n",
            "  -> Best model saved with val_loss: 5.3648\n",
            "[Epoch 325/400] Train Loss: 5.1085, Acc: 0.3282 | Val Loss: 5.3639, Val Acc: 0.3232\n",
            "  -> Best model saved with val_loss: 5.3639\n",
            "[Epoch 326/400] Train Loss: 5.1075, Acc: 0.3285 | Val Loss: 5.3628, Val Acc: 0.3233\n",
            "  -> Best model saved with val_loss: 5.3628\n",
            "[Epoch 327/400] Train Loss: 5.1045, Acc: 0.3290 | Val Loss: 5.3620, Val Acc: 0.3232\n",
            "  -> Best model saved with val_loss: 5.3620\n",
            "[Epoch 328/400] Train Loss: 5.1043, Acc: 0.3291 | Val Loss: 5.3610, Val Acc: 0.3234\n",
            "  -> Best model saved with val_loss: 5.3610\n",
            "[Epoch 329/400] Train Loss: 5.1042, Acc: 0.3287 | Val Loss: 5.3604, Val Acc: 0.3235\n",
            "  -> Best model saved with val_loss: 5.3604\n",
            "[Epoch 330/400] Train Loss: 5.1020, Acc: 0.3290 | Val Loss: 5.3590, Val Acc: 0.3235\n",
            "  -> Best model saved with val_loss: 5.3590\n",
            "[Epoch 331/400] Train Loss: 5.1013, Acc: 0.3286 | Val Loss: 5.3584, Val Acc: 0.3237\n",
            "  -> Best model saved with val_loss: 5.3584\n",
            "[Epoch 332/400] Train Loss: 5.0985, Acc: 0.3284 | Val Loss: 5.3574, Val Acc: 0.3237\n",
            "  -> Best model saved with val_loss: 5.3574\n",
            "[Epoch 333/400] Train Loss: 5.0968, Acc: 0.3292 | Val Loss: 5.3563, Val Acc: 0.3237\n",
            "  -> Best model saved with val_loss: 5.3563\n",
            "[Epoch 334/400] Train Loss: 5.0959, Acc: 0.3290 | Val Loss: 5.3559, Val Acc: 0.3238\n",
            "  -> Best model saved with val_loss: 5.3559\n",
            "[Epoch 335/400] Train Loss: 5.0927, Acc: 0.3295 | Val Loss: 5.3546, Val Acc: 0.3240\n",
            "  -> Best model saved with val_loss: 5.3546\n",
            "[Epoch 336/400] Train Loss: 5.0938, Acc: 0.3293 | Val Loss: 5.3543, Val Acc: 0.3238\n",
            "  -> Best model saved with val_loss: 5.3543\n",
            "[Epoch 337/400] Train Loss: 5.0910, Acc: 0.3293 | Val Loss: 5.3528, Val Acc: 0.3240\n",
            "  -> Best model saved with val_loss: 5.3528\n",
            "[Epoch 338/400] Train Loss: 5.0885, Acc: 0.3300 | Val Loss: 5.3519, Val Acc: 0.3243\n",
            "  -> Best model saved with val_loss: 5.3519\n",
            "[Epoch 339/400] Train Loss: 5.0881, Acc: 0.3293 | Val Loss: 5.3511, Val Acc: 0.3244\n",
            "  -> Best model saved with val_loss: 5.3511\n",
            "[Epoch 340/400] Train Loss: 5.0871, Acc: 0.3295 | Val Loss: 5.3498, Val Acc: 0.3242\n",
            "  -> Best model saved with val_loss: 5.3498\n",
            "[Epoch 341/400] Train Loss: 5.0846, Acc: 0.3299 | Val Loss: 5.3489, Val Acc: 0.3243\n",
            "  -> Best model saved with val_loss: 5.3489\n",
            "[Epoch 342/400] Train Loss: 5.0867, Acc: 0.3296 | Val Loss: 5.3481, Val Acc: 0.3245\n",
            "  -> Best model saved with val_loss: 5.3481\n",
            "[Epoch 343/400] Train Loss: 5.0830, Acc: 0.3295 | Val Loss: 5.3472, Val Acc: 0.3244\n",
            "  -> Best model saved with val_loss: 5.3472\n",
            "[Epoch 344/400] Train Loss: 5.0836, Acc: 0.3293 | Val Loss: 5.3464, Val Acc: 0.3245\n",
            "  -> Best model saved with val_loss: 5.3464\n",
            "[Epoch 345/400] Train Loss: 5.0777, Acc: 0.3302 | Val Loss: 5.3454, Val Acc: 0.3245\n",
            "  -> Best model saved with val_loss: 5.3454\n",
            "[Epoch 346/400] Train Loss: 5.0782, Acc: 0.3303 | Val Loss: 5.3445, Val Acc: 0.3247\n",
            "  -> Best model saved with val_loss: 5.3445\n",
            "[Epoch 347/400] Train Loss: 5.0785, Acc: 0.3301 | Val Loss: 5.3440, Val Acc: 0.3248\n",
            "  -> Best model saved with val_loss: 5.3440\n",
            "[Epoch 348/400] Train Loss: 5.0750, Acc: 0.3300 | Val Loss: 5.3428, Val Acc: 0.3239\n",
            "  -> Best model saved with val_loss: 5.3428\n",
            "[Epoch 349/400] Train Loss: 5.0767, Acc: 0.3300 | Val Loss: 5.3420, Val Acc: 0.3245\n",
            "  -> Best model saved with val_loss: 5.3420\n",
            "[Epoch 350/400] Train Loss: 5.0726, Acc: 0.3302 | Val Loss: 5.3412, Val Acc: 0.3248\n",
            "  -> Best model saved with val_loss: 5.3412\n",
            "[Epoch 351/400] Train Loss: 5.0719, Acc: 0.3305 | Val Loss: 5.3401, Val Acc: 0.3244\n",
            "  -> Best model saved with val_loss: 5.3401\n",
            "[Epoch 352/400] Train Loss: 5.0700, Acc: 0.3304 | Val Loss: 5.3393, Val Acc: 0.3240\n",
            "  -> Best model saved with val_loss: 5.3393\n",
            "[Epoch 353/400] Train Loss: 5.0681, Acc: 0.3307 | Val Loss: 5.3385, Val Acc: 0.3248\n",
            "  -> Best model saved with val_loss: 5.3385\n",
            "[Epoch 354/400] Train Loss: 5.0661, Acc: 0.3305 | Val Loss: 5.3377, Val Acc: 0.3247\n",
            "  -> Best model saved with val_loss: 5.3377\n",
            "[Epoch 355/400] Train Loss: 5.0645, Acc: 0.3307 | Val Loss: 5.3369, Val Acc: 0.3247\n",
            "  -> Best model saved with val_loss: 5.3369\n",
            "[Epoch 356/400] Train Loss: 5.0646, Acc: 0.3304 | Val Loss: 5.3361, Val Acc: 0.3247\n",
            "  -> Best model saved with val_loss: 5.3361\n",
            "[Epoch 357/400] Train Loss: 5.0627, Acc: 0.3306 | Val Loss: 5.3349, Val Acc: 0.3242\n",
            "  -> Best model saved with val_loss: 5.3349\n",
            "[Epoch 358/400] Train Loss: 5.0633, Acc: 0.3304 | Val Loss: 5.3344, Val Acc: 0.3248\n",
            "  -> Best model saved with val_loss: 5.3344\n",
            "[Epoch 359/400] Train Loss: 5.0607, Acc: 0.3312 | Val Loss: 5.3336, Val Acc: 0.3248\n",
            "  -> Best model saved with val_loss: 5.3336\n",
            "[Epoch 360/400] Train Loss: 5.0595, Acc: 0.3308 | Val Loss: 5.3324, Val Acc: 0.3243\n",
            "  -> Best model saved with val_loss: 5.3324\n",
            "[Epoch 361/400] Train Loss: 5.0582, Acc: 0.3311 | Val Loss: 5.3317, Val Acc: 0.3242\n",
            "  -> Best model saved with val_loss: 5.3317\n",
            "[Epoch 362/400] Train Loss: 5.0581, Acc: 0.3315 | Val Loss: 5.3307, Val Acc: 0.3243\n",
            "  -> Best model saved with val_loss: 5.3307\n",
            "[Epoch 363/400] Train Loss: 5.0549, Acc: 0.3308 | Val Loss: 5.3298, Val Acc: 0.3243\n",
            "  -> Best model saved with val_loss: 5.3298\n",
            "[Epoch 364/400] Train Loss: 5.0536, Acc: 0.3312 | Val Loss: 5.3294, Val Acc: 0.3252\n",
            "  -> Best model saved with val_loss: 5.3294\n",
            "[Epoch 365/400] Train Loss: 5.0521, Acc: 0.3309 | Val Loss: 5.3280, Val Acc: 0.3244\n",
            "  -> Best model saved with val_loss: 5.3280\n",
            "[Epoch 366/400] Train Loss: 5.0526, Acc: 0.3311 | Val Loss: 5.3272, Val Acc: 0.3244\n",
            "  -> Best model saved with val_loss: 5.3272\n",
            "[Epoch 367/400] Train Loss: 5.0483, Acc: 0.3314 | Val Loss: 5.3266, Val Acc: 0.3243\n",
            "  -> Best model saved with val_loss: 5.3266\n",
            "[Epoch 368/400] Train Loss: 5.0503, Acc: 0.3312 | Val Loss: 5.3255, Val Acc: 0.3244\n",
            "  -> Best model saved with val_loss: 5.3255\n",
            "[Epoch 369/400] Train Loss: 5.0460, Acc: 0.3321 | Val Loss: 5.3245, Val Acc: 0.3244\n",
            "  -> Best model saved with val_loss: 5.3245\n",
            "[Epoch 370/400] Train Loss: 5.0458, Acc: 0.3311 | Val Loss: 5.3239, Val Acc: 0.3245\n",
            "  -> Best model saved with val_loss: 5.3239\n",
            "[Epoch 371/400] Train Loss: 5.0440, Acc: 0.3317 | Val Loss: 5.3233, Val Acc: 0.3247\n",
            "  -> Best model saved with val_loss: 5.3233\n",
            "[Epoch 372/400] Train Loss: 5.0432, Acc: 0.3319 | Val Loss: 5.3221, Val Acc: 0.3245\n",
            "  -> Best model saved with val_loss: 5.3221\n",
            "[Epoch 373/400] Train Loss: 5.0395, Acc: 0.3318 | Val Loss: 5.3210, Val Acc: 0.3247\n",
            "  -> Best model saved with val_loss: 5.3210\n",
            "[Epoch 374/400] Train Loss: 5.0392, Acc: 0.3317 | Val Loss: 5.3203, Val Acc: 0.3247\n",
            "  -> Best model saved with val_loss: 5.3203\n",
            "[Epoch 375/400] Train Loss: 5.0389, Acc: 0.3324 | Val Loss: 5.3195, Val Acc: 0.3248\n",
            "  -> Best model saved with val_loss: 5.3195\n",
            "[Epoch 376/400] Train Loss: 5.0367, Acc: 0.3320 | Val Loss: 5.3188, Val Acc: 0.3248\n",
            "  -> Best model saved with val_loss: 5.3188\n",
            "[Epoch 377/400] Train Loss: 5.0344, Acc: 0.3323 | Val Loss: 5.3178, Val Acc: 0.3249\n",
            "  -> Best model saved with val_loss: 5.3178\n",
            "[Epoch 378/400] Train Loss: 5.0345, Acc: 0.3323 | Val Loss: 5.3170, Val Acc: 0.3248\n",
            "  -> Best model saved with val_loss: 5.3170\n",
            "[Epoch 379/400] Train Loss: 5.0330, Acc: 0.3318 | Val Loss: 5.3161, Val Acc: 0.3251\n",
            "  -> Best model saved with val_loss: 5.3161\n",
            "[Epoch 380/400] Train Loss: 5.0338, Acc: 0.3322 | Val Loss: 5.3152, Val Acc: 0.3251\n",
            "  -> Best model saved with val_loss: 5.3152\n",
            "[Epoch 381/400] Train Loss: 5.0309, Acc: 0.3323 | Val Loss: 5.3145, Val Acc: 0.3251\n",
            "  -> Best model saved with val_loss: 5.3145\n",
            "[Epoch 382/400] Train Loss: 5.0305, Acc: 0.3327 | Val Loss: 5.3139, Val Acc: 0.3251\n",
            "  -> Best model saved with val_loss: 5.3139\n",
            "[Epoch 383/400] Train Loss: 5.0303, Acc: 0.3321 | Val Loss: 5.3129, Val Acc: 0.3253\n",
            "  -> Best model saved with val_loss: 5.3129\n",
            "[Epoch 384/400] Train Loss: 5.0278, Acc: 0.3325 | Val Loss: 5.3121, Val Acc: 0.3252\n",
            "  -> Best model saved with val_loss: 5.3121\n",
            "[Epoch 385/400] Train Loss: 5.0271, Acc: 0.3325 | Val Loss: 5.3116, Val Acc: 0.3257\n",
            "  -> Best model saved with val_loss: 5.3116\n",
            "[Epoch 386/400] Train Loss: 5.0251, Acc: 0.3322 | Val Loss: 5.3104, Val Acc: 0.3256\n",
            "  -> Best model saved with val_loss: 5.3104\n",
            "[Epoch 387/400] Train Loss: 5.0242, Acc: 0.3328 | Val Loss: 5.3097, Val Acc: 0.3256\n",
            "  -> Best model saved with val_loss: 5.3097\n",
            "[Epoch 388/400] Train Loss: 5.0221, Acc: 0.3327 | Val Loss: 5.3090, Val Acc: 0.3256\n",
            "  -> Best model saved with val_loss: 5.3090\n",
            "[Epoch 389/400] Train Loss: 5.0207, Acc: 0.3328 | Val Loss: 5.3083, Val Acc: 0.3256\n",
            "  -> Best model saved with val_loss: 5.3083\n",
            "[Epoch 390/400] Train Loss: 5.0186, Acc: 0.3329 | Val Loss: 5.3074, Val Acc: 0.3257\n",
            "  -> Best model saved with val_loss: 5.3074\n",
            "[Epoch 391/400] Train Loss: 5.0180, Acc: 0.3332 | Val Loss: 5.3064, Val Acc: 0.3259\n",
            "  -> Best model saved with val_loss: 5.3064\n",
            "[Epoch 392/400] Train Loss: 5.0156, Acc: 0.3331 | Val Loss: 5.3057, Val Acc: 0.3263\n",
            "  -> Best model saved with val_loss: 5.3057\n",
            "[Epoch 393/400] Train Loss: 5.0155, Acc: 0.3335 | Val Loss: 5.3051, Val Acc: 0.3261\n",
            "  -> Best model saved with val_loss: 5.3051\n",
            "[Epoch 394/400] Train Loss: 5.0119, Acc: 0.3331 | Val Loss: 5.3044, Val Acc: 0.3262\n",
            "  -> Best model saved with val_loss: 5.3044\n",
            "[Epoch 395/400] Train Loss: 5.0131, Acc: 0.3332 | Val Loss: 5.3032, Val Acc: 0.3261\n",
            "  -> Best model saved with val_loss: 5.3032\n",
            "[Epoch 396/400] Train Loss: 5.0124, Acc: 0.3330 | Val Loss: 5.3024, Val Acc: 0.3260\n",
            "  -> Best model saved with val_loss: 5.3024\n",
            "[Epoch 397/400] Train Loss: 5.0119, Acc: 0.3336 | Val Loss: 5.3016, Val Acc: 0.3262\n",
            "  -> Best model saved with val_loss: 5.3016\n",
            "[Epoch 398/400] Train Loss: 5.0115, Acc: 0.3336 | Val Loss: 5.3008, Val Acc: 0.3262\n",
            "  -> Best model saved with val_loss: 5.3008\n",
            "[Epoch 399/400] Train Loss: 5.0062, Acc: 0.3337 | Val Loss: 5.2998, Val Acc: 0.3263\n",
            "  -> Best model saved with val_loss: 5.2998\n",
            "[Epoch 400/400] Train Loss: 5.0088, Acc: 0.3335 | Val Loss: 5.2991, Val Acc: 0.3263\n",
            "  -> Best model saved with val_loss: 5.2991\n"
          ]
        }
      ],
      "source": [
        "# 에폭 다시 늘려서 밤 사이 켜놓고 잘 테스트 시작\n",
        "print(\"--- 데이터 로딩 시작 ---\")\n",
        "MAX_LENGTH = 15\n",
        "BATCH_SIZE = 64\n",
        "csv_file_path = 'data/ChatbotData.csv'\n",
        "\n",
        "# 데이터셋 생성\n",
        "original_dataset = ChatbotDataset(csv_file_path, sp, max_length=MAX_LENGTH)\n",
        "\n",
        "train_size = int(0.9 * len(original_dataset))\n",
        "val_size = len(original_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(original_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "print(f\"훈련: {len(train_dataset)}개, 검증: {len(val_dataset)}개\")\n",
        "\n",
        "\n",
        "# 얼리 스토핑 기능 추가\n",
        "def final_train_and_validate(model, train_loader, val_loader, optimizer, loss_function,\n",
        "scheduler, num_epochs, device, patience=10):\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0 # 얼리 스토핑 카운터\n",
        "\n",
        "    print(\"\\n--- 학습 시작 ---\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss, total_train_acc = 0, 0\n",
        "        for batch in train_loader:\n",
        "            loss, acc = train_step(model, batch, optimizer, loss_function, device)\n",
        "            total_train_loss += loss\n",
        "            total_train_acc += acc\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_train_acc = total_train_acc / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss, total_val_acc = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                loss, acc = evaluate_step(model, batch, loss_function, device)\n",
        "                total_val_loss += loss\n",
        "                total_val_acc += acc\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        avg_val_acc = total_val_acc / len(val_loader)\n",
        "\n",
        "        print(f\"[Epoch {epoch+1:03d}/{num_epochs}] \"\n",
        "            f\"Train Loss: {avg_train_loss:.4f}, Acc: {avg_train_acc:.4f} | \"\n",
        "            f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
        "\n",
        "        # 얼리 스토핑\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model_final.pth')\n",
        "            print(f\"  -> Best model saved with val_loss: {best_val_loss:.4f}\")\n",
        "            patience_counter = 0 # 카운터 리셋\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  -> Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
        "            break # 학습 중단\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "NUM_LAYERS = 6\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "UNITS = 1024\n",
        "DROPOUT = 0.1\n",
        "VOCAB_SIZE = sp.GetPieceSize()\n",
        "\n",
        "model = Transformer(VOCAB_SIZE, NUM_LAYERS, UNITS, D_MODEL, NUM_HEADS, DROPOUT)\n",
        "optimizer = optim.AdamW(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda(D_MODEL, warmup_steps=4000))\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
        "\n",
        "# 학습 실행\n",
        "EPOCHS = 400\n",
        "PATIENCE = 10\n",
        "\n",
        "final_train_and_validate(\n",
        "    model, train_dataloader, val_dataloader,\n",
        "    optimizer, loss_function, scheduler,\n",
        "    EPOCHS, device, PATIENCE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "8596336a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/14/wm3n_r1s01jfgw6n8j7pr4740000gn/T/ipykernel_11986/3089516203.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model_final.load_state_dict(torch.load('best_model_final.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력: 벌써 12시네\n",
            "답변 (greedy): 좋은 사람 잘 될 거예요\n",
            "---\n",
            "입력: 벌써 12시네\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 벌써 12시네\n",
            "답변 (sampling): 저도 사람 만날 수 있어요\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n",
            "입력: 배고프다.\n",
            "답변 (greedy): \n",
            "---\n",
            "입력: 배고프다.\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 배고프다.\n",
            "답변 (sampling): 그럴 수 있어요\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n",
            "입력: 오늘 날씨 어때?\n",
            "답변 (greedy): 좋은 사람 거예요\n",
            "---\n",
            "입력: 오늘 날씨 어때?\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 오늘 날씨 어때?\n",
            "답변 (sampling): 사람일 거예요\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n",
            "입력: 나만 사랑하고 있는건 아니겠지?\n",
            "답변 (greedy): 잘 될 거예요\n",
            "---\n",
            "입력: 나만 사랑하고 있는건 아니겠지?\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 나만 사랑하고 있는건 아니겠지?\n",
            "답변 (sampling): 많이 안 될 거예요\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n",
            "입력: 영화 추천해줘\n",
            "답변 (greedy): 좋은 사람 잘 될 거예요\n",
            "---\n",
            "입력: 영화 추천해줘\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 영화 추천해줘\n",
            "답변 (sampling): 잘\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n",
            "입력: 헤어지자고 말하는게 나을까?\n",
            "답변 (greedy): 좋은 사람 잘 될 거예요\n",
            "---\n",
            "입력: 헤어지자고 말하는게 나을까?\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 헤어지자고 말하는게 나을까?\n",
            "답변 (sampling): 천천히 것 같아요\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n",
            "입력: 제주도로 여행가고 싶어\n",
            "답변 (greedy): 좋은 사람 잘 될 거예요\n",
            "---\n",
            "입력: 제주도로 여행가고 싶어\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 제주도로 여행가고 싶어\n",
            "답변 (sampling): 마음서 말해보세요\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n",
            "입력: 요즘 기분이 우울해\n",
            "답변 (greedy): 좋은 사람 잘 될 거예요\n",
            "---\n",
            "입력: 요즘 기분이 우울해\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 요즘 기분이 우울해\n",
            "답변 (sampling): 많이 좋은한 건 어떨까요\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n",
            "입력: 친구랑 싸웠어\n",
            "답변 (greedy): 좋은 사람 잘 될 거예요\n",
            "---\n",
            "입력: 친구랑 싸웠어\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 친구랑 싸웠어\n",
            "답변 (sampling): 좋은 아니에요\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n",
            "입력: 취업 준비는 어떻게 해야 할까?\n",
            "답변 (greedy): 좋은 사람 잘 될 거예요\n",
            "---\n",
            "입력: 취업 준비는 어떻게 해야 할까?\n",
            "답변 (beam): 잘 될 거예요.\n",
            "---\n",
            "입력: 취업 준비는 어떻게 해야 할까?\n",
            "답변 (sampling): 잘을 것 같아요\n",
            "---\n",
            "\n",
            "=====================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 최종 모델을 저장하기 위한 객체 생성\n",
        "best_model_final = Transformer(                                                                             \n",
        "    vocab_size=VOCAB_SIZE,                                                                            \n",
        "    num_layers=NUM_LAYERS,                                                                            \n",
        "    units=UNITS,                                                                                      \n",
        "    d_model=D_MODEL,                                                                                  \n",
        "    num_heads=NUM_HEADS,                                                                              \n",
        "    dropout=DROPOUT                                                                                   \n",
        ")                                                                                                     \n",
        "                                                                                                    \n",
        "# 저장된 가중치를 불러오기\n",
        "best_model_final.load_state_dict(torch.load('best_model_final.pth'))                                              \n",
        "best_model_final.to(device)\n",
        "\n",
        "# 답변 생성을 위한 래퍼 함수                                                                          \n",
        "def generate_answer(model, sentence, tokenizer, device, decoding_strategy='sampling',                 \n",
        "beam_width=5, top_k=50):                                                                              \n",
        "    if decoding_strategy == 'greedy':                                                                 \n",
        "        output_seq = decoder_inference(model, sentence, tokenizer, device)                               \n",
        "    elif decoding_strategy == 'beam':                                                                 \n",
        "        output_seq = beam_search_decoder(model, sentence, tokenizer, device, beam_width=beam_width)                                                                                \n",
        "    elif decoding_strategy == 'sampling':                                                             \n",
        "        output_seq = top_k_sampling_decoder(model, sentence, tokenizer, device, top_k=top_k)                                                                                          \n",
        "    else:                                                                                             \n",
        "        raise ValueError(\"Unknown decoding strategy\")                                                 \n",
        "                                                                                                    \n",
        "    predicted_sentence = tokenizer.decode(output_seq[1:-1]) # BOS, EOS 제외                           \n",
        "                                                                                                    \n",
        "    print(f\"입력: {sentence}\")                                                                        \n",
        "    print(f\"답변 ({decoding_strategy}): {predicted_sentence}\")                                        \n",
        "    print('---')                                                                                      \n",
        "    return predicted_sentence                                                                         \n",
        "\n",
        "# 여러 문장과 디코딩 전략으로 테스트                                                                  \n",
        "test_sentences = [\"벌써 12시네\", \"배고프다.\", \"오늘 날씨 어때?\", \"나만 사랑하고 있는건 아니겠지?\", \"영화 추천해줘\", \n",
        "                  \"헤어지자고 말하는게 나을까?\", \"제주도로 여행가고 싶어\", \"요즘 기분이 우울해\", \"친구랑 싸웠어\", \"취업 준비는 어떻게 해야 할까?\"]                                                                          \n",
        "                                                                                                    \n",
        "for sentence in test_sentences:                                                                       \n",
        "    generate_answer(best_model_final, sentence, sp, device, decoding_strategy='greedy')                     \n",
        "    generate_answer(best_model_final, sentence, sp, device, decoding_strategy='beam', beam_width=5)                                                                                         \n",
        "    generate_answer(best_model_final, sentence, sp, device, decoding_strategy='sampling', top_k=50)                                                                                             \n",
        "    print('\\n=====================================\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "029abff0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 데이터 로딩 시작 ---\n",
            "훈련: 10474개, 검증: 1164개\n",
            "레이블 스무딩(0.1)이 적용된 손실 함수를 사용\n",
            "\n",
            "--- 학습 시작 ---\n",
            "[Epoch 001/100] Train Loss: 9.1435, Acc: 0.0001 | Val Loss: 9.1294, Val Acc: 0.0000\n",
            "  -> Best model saved with val_loss: 9.1294\n",
            "[Epoch 002/100] Train Loss: 9.0867, Acc: 0.0002 | Val Loss: 9.0349, Val Acc: 0.0000\n",
            "  -> Best model saved with val_loss: 9.0349\n",
            "[Epoch 003/100] Train Loss: 8.9719, Acc: 0.0004 | Val Loss: 8.8780, Val Acc: 0.0008\n",
            "  -> Best model saved with val_loss: 8.8780\n",
            "[Epoch 004/100] Train Loss: 8.8022, Acc: 0.0153 | Val Loss: 8.6621, Val Acc: 0.0819\n",
            "  -> Best model saved with val_loss: 8.6621\n",
            "[Epoch 005/100] Train Loss: 8.5820, Acc: 0.1349 | Val Loss: 8.3946, Val Acc: 0.2195\n",
            "  -> Best model saved with val_loss: 8.3946\n",
            "[Epoch 006/100] Train Loss: 8.3195, Acc: 0.2204 | Val Loss: 8.0923, Val Acc: 0.2419\n",
            "  -> Best model saved with val_loss: 8.0923\n",
            "[Epoch 007/100] Train Loss: 8.0320, Acc: 0.2367 | Val Loss: 7.7823, Val Acc: 0.2523\n",
            "  -> Best model saved with val_loss: 7.7823\n",
            "[Epoch 008/100] Train Loss: 7.7537, Acc: 0.2484 | Val Loss: 7.5074, Val Acc: 0.2653\n",
            "  -> Best model saved with val_loss: 7.5074\n",
            "[Epoch 009/100] Train Loss: 7.5139, Acc: 0.2635 | Val Loss: 7.3080, Val Acc: 0.2825\n",
            "  -> Best model saved with val_loss: 7.3080\n",
            "[Epoch 010/100] Train Loss: 7.3489, Acc: 0.2766 | Val Loss: 7.1929, Val Acc: 0.2929\n",
            "  -> Best model saved with val_loss: 7.1929\n",
            "[Epoch 011/100] Train Loss: 7.2472, Acc: 0.2845 | Val Loss: 7.1197, Val Acc: 0.2963\n",
            "  -> Best model saved with val_loss: 7.1197\n",
            "[Epoch 012/100] Train Loss: 7.1728, Acc: 0.2884 | Val Loss: 7.0564, Val Acc: 0.2986\n",
            "  -> Best model saved with val_loss: 7.0564\n",
            "[Epoch 013/100] Train Loss: 7.1129, Acc: 0.2905 | Val Loss: 7.0012, Val Acc: 0.2997\n",
            "  -> Best model saved with val_loss: 7.0012\n",
            "[Epoch 014/100] Train Loss: 7.0587, Acc: 0.2914 | Val Loss: 6.9547, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.9547\n",
            "[Epoch 015/100] Train Loss: 7.0121, Acc: 0.2917 | Val Loss: 6.9143, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.9143\n",
            "[Epoch 016/100] Train Loss: 6.9709, Acc: 0.2917 | Val Loss: 6.8788, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.8788\n",
            "[Epoch 017/100] Train Loss: 6.9344, Acc: 0.2916 | Val Loss: 6.8465, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.8465\n",
            "[Epoch 018/100] Train Loss: 6.9001, Acc: 0.2916 | Val Loss: 6.8167, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.8167\n",
            "[Epoch 019/100] Train Loss: 6.8702, Acc: 0.2916 | Val Loss: 6.7890, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.7890\n",
            "[Epoch 020/100] Train Loss: 6.8384, Acc: 0.2918 | Val Loss: 6.7630, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.7630\n",
            "[Epoch 021/100] Train Loss: 6.8116, Acc: 0.2917 | Val Loss: 6.7386, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.7386\n",
            "[Epoch 022/100] Train Loss: 6.7859, Acc: 0.2916 | Val Loss: 6.7151, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.7151\n",
            "[Epoch 023/100] Train Loss: 6.7581, Acc: 0.2919 | Val Loss: 6.6921, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.6921\n",
            "[Epoch 024/100] Train Loss: 6.7370, Acc: 0.2916 | Val Loss: 6.6696, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.6696\n",
            "[Epoch 025/100] Train Loss: 6.7103, Acc: 0.2917 | Val Loss: 6.6479, Val Acc: 0.3000\n",
            "  -> Best model saved with val_loss: 6.6479\n",
            "[Epoch 026/100] Train Loss: 6.6885, Acc: 0.2919 | Val Loss: 6.6275, Val Acc: 0.3002\n",
            "  -> Best model saved with val_loss: 6.6275\n",
            "[Epoch 027/100] Train Loss: 6.6668, Acc: 0.2918 | Val Loss: 6.6086, Val Acc: 0.3009\n",
            "  -> Best model saved with val_loss: 6.6086\n",
            "[Epoch 028/100] Train Loss: 6.6470, Acc: 0.2921 | Val Loss: 6.5904, Val Acc: 0.3020\n",
            "  -> Best model saved with val_loss: 6.5904\n",
            "[Epoch 029/100] Train Loss: 6.6275, Acc: 0.2926 | Val Loss: 6.5729, Val Acc: 0.3020\n",
            "  -> Best model saved with val_loss: 6.5729\n",
            "[Epoch 030/100] Train Loss: 6.6108, Acc: 0.2931 | Val Loss: 6.5564, Val Acc: 0.3021\n",
            "  -> Best model saved with val_loss: 6.5564\n",
            "[Epoch 031/100] Train Loss: 6.5930, Acc: 0.2936 | Val Loss: 6.5405, Val Acc: 0.3023\n",
            "  -> Best model saved with val_loss: 6.5405\n",
            "[Epoch 032/100] Train Loss: 6.5764, Acc: 0.2942 | Val Loss: 6.5253, Val Acc: 0.3032\n",
            "  -> Best model saved with val_loss: 6.5253\n",
            "[Epoch 033/100] Train Loss: 6.5603, Acc: 0.2944 | Val Loss: 6.5107, Val Acc: 0.3044\n",
            "  -> Best model saved with val_loss: 6.5107\n",
            "[Epoch 034/100] Train Loss: 6.5453, Acc: 0.2949 | Val Loss: 6.4973, Val Acc: 0.3047\n",
            "  -> Best model saved with val_loss: 6.4973\n",
            "[Epoch 035/100] Train Loss: 6.5292, Acc: 0.2955 | Val Loss: 6.4830, Val Acc: 0.3049\n",
            "  -> Best model saved with val_loss: 6.4830\n",
            "[Epoch 036/100] Train Loss: 6.5149, Acc: 0.2959 | Val Loss: 6.4701, Val Acc: 0.3056\n",
            "  -> Best model saved with val_loss: 6.4701\n",
            "[Epoch 037/100] Train Loss: 6.5002, Acc: 0.2962 | Val Loss: 6.4574, Val Acc: 0.3058\n",
            "  -> Best model saved with val_loss: 6.4574\n",
            "[Epoch 038/100] Train Loss: 6.4878, Acc: 0.2965 | Val Loss: 6.4453, Val Acc: 0.3060\n",
            "  -> Best model saved with val_loss: 6.4453\n",
            "[Epoch 039/100] Train Loss: 6.4756, Acc: 0.2968 | Val Loss: 6.4336, Val Acc: 0.3060\n",
            "  -> Best model saved with val_loss: 6.4336\n",
            "[Epoch 040/100] Train Loss: 6.4612, Acc: 0.2970 | Val Loss: 6.4223, Val Acc: 0.3060\n",
            "  -> Best model saved with val_loss: 6.4223\n",
            "[Epoch 041/100] Train Loss: 6.4503, Acc: 0.2972 | Val Loss: 6.4112, Val Acc: 0.3060\n",
            "  -> Best model saved with val_loss: 6.4112\n",
            "[Epoch 042/100] Train Loss: 6.4372, Acc: 0.2974 | Val Loss: 6.4005, Val Acc: 0.3063\n",
            "  -> Best model saved with val_loss: 6.4005\n",
            "[Epoch 043/100] Train Loss: 6.4262, Acc: 0.2978 | Val Loss: 6.3903, Val Acc: 0.3068\n",
            "  -> Best model saved with val_loss: 6.3903\n",
            "[Epoch 044/100] Train Loss: 6.4157, Acc: 0.2979 | Val Loss: 6.3804, Val Acc: 0.3069\n",
            "  -> Best model saved with val_loss: 6.3804\n",
            "[Epoch 045/100] Train Loss: 6.4048, Acc: 0.2980 | Val Loss: 6.3707, Val Acc: 0.3072\n",
            "  -> Best model saved with val_loss: 6.3707\n",
            "[Epoch 046/100] Train Loss: 6.3929, Acc: 0.2984 | Val Loss: 6.3615, Val Acc: 0.3073\n",
            "  -> Best model saved with val_loss: 6.3615\n",
            "[Epoch 047/100] Train Loss: 6.3842, Acc: 0.2986 | Val Loss: 6.3523, Val Acc: 0.3073\n",
            "  -> Best model saved with val_loss: 6.3523\n",
            "[Epoch 048/100] Train Loss: 6.3730, Acc: 0.2989 | Val Loss: 6.3435, Val Acc: 0.3077\n",
            "  -> Best model saved with val_loss: 6.3435\n",
            "[Epoch 049/100] Train Loss: 6.3654, Acc: 0.2990 | Val Loss: 6.3351, Val Acc: 0.3078\n",
            "  -> Best model saved with val_loss: 6.3351\n",
            "[Epoch 050/100] Train Loss: 6.3545, Acc: 0.2995 | Val Loss: 6.3269, Val Acc: 0.3081\n",
            "  -> Best model saved with val_loss: 6.3269\n",
            "[Epoch 051/100] Train Loss: 6.3459, Acc: 0.2994 | Val Loss: 6.3190, Val Acc: 0.3081\n",
            "  -> Best model saved with val_loss: 6.3190\n",
            "[Epoch 052/100] Train Loss: 6.3359, Acc: 0.2998 | Val Loss: 6.3110, Val Acc: 0.3083\n",
            "  -> Best model saved with val_loss: 6.3110\n",
            "[Epoch 053/100] Train Loss: 6.3280, Acc: 0.3002 | Val Loss: 6.3034, Val Acc: 0.3086\n",
            "  -> Best model saved with val_loss: 6.3034\n",
            "[Epoch 054/100] Train Loss: 6.3200, Acc: 0.3002 | Val Loss: 6.2961, Val Acc: 0.3094\n",
            "  -> Best model saved with val_loss: 6.2961\n",
            "[Epoch 055/100] Train Loss: 6.3110, Acc: 0.3003 | Val Loss: 6.2890, Val Acc: 0.3094\n",
            "  -> Best model saved with val_loss: 6.2890\n",
            "[Epoch 056/100] Train Loss: 6.3029, Acc: 0.3007 | Val Loss: 6.2820, Val Acc: 0.3099\n",
            "  -> Best model saved with val_loss: 6.2820\n",
            "[Epoch 057/100] Train Loss: 6.2949, Acc: 0.3011 | Val Loss: 6.2754, Val Acc: 0.3101\n",
            "  -> Best model saved with val_loss: 6.2754\n",
            "[Epoch 058/100] Train Loss: 6.2854, Acc: 0.3012 | Val Loss: 6.2689, Val Acc: 0.3107\n",
            "  -> Best model saved with val_loss: 6.2689\n",
            "[Epoch 059/100] Train Loss: 6.2787, Acc: 0.3013 | Val Loss: 6.2623, Val Acc: 0.3107\n",
            "  -> Best model saved with val_loss: 6.2623\n",
            "[Epoch 060/100] Train Loss: 6.2723, Acc: 0.3017 | Val Loss: 6.2560, Val Acc: 0.3111\n",
            "  -> Best model saved with val_loss: 6.2560\n",
            "[Epoch 061/100] Train Loss: 6.2636, Acc: 0.3019 | Val Loss: 6.2500, Val Acc: 0.3113\n",
            "  -> Best model saved with val_loss: 6.2500\n",
            "[Epoch 062/100] Train Loss: 6.2569, Acc: 0.3024 | Val Loss: 6.2441, Val Acc: 0.3118\n",
            "  -> Best model saved with val_loss: 6.2441\n",
            "[Epoch 063/100] Train Loss: 6.2506, Acc: 0.3023 | Val Loss: 6.2383, Val Acc: 0.3118\n",
            "  -> Best model saved with val_loss: 6.2383\n",
            "[Epoch 064/100] Train Loss: 6.2437, Acc: 0.3027 | Val Loss: 6.2327, Val Acc: 0.3120\n",
            "  -> Best model saved with val_loss: 6.2327\n",
            "[Epoch 065/100] Train Loss: 6.2370, Acc: 0.3029 | Val Loss: 6.2273, Val Acc: 0.3121\n",
            "  -> Best model saved with val_loss: 6.2273\n",
            "[Epoch 066/100] Train Loss: 6.2305, Acc: 0.3030 | Val Loss: 6.2222, Val Acc: 0.3121\n",
            "  -> Best model saved with val_loss: 6.2222\n",
            "[Epoch 067/100] Train Loss: 6.2252, Acc: 0.3034 | Val Loss: 6.2168, Val Acc: 0.3124\n",
            "  -> Best model saved with val_loss: 6.2168\n",
            "[Epoch 068/100] Train Loss: 6.2181, Acc: 0.3040 | Val Loss: 6.2117, Val Acc: 0.3125\n",
            "  -> Best model saved with val_loss: 6.2117\n",
            "[Epoch 069/100] Train Loss: 6.2122, Acc: 0.3040 | Val Loss: 6.2067, Val Acc: 0.3131\n",
            "  -> Best model saved with val_loss: 6.2067\n",
            "[Epoch 070/100] Train Loss: 6.2060, Acc: 0.3045 | Val Loss: 6.2019, Val Acc: 0.3134\n",
            "  -> Best model saved with val_loss: 6.2019\n",
            "[Epoch 071/100] Train Loss: 6.2010, Acc: 0.3046 | Val Loss: 6.1973, Val Acc: 0.3135\n",
            "  -> Best model saved with val_loss: 6.1973\n",
            "[Epoch 072/100] Train Loss: 6.1958, Acc: 0.3049 | Val Loss: 6.1929, Val Acc: 0.3141\n",
            "  -> Best model saved with val_loss: 6.1929\n",
            "[Epoch 073/100] Train Loss: 6.1901, Acc: 0.3053 | Val Loss: 6.1882, Val Acc: 0.3144\n",
            "  -> Best model saved with val_loss: 6.1882\n",
            "[Epoch 074/100] Train Loss: 6.1838, Acc: 0.3060 | Val Loss: 6.1838, Val Acc: 0.3144\n",
            "  -> Best model saved with val_loss: 6.1838\n",
            "[Epoch 075/100] Train Loss: 6.1790, Acc: 0.3061 | Val Loss: 6.1795, Val Acc: 0.3142\n",
            "  -> Best model saved with val_loss: 6.1795\n",
            "[Epoch 076/100] Train Loss: 6.1731, Acc: 0.3064 | Val Loss: 6.1754, Val Acc: 0.3144\n",
            "  -> Best model saved with val_loss: 6.1754\n",
            "[Epoch 077/100] Train Loss: 6.1685, Acc: 0.3064 | Val Loss: 6.1711, Val Acc: 0.3146\n",
            "  -> Best model saved with val_loss: 6.1711\n",
            "[Epoch 078/100] Train Loss: 6.1628, Acc: 0.3071 | Val Loss: 6.1671, Val Acc: 0.3145\n",
            "  -> Best model saved with val_loss: 6.1671\n",
            "[Epoch 079/100] Train Loss: 6.1611, Acc: 0.3069 | Val Loss: 6.1631, Val Acc: 0.3145\n",
            "  -> Best model saved with val_loss: 6.1631\n",
            "[Epoch 080/100] Train Loss: 6.1538, Acc: 0.3072 | Val Loss: 6.1593, Val Acc: 0.3146\n",
            "  -> Best model saved with val_loss: 6.1593\n",
            "[Epoch 081/100] Train Loss: 6.1484, Acc: 0.3073 | Val Loss: 6.1555, Val Acc: 0.3143\n",
            "  -> Best model saved with val_loss: 6.1555\n",
            "[Epoch 082/100] Train Loss: 6.1441, Acc: 0.3077 | Val Loss: 6.1519, Val Acc: 0.3136\n",
            "  -> Best model saved with val_loss: 6.1519\n",
            "[Epoch 083/100] Train Loss: 6.1403, Acc: 0.3077 | Val Loss: 6.1483, Val Acc: 0.3139\n",
            "  -> Best model saved with val_loss: 6.1483\n",
            "[Epoch 084/100] Train Loss: 6.1345, Acc: 0.3082 | Val Loss: 6.1448, Val Acc: 0.3139\n",
            "  -> Best model saved with val_loss: 6.1448\n",
            "[Epoch 085/100] Train Loss: 6.1311, Acc: 0.3084 | Val Loss: 6.1414, Val Acc: 0.3138\n",
            "  -> Best model saved with val_loss: 6.1414\n",
            "[Epoch 086/100] Train Loss: 6.1243, Acc: 0.3085 | Val Loss: 6.1379, Val Acc: 0.3146\n",
            "  -> Best model saved with val_loss: 6.1379\n",
            "[Epoch 087/100] Train Loss: 6.1220, Acc: 0.3086 | Val Loss: 6.1346, Val Acc: 0.3147\n",
            "  -> Best model saved with val_loss: 6.1346\n",
            "[Epoch 088/100] Train Loss: 6.1176, Acc: 0.3087 | Val Loss: 6.1312, Val Acc: 0.3147\n",
            "  -> Best model saved with val_loss: 6.1312\n",
            "[Epoch 089/100] Train Loss: 6.1135, Acc: 0.3086 | Val Loss: 6.1279, Val Acc: 0.3146\n",
            "  -> Best model saved with val_loss: 6.1279\n",
            "[Epoch 090/100] Train Loss: 6.1089, Acc: 0.3089 | Val Loss: 6.1251, Val Acc: 0.3149\n",
            "  -> Best model saved with val_loss: 6.1251\n",
            "[Epoch 091/100] Train Loss: 6.1065, Acc: 0.3087 | Val Loss: 6.1219, Val Acc: 0.3151\n",
            "  -> Best model saved with val_loss: 6.1219\n",
            "[Epoch 092/100] Train Loss: 6.1013, Acc: 0.3092 | Val Loss: 6.1187, Val Acc: 0.3147\n",
            "  -> Best model saved with val_loss: 6.1187\n",
            "[Epoch 093/100] Train Loss: 6.0977, Acc: 0.3090 | Val Loss: 6.1156, Val Acc: 0.3150\n",
            "  -> Best model saved with val_loss: 6.1156\n",
            "[Epoch 094/100] Train Loss: 6.0926, Acc: 0.3094 | Val Loss: 6.1126, Val Acc: 0.3156\n",
            "  -> Best model saved with val_loss: 6.1126\n",
            "[Epoch 095/100] Train Loss: 6.0899, Acc: 0.3097 | Val Loss: 6.1098, Val Acc: 0.3151\n",
            "  -> Best model saved with val_loss: 6.1098\n",
            "[Epoch 096/100] Train Loss: 6.0864, Acc: 0.3099 | Val Loss: 6.1070, Val Acc: 0.3154\n",
            "  -> Best model saved with val_loss: 6.1070\n",
            "[Epoch 097/100] Train Loss: 6.0811, Acc: 0.3096 | Val Loss: 6.1040, Val Acc: 0.3157\n",
            "  -> Best model saved with val_loss: 6.1040\n",
            "[Epoch 098/100] Train Loss: 6.0774, Acc: 0.3095 | Val Loss: 6.1013, Val Acc: 0.3158\n",
            "  -> Best model saved with val_loss: 6.1013\n",
            "[Epoch 099/100] Train Loss: 6.0753, Acc: 0.3095 | Val Loss: 6.0986, Val Acc: 0.3158\n",
            "  -> Best model saved with val_loss: 6.0986\n",
            "[Epoch 100/100] Train Loss: 6.0712, Acc: 0.3099 | Val Loss: 6.0958, Val Acc: 0.3157\n",
            "  -> Best model saved with val_loss: 6.0958\n",
            "학습 완료! 최고 성능 모델로 테스트 시작\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/14/wm3n_r1s01jfgw6n8j7pr4740000gn/T/ipykernel_11986/726066875.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model.load_state_dict(torch.load('best_model_final.pth')) # 파일 이름은 이전과 동일하게 사용\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 배고프다.\n",
            "출력 : 너무 있어요.\n",
            "입력 : 오늘 날씨 어때?\n",
            "출력 : 하지 것도 할 같아요.\n",
            "입력 : 영화 추천해줘\n",
            "출력 : 잘 있어요.\n",
            "입력 : 요즘 기분이 우울해\n",
            "출력 : 충분히 거예요.\n"
          ]
        }
      ],
      "source": [
        "# --- 레이블 스무딩을 적용한 최종 학습 코드 ---\n",
        "\n",
        "# 1. 데이터셋 및 데이터로더 준비 (이전과 동일)\n",
        "print(\"--- 데이터 로딩 시작 ---\")\n",
        "MAX_LENGTH = 15\n",
        "BATCH_SIZE = 64\n",
        "csv_file_path = 'data/ChatbotData.csv'\n",
        "\n",
        "dataset = ChatbotDataset(csv_file_path, sp, max_length=MAX_LENGTH)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "print(f\"훈련: {len(train_dataset)}개, 검증: {len(val_dataset)}개\")\n",
        "\n",
        "\n",
        "# 2. 모델 및 관련 요소 정의\n",
        "NUM_LAYERS = 6\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "UNITS = 1024\n",
        "DROPOUT = 0.1\n",
        "VOCAB_SIZE = sp.GetPieceSize()\n",
        "\n",
        "model = Transformer(VOCAB_SIZE, NUM_LAYERS, UNITS, D_MODEL, NUM_HEADS, DROPOUT)\n",
        "optimizer = optim.AdamW(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda(D_MODEL, warmup_steps=4000\n",
        "))\n",
        "\n",
        "# --- 여기가 핵심: 레이블 스무딩 적용 ---\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=sp.pad_id(), label_smoothing=0.1)\n",
        "print('레이블 스무딩(0.1)이 적용된 손실 함수를 사용')\n",
        "# ------------------------------------\n",
        "\n",
        "# 3. 학습 실행 (train_and_validate 함수는 이전과 동일)\n",
        "EPOCHS = 100 # 경향을 보기 위해 100으로 설정\n",
        "PATIENCE = 5   # 조기 종료 patience\n",
        "\n",
        "final_train_and_validate(\n",
        "    model, train_dataloader, val_dataloader,\n",
        "    optimizer, loss_function, scheduler,\n",
        "    EPOCHS, device, PATIENCE\n",
        ")\n",
        "\n",
        "# 4. 학습 완료 후 최고 성능 모델로 테스트\n",
        "print('학습 완료! 최고 성능 모델로 테스트 시작')\n",
        "best_model = Transformer(VOCAB_SIZE, NUM_LAYERS, UNITS, D_MODEL, NUM_HEADS, DROPOUT)\n",
        "best_model.load_state_dict(torch.load('best_model_final.pth')) # 파일 이름은 이전과 동일하게 사용\n",
        "best_model.to(device)\n",
        "\n",
        "test_sentences = [\"배고프다.\", \"오늘 날씨 어때?\", \"영화 추천해줘\", \"요즘 기분이 우울해\"]\n",
        "for sentence in test_sentences:\n",
        "    sentence_generation_sampling(best_model, sentence, sp, device, top_k=50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "my-pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
